<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="UTF-8" />
<meta name="author" content="Nic Freeman" />
<meta name="generator" content="LaTeX Lwarp package" />
<meta name="description" content="MAS350 Probability with Measure, Sheffield University, March 23, 2023." />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<!--[if lt IE 9]>
<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
<title>MAS350 â€” Solutions to exercises</title>
<link rel="stylesheet" type="text/css" href="sans-serif-lwarp-sagebrush.css" />
<script>
// Lwarp MathJax emulation code
//
// Based on code by Davide P. Cervone.
// Equation numbering: https://github.com/mathjax/MathJax/issues/2427
// Starred and ifnextchar macros: https://github.com/mathjax/MathJax/issues/2428
// \left, \right delimiters: https://github.com/mathjax/MathJax/issues/2535
//
// Modified by Brian Dunn to adjust equation numbering and add subequations.
//
// LaTeX can use \seteqnumber{subequations?}{section}{number} before each equation.
// subequations? is 0 usually, 1 if inside subequations.
// section is a string printed as-is, or empty.
// number is auto-incremented by MathJax between equations.
//
MathJax = {
  subequations: "0",
  section: "",
  loader: {
    load: ['[tex]/tagformat', '[tex]/textmacros'],
  },
  startup: {
    ready() {
      // These would be replaced by import commands if you wanted to make
      // a proper extension.
      const Configuration = MathJax._.input.tex.Configuration.Configuration;
      const CommandMap = MathJax._.input.tex.SymbolMap.CommandMap;
      const Macro = MathJax._.input.tex.Symbol.Macro;
      const TexError = MathJax._.input.tex.TexError.default;
      const ParseUtil = MathJax._.input.tex.ParseUtil.default;
      const expandable = MathJax._.util.Options.expandable;

        // Insert the replacement string into the TeX string, and check
        // that there haven't been too many maxro substitutions (prevents
        // infinite loops).
        const useArgument = (parser, text) => {
          parser.string = ParseUtil.addArgs(parser, text, parser.string.slice(parser.i));
          parser.i = 0;
          if (++parser.macroCount > parser.configuration.options.maxMacros) {
            throw new TexError('MaxMacroSub1',
            'MathJax maximum macro substitution count exceeded; ' +
            'is there a recursive macro call?');
          }
        }

        // Create the command map for:
        //     \ifstar, \ifnextchar, \ifblank, \ifstrequal, \gsub, \seteqnumber
        new CommandMap('Lwarp-macros', {
          ifstar: 'IfstarFunction',
          ifnextchar: 'IfnextcharFunction',
          ifblank: 'IfblankFunction',
          ifstrequal: 'IfstrequalFunction',
          gsubstitute: 'GsubstituteFunction',
          seteqnumber: 'SeteqnumberFunction'
        }, {
          // This function implements an ifstar macro.
          IfstarFunction(parser, name) {
             const resultstar = parser.GetArgument(name);
             const resultnostar = parser.GetArgument(name);
             const star = parser.GetStar();                 // true if there is a *
             useArgument(parser, star ? resultstar : resultnostar);
          },

          // This function implements an ifnextchar macro.
          IfnextcharFunction(parser, name) {
            let whichchar = parser.GetArgument(name);
            if (whichchar.match(/^(?:0x[0-9A-F]+|[0-9]+)$/i)) {
              // $ syntax highlighting
              whichchar = String.fromCodePoint(parseInt(whichchar));
            }
            const resultnextchar = parser.GetArgument(name);
            const resultnotnextchar = parser.GetArgument(name);
            const gotchar = (parser.GetNext() === whichchar);
            useArgument(parser, gotchar ? resultnextchar : resultnotnextchar);
          },

          // This function implements an ifblank macro.
          IfblankFunction(parser, name) {
            const blankarg = parser.GetArgument(name);
            const resultblank = parser.GetArgument(name);
            const resultnotblank = parser.GetArgument(name);
            const isblank = (blankarg.trim() == "");
            useArgument(parser, isblank ? resultblank : resultnotblank);
          },

          // This function implements an ifstrequal macro.
          IfstrequalFunction(parser, name) {
            const strequalfirst = parser.GetArgument(name);
            const strequalsecond = parser.GetArgument(name);
            const resultequal = parser.GetArgument(name);
            const resultnotequal = parser.GetArgument(name);
            const isequal = (strequalfirst == strequalsecond);
            useArgument(parser, isequal ? resultequal : resultnotequal);
          },

          // This function implements a gsub macro.
          GsubstituteFunction(parser, name) {
            const gsubfirst = parser.GetArgument(name);
            const gsubsecond = parser.GetArgument(name);
            const gsubthird = parser.GetArgument(name);
            let gsubresult=gsubfirst.replace(gsubsecond, gsubthird);
            useArgument(parser, gsubresult);
          },

          // This function modifies the equation numbers.
          SeteqnumberFunction(parser, name) {
              // Get the macro parameters
              const star = parser.GetStar();                  // true if there is a *
              const optBrackets = parser.GetBrackets(name);   // contents of optional brackets
              const newsubequations = parser.GetArgument(name); // the subequations argument
              const neweqsection = parser.GetArgument(name); // the eq section argument
              const neweqnumber = parser.GetArgument(name);   // the eq number argument
              MathJax.config.subequations=newsubequations ;   // a string with boolean meaning
              MathJax.config.section=neweqsection ;           // a string with numeric meaning
              parser.tags.counter = parser.tags.allCounter = neweqnumber ;
          }

        });

        // Create the Lwarp-macros package
        Configuration.create('Lwarp-macros', {
          handler: {macro: ['Lwarp-macros']}
        });

        MathJax.startup.defaultReady();

        // For forward references:
        MathJax.startup.input[0].preFilters.add(({math}) => {
          if (math.inputData.recompile){
              MathJax.config.subequations = math.inputData.recompile.subequations;
              MathJax.config.section = math.inputData.recompile.section;
          }
        });
        MathJax.startup.input[0].postFilters.add(({math}) => {
          if (math.inputData.recompile){
              math.inputData.recompile.subequations = MathJax.config.subequations;
              math.inputData.recompile.section = MathJax.config.section;
          }
        });

          // For \left, \right with unicode-math:
          const {DelimiterMap} = MathJax._.input.tex.SymbolMap;
          const {Symbol} = MathJax._.input.tex.Symbol;
          const {MapHandler} = MathJax._.input.tex.MapHandler;
          const delimiter = MapHandler.getMap('delimiter');
          delimiter.add('\\lBrack', new Symbol('\\lBrack', '\u27E6'));
          delimiter.add('\\rBrack', new Symbol('\\rBrack', '\u27E7'));
          delimiter.add('\\lAngle', new Symbol('\\lAngle', '\u27EA'));
          delimiter.add('\\rAngle', new Symbol('\\rAngle', '\u27EB'));
          delimiter.add('\\lbrbrak', new Symbol('\\lbrbrak', '\u2772'));
          delimiter.add('\\rbrbrak', new Symbol('\\rbrbrak', '\u2773'));
          delimiter.add('\\lbag', new Symbol('\\lbag', '\u27C5'));
          delimiter.add('\\rbag', new Symbol('\\rbag', '\u27C6'));
          delimiter.add('\\llparenthesis', new Symbol('\\llparenthesis', '\u2987'));
          delimiter.add('\\rrparenthesis', new Symbol('\\rrparenthesis', '\u2988'));
          delimiter.add('\\llangle', new Symbol('\\llangle', '\u2989'));
          delimiter.add('\\rrangle', new Symbol('\\rrangle', '\u298A'));
          delimiter.add('\\Lbrbrak', new Symbol('\\Lbrbrak', '\u27EC'));
          delimiter.add('\\Rbrbrak', new Symbol('\\Rbrbrak', '\u27ED'));
          delimiter.add('\\lBrace', new Symbol('\\lBrace', '\u2983'));
          delimiter.add('\\rBrace', new Symbol('\\rBrace', '\u2984'));
          delimiter.add('\\lParen', new Symbol('\\lParen', '\u2985'));
          delimiter.add('\\rParen', new Symbol('\\rParen', '\u2986'));
          delimiter.add('\\lbrackubar', new Symbol('\\lbrackubar', '\u298B'));
          delimiter.add('\\rbrackubar', new Symbol('\\rbrackubar', '\u298C'));
          delimiter.add('\\lbrackultick', new Symbol('\\lbrackultick', '\u298D'));
          delimiter.add('\\rbracklrtick', new Symbol('\\rbracklrtick', '\u298E'));
          delimiter.add('\\lbracklltick', new Symbol('\\lbracklltick', '\u298F'));
          delimiter.add('\\rbrackurtick', new Symbol('\\rbrackurtick', '\u2990'));
          delimiter.add('\\langledot', new Symbol('\\langledot', '\u2991'));
          delimiter.add('\\rangledot', new Symbol('\\rangledot', '\u2992'));
          delimiter.add('\\lparenless', new Symbol('\\lparenless', '\u2993'));
          delimiter.add('\\rparengtr', new Symbol('\\rparengtr', '\u2994'));
          delimiter.add('\\Lparengtr', new Symbol('\\Lparengtr', '\u2995'));
          delimiter.add('\\Rparenless', new Symbol('\\Rparenless', '\u2996'));
          delimiter.add('\\lblkbrbrak', new Symbol('\\lblkbrbrak', '\u2997'));
          delimiter.add('\\rblkbrbrak', new Symbol('\\rblkbrbrak', '\u2998'));
          delimiter.add('\\lvzigzag', new Symbol('\\lvzigzag', '\u29D8'));
          delimiter.add('\\rvzigzag', new Symbol('\\rvzigzag', '\u29D9'));
          delimiter.add('\\Lvzigzag', new Symbol('\\Lvzigzag', '\u29DA'));
          delimiter.add('\\Rvzigzag', new Symbol('\\Rvzigzag', '\u29DB'));
          delimiter.add('\\lcurvyangle', new Symbol('\\lcurvyangle', '\u29FC'));
          delimiter.add('\\rcurvyangle', new Symbol('\\rcurvyangle', '\u29FD'));
          delimiter.add('\\Vvert', new Symbol('\\Vvert', '\u2980'));
    }     // ready
  },      // startup

  tex: {
    packages: {'[+]': ['tagformat', 'Lwarp-macros', 'textmacros']},
    tags: "ams",
         tagformat: {
             number: function (n) {
                 if(MathJax.config.subequations==0)
                     return(MathJax.config.section + n);
                 else
                     return(MathJax.config.section + String.fromCharCode(96+n));
             },
         },
  }
}
</script>

<script
    id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"
></script>


</head>
<body>



<a id="notes-autopage-247"></a>
<nav class="topnavigation"><a href="notes.html" class="linkhome" >
Home</a></nav>

<header>

<p>
MAS350 Probability with Measure
</p>

</header>



<div class="bodyandsidetoc">
<div class="sidetoccontainer">



<nav class="sidetoc">



<div class="sidetoctitle">

<p>
<span class="sidetocthetitle">Probability with Measure</span>
</p>

<p>
Contents
</p>
</div>



<div class="sidetoccontents">

<p>
<a href="notes.html" class="linkhome" >
Home</a>
</p>

<p>
<a href="Introduction.html#autosec-5" class="tocchapter" >
<span class="sectionnumber">0</span>&#x2003;Introduction</a>
</p>



<p>
<a href="Introduction.html#autosec-6" class="tocsection" >
<span class="sectionnumber">0.1</span>&#x2003;Organization</a>
</p>



<p>
<a href="Preliminaries.html#autosec-13" class="tocsection" >
<span class="sectionnumber">0.2</span>&#x2003;Preliminaries</a>
</p>



<p>
<a href="Measure-Spaces.html#autosec-16" class="tocchapter" >
<span class="sectionnumber">1</span>&#x2003;Measure Spaces</a>
</p>



<p>
<a href="Measure-Spaces.html#autosec-17" class="tocsection" >
<span class="sectionnumber">1.1</span>&#x2003;What is measure theory?</a>
</p>



<p>
<a href="Sigma-Fields.html#autosec-20" class="tocsection" >
<span class="sectionnumber">1.2</span>&#x2003;Sigma Fields</a>
</p>



<p>
<a href="Measure.html#autosec-31" class="tocsection" >
<span class="sectionnumber">1.3</span>&#x2003;Measure</a>
</p>



<p>
<a href="The-Borel-field.html#autosec-37" class="tocsection" >
<span class="sectionnumber">1.4</span>&#x2003;The Borel \(\sigma \)-field</a>
</p>



<p>
<a href="Lebesgue-Measure.html#autosec-42" class="tocsection" >
<span class="sectionnumber">1.5</span>&#x2003;Lebesgue Measure</a>
</p>



<p>
<a href="An-example-non-measurable-set.html#autosec-49" class="tocsection" >
<span class="sectionnumber">1.6</span>&#x2003;An example of a non-measurable set \((\star )\)</a>
</p>



<p>
<a href="Measure-limits.html#autosec-53" class="tocsection" >
<span class="sectionnumber">1.7</span>&#x2003;Measure and limits</a>
</p>



<p>
<a href="Product-Measures.html#autosec-57" class="tocsection" >
<span class="sectionnumber">1.8</span>&#x2003;Product Measures</a>
</p>



<p>
<a href="Exercises-1.html#autosec-62" class="tocsection" >
<span class="sectionnumber">1.9</span>&#x2003;Exercises 1</a>
</p>



<p>
<a href="Measurable-Functions.html#autosec-66" class="tocchapter" >
<span class="sectionnumber">2</span>&#x2003;Measurable Functions</a>
</p>



<p>
<a href="Measurable-Functions.html#autosec-67" class="tocsection" >
<span class="sectionnumber">2.1</span>&#x2003;Liminf and Limsup</a>
</p>



<p>
<a href="Measurable-Functions-Basic-Concepts.html#autosec-70" class="tocsection" >
<span class="sectionnumber">2.2</span>&#x2003;Measurable Functions - Basic Concepts</a>
</p>



<p>
<a href="Examples-Measurable-Functions.html#autosec-78" class="tocsection" >
<span class="sectionnumber">2.3</span>&#x2003;Examples of Measurable Functions</a>
</p>



<p>
<a href="Algebra-Measurable-Functions.html#autosec-83" class="tocsection" >
<span class="sectionnumber">2.4</span>&#x2003;Algebra of Measurable Functions</a>
</p>



<p>
<a href="Simple-Functions.html#autosec-90" class="tocsection" >
<span class="sectionnumber">2.5</span>&#x2003;Simple Functions</a>
</p>



<p>
<a href="Extended-Real-Functions.html#autosec-93" class="tocsection" >
<span class="sectionnumber">2.6</span>&#x2003;Extended Real Functions</a>
</p>



<p>
<a href="Exercises-2.html#autosec-95" class="tocsection" >
<span class="sectionnumber">2.7</span>&#x2003;Exercises 2</a>
</p>



<p>
<a href="Lebesgue-Integration.html#autosec-99" class="tocchapter" >
<span class="sectionnumber">3</span>&#x2003;Lebesgue Integration</a>
</p>



<p>
<a href="Lebesgue-Integration.html#autosec-100" class="tocsection" >
<span class="sectionnumber">3.1</span>&#x2003;Introduction</a>
</p>



<p>
<a href="The-Lebesgue-Integral-Simple-Functions.html#autosec-103" class="tocsection" >
<span class="sectionnumber">3.2</span>&#x2003;The Lebesgue Integral for Simple Functions</a>
</p>



<p>
<a href="The-Lebesgue-Integral-Non-negative-Measurable-Functions.html#autosec-108" class="tocsection" >
<span class="sectionnumber">3.3</span>&#x2003;The Lebesgue Integral for Non-negative Measurable Functions</a>
</p>



<p>
<a href="The-Monotone-Convergence-Theorem.html#autosec-119" class="tocsection" >
<span class="sectionnumber">3.4</span>&#x2003;The Monotone Convergence Theorem</a>
</p>



<p>
<a href="Fatou-Lemma.html#autosec-124" class="tocsection" >
<span class="sectionnumber">3.5</span>&#x2003;Fatouâ€™s Lemma</a>
</p>



<p>
<a href="Lebesgue-Integrability.html#autosec-127" class="tocsection" >
<span class="sectionnumber">3.6</span>&#x2003;Lebesgue Integrability</a>
</p>



<p>
<a href="The-Dominated-Convergence-Theorem.html#autosec-133" class="tocsection" >
<span class="sectionnumber">3.7</span>&#x2003;The Dominated Convergence Theorem</a>
</p>



<p>
<a href="Calculations-with-Lebesgue-Integral.html#autosec-137" class="tocsection" >
<span class="sectionnumber">3.8</span>&#x2003;Calculations with the Lebesgue Integral</a>
</p>



<p>
<a href="Fubini-Theorem-Function-Spaces.html#autosec-144" class="tocsection" >
<span class="sectionnumber">3.9</span>&#x2003;Fubiniâ€™s Theorem and Function Spaces \((\star )\)</a>
</p>



<p>
<a href="Riemann-Integration.html#autosec-149" class="tocsection" >
<span class="sectionnumber">3.10</span>&#x2003;Riemann Integration \((\star )\)</a>
</p>



<p>
<a href="Exercises-3.html#autosec-159" class="tocsection" >
<span class="sectionnumber">3.11</span>&#x2003;Exercises 3</a>
</p>



<p>
<a href="Probability-Measure.html#autosec-167" class="tocchapter" >
<span class="sectionnumber">4</span>&#x2003;Probability and Measure</a>
</p>



<p>
<a href="Probability-Measure.html#autosec-169" class="tocsection" >
<span class="sectionnumber">4.1</span>&#x2003;Probability as Measure</a>
</p>



<p>
<a href="The-Cumulative-Distribution-Function.html#autosec-172" class="tocsection" >
<span class="sectionnumber">4.2</span>&#x2003;The Cumulative Distribution Function</a>
</p>



<p>
<a href="Discrete-Continuous-Random-Variables.html#autosec-177" class="tocsection" >
<span class="sectionnumber">4.3</span>&#x2003;Discrete and Continuous Random Variables</a>
</p>



<p>
<a href="Independence.html#autosec-180" class="tocsection" >
<span class="sectionnumber">4.4</span>&#x2003;Independence</a>
</p>



<p>
<a href="Exercises-4.html#autosec-185" class="tocsection" >
<span class="sectionnumber">4.5</span>&#x2003;Exercises 4</a>
</p>



<p>
<a href="Sequences-random-variables.html#autosec-189" class="tocchapter" >
<span class="sectionnumber">5</span>&#x2003;Sequences of random variables</a>
</p>



<p>
<a href="Sequences-random-variables.html#autosec-190" class="tocsection" >
<span class="sectionnumber">5.1</span>&#x2003;The Borel-Cantelli lemmas</a>
</p>



<p>
<a href="Convergence-Random-Variables.html#autosec-194" class="tocsection" >
<span class="sectionnumber">5.2</span>&#x2003;Convergence of Random Variables</a>
</p>



<p>
<a href="Laws-Large-Numbers.html#autosec-202" class="tocsection" >
<span class="sectionnumber">5.3</span>&#x2003;Laws of Large Numbers</a>
</p>



<p>
<a href="Characteristic-Functions-Weak-Convergence.html#autosec-208" class="tocsection" >
<span class="sectionnumber">5.4</span>&#x2003;Characteristic Functions and Weak Convergence (\(\star \))</a>
</p>



<p>
<a href="The-Central-Limit-Theorem.html#autosec-218" class="tocsection" >
<span class="sectionnumber">5.5</span>&#x2003;The Central Limit Theorem (\(\star \))</a>
</p>



<p>
<a href="Exercises-5.html#autosec-222" class="tocsection" >
<span class="sectionnumber">5.6</span>&#x2003;Exercises 5</a>
</p>



<p>
<a href="Product-Measures-Fubini-Theorem.html#autosec-227" class="tocchapter" >
<span class="sectionnumber">6</span>&#x2003;Product Measures and Fubiniâ€™s Theorem \((\Delta )\)</a>
</p>



<p>
<a href="Product-Measures-Fubini-Theorem.html#autosec-228" class="tocsection" >
<span class="sectionnumber">6.1</span>&#x2003;Dynkinâ€™s \(\pi -\lambda \) Lemma \((\Delta )\)</a>
</p>



<p>
<a href="Product-Measure.html#autosec-233" class="tocsection" >
<span class="sectionnumber">6.2</span>&#x2003;Product Measure \((\Delta )\)</a>
</p>



<p>
<a href="Fubini-Theorem.html#autosec-237" class="tocsection" >
<span class="sectionnumber">6.3</span>&#x2003;Fubiniâ€™s Theorem \((\Delta )\)</a>
</p>



<p>
<a href="Exercises-6.html#autosec-241" class="tocsection" >
<span class="sectionnumber">6.4</span>&#x2003;Exercises 6</a>
</p>



<p>
<a href="Advice-revision-exams.html#autosec-244" class="tocchapter" >
<span class="sectionnumber">A</span>&#x2003;Advice for revision/exams</a>
</p>



<p>
<a href="Solutions-exercises.html#autosec-248" class="tocchapter" >
<span class="sectionnumber">B</span>&#x2003;Solutions to exercises</a>
</p>



</div>

</nav>

</div>



<main class="bodycontainer">



<section class="textbody">

<h1>Probability with Measure</h1>

<!--MathJax customizations:-->



<div class="hidden">

\(\newcommand{\footnotename}{footnote}\)

\(\def \LWRfootnote {1}\)

\(\newcommand {\footnote }[2][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand {\footnotemark }[1][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\let \LWRorighspace \hspace \)

\(\renewcommand {\hspace }{\ifstar \LWRorighspace \LWRorighspace }\)

\(\newcommand {\mathnormal }[1]{{#1}}\)

\(\newcommand \ensuremath [1]{#1}\)

\(\newcommand {\LWRframebox }[2][]{\fbox {#2}} \newcommand {\framebox }[1][]{\LWRframebox } \)

\(\newcommand {\setlength }[2]{}\)

\(\newcommand {\addtolength }[2]{}\)

\(\newcommand {\setcounter }[2]{}\)

\(\newcommand {\addtocounter }[2]{}\)

\(\newcommand {\arabic }[1]{}\)

\(\newcommand {\number }[1]{}\)

\(\newcommand {\noalign }[1]{\text {#1}\notag \\}\)

\(\newcommand {\cline }[1]{}\)

\(\newcommand {\directlua }[1]{\text {(directlua)}}\)

\(\newcommand {\luatexdirectlua }[1]{\text {(directlua)}}\)

\(\newcommand {\protect }{}\)

\(\def \LWRabsorbnumber #1 {}\)

\(\def \LWRabsorbquotenumber &quot;#1 {}\)

\(\newcommand {\LWRabsorboption }[1][]{}\)

\(\newcommand {\LWRabsorbtwooptions }[1][]{\LWRabsorboption }\)

\(\def \mathchar {\ifnextchar &quot;\LWRabsorbquotenumber \LWRabsorbnumber }\)

\(\def \mathcode #1={\mathchar }\)

\(\let \delcode \mathcode \)

\(\let \delimiter \mathchar \)

\(\def \oe {\unicode {x0153}}\)

\(\def \OE {\unicode {x0152}}\)

\(\def \ae {\unicode {x00E6}}\)

\(\def \AE {\unicode {x00C6}}\)

\(\def \aa {\unicode {x00E5}}\)

\(\def \AA {\unicode {x00C5}}\)

\(\def \o {\unicode {x00F8}}\)

\(\def \O {\unicode {x00D8}}\)

\(\def \l {\unicode {x0142}}\)

\(\def \L {\unicode {x0141}}\)

\(\def \ss {\unicode {x00DF}}\)

\(\def \SS {\unicode {x1E9E}}\)

\(\def \dag {\unicode {x2020}}\)

\(\def \ddag {\unicode {x2021}}\)

\(\def \P {\unicode {x00B6}}\)

\(\def \copyright {\unicode {x00A9}}\)

\(\def \pounds {\unicode {x00A3}}\)

\(\let \LWRref \ref \)

\(\renewcommand {\ref }{\ifstar \LWRref \LWRref }\)

\( \newcommand {\multicolumn }[3]{#3}\)

\(\require {textcomp}\)

\(\newcommand {\intertext }[1]{\text {#1}\notag \\}\)

\(\let \Hat \hat \)

\(\let \Check \check \)

\(\let \Tilde \tilde \)

\(\let \Acute \acute \)

\(\let \Grave \grave \)

\(\let \Dot \dot \)

\(\let \Ddot \ddot \)

\(\let \Breve \breve \)

\(\let \Bar \bar \)

\(\let \Vec \vec \)

\(\DeclareMathOperator {\var }{var}\)

\(\DeclareMathOperator {\cov }{cov}\)

\(\newcommand {\nN }{n \in \mathbb {N}}\)

\(\newcommand {\Br }{{\cal B}(\R )}\)

\(\newcommand {\F }{{\cal F}}\)

\(\newcommand {\ds }{\displaystyle }\)

\(\newcommand {\st }{\stackrel {d}{=}}\)

\(\newcommand {\uc }{\stackrel {uc}{\rightarrow }}\)

\(\newcommand {\la }{\langle }\)

\(\newcommand {\ra }{\rangle }\)

\(\newcommand {\li }{\liminf _{n \rightarrow \infty }}\)

\(\newcommand {\ls }{\limsup _{n \rightarrow \infty }}\)

\(\newcommand {\limn }{\lim _{n \rightarrow \infty }}\)

\(\def \to {\rightarrow }\)

\(\def \iff {\Leftrightarrow }\)

\(\def \sw {\subseteq }\)

\(\def \mc {\mathcal }\)

\(\def \mb {\mathbb }\)

\(\def \sc {\setminus }\)

\(\def \v {\textbf }\)

\(\def \E {\mb {E}}\)

\(\def \P {\mb {P}}\)

\(\def \R {\mb {R}}\)

\(\def \C {\mb {C}}\)

\(\def \N {\mb {N}}\)

\(\def \Q {\mb {Q}}\)

\(\def \Z {\mb {Z}}\)

\(\def \B {\mb {B}}\)

\(\def \~{\sim }\)

\(\def \-{\,;\,}\)

\(\def \qed {$\blacksquare $}\)

\(\def \1{\unicode {x1D7D9}}\)

\(\def \cadlag {c\&grave;{a}dl\&grave;{a}g}\)

\(\def \p {\partial }\)

\(\def \l {\left }\)

\(\def \r {\right }\)

\(\def \Om {\Omega }\)

\(\def \om {\omega }\)

</div>

<p>
<!--
......     chapter Solutions to exercises ......
-->
<h3 id="autosec-248">Chapter&nbsp;<span class="sectionnumber">B&#x2003;</span>Solutions to exercises</h3>
<a id="notes-autopage-248"></a>
<a id="notes-autofile-46"></a>
<!--
......    subsection Chapter <a href=Measure-Spaces.html#chap:measure_spaces>1</a> ......
-->
<h5 id="autosec-249">Chapter <a href="Measure-Spaces.html#chap:measure_spaces">1</a></h5>
<a id="notes-autopage-249"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-1.html#ps:size_Pn"><b>1.1</b></a> There are \(n \choose r\) subsets of size \(r\) for \(0 \leq r \leq n\) and so the total number of subsets is \(\sum _{r=0}^{n}{n \choose r} =
(1 + 1)^{2} = 2^{n}\). Here we used the binomial theorem \((x + y)^{n} = \sum _{r=0}^{n}{n \choose r}x^{r}y^{n-r}\).
</p>
</li>
<li>


<p>
<a href="Exercises-1.html#ps:sigma_union"><b>1.2</b></a> To show \(\Sigma _{1} \cap \Sigma _{2}\) is a \(\sigma \)-field we must verify S(i) to S(iii).
</p>
<p>
S(i) Since \(S \in \Sigma _{1}\) and \(S \in \Sigma _{2}\), \(S \in \Sigma _{1} \cap \Sigma _{2}\).
</p>
<p>
S(ii) Suppose \((A_{n})\) is a sequence of sets in \(\Sigma _{1} \cap \Sigma _{2}\). Then \(A_{n} \in \Sigma _{1}\) for all \(\nN \) and so \(\bigcup _{n=1}^{\infty }A_{n} \in \Sigma _{1}\). But
also \(A_{n} \in \Sigma _{2}\) for all \(\nN \) and so \(\bigcup _{n=1}^{\infty }A_{n} \in \Sigma _{2}\). Hence \(\bigcup _{n=1}^{\infty }A_{n} \in \Sigma _{1} \cap \Sigma _{2}\).
</p>
<p>
S(iii) If \(A \in \Sigma _{1} \cap \Sigma _{2}, A^{c} \in \Sigma _{1}\) and \(A^{c} \in \Sigma _{2}\). Hence \(A^{c} \in \Sigma _{1} \cap \Sigma _{2}\).
</p>
<p>
Note that the same argument can be used to show that if \(\{\Sigma _{n}, \nN )\) are all \(\sigma \)-fields of subsets of \(S\) then so is \(\bigcap _{n=1}^{\infty }\Sigma _{n}\).
</p>
<p>
\(\Sigma _{1} \cup \Sigma _{2}\) is not in general a \(\sigma \)-field for if \(A \in \Sigma _{1}\) and \(B \in \Sigma _{2}\) there is no good reason why \(A \cup B \in \Sigma _{1} \cup \Sigma
_{2}\). For example let \(S = \{1,2,3\}, \Sigma _{1} = \{\emptyset , \{1\}, \{2,3\}, S\}, \Sigma _{2} = \{\emptyset , \{2\}, \{1,3\}, S\}, A = \{1\}, B = \{2\}\). Then \(A \cup B =
\{1,2\}\) is neither in \(\Sigma _{1}\) nor \(\Sigma _{2}\).
</p>
</li>
<li>


<p>
<a href="Exercises-1.html#ps:measure_basic"><b>1.3</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> \(A \cup B = [A - (A \cap B)] \cup [B - (A \cap B)] \cup (A \cap B)\) is a disjoint union, hence using finite additivity and (1.3.2)
</p>
<p>
\[ m(A \cup B) = m(A- A \cap B) + m(B- A \cap B) + m(A \cap B).\]
</p>
<p>
Then
</p>
<p>
\[\begin {aligned} m(A \cup B) + m(A \cap B) &amp; = m(A- A \cap B) + m(B- A \cap B) + 2m(A \cap B)\\ &amp; = [m(A - A \cap B) + m(A \cap B)]\\ &amp; + &amp; [m(B-A \cap B) + m(A
\cap B)]\\ &amp; = m(A) + m(B), \end {aligned}\]
</p>
<p>
where we use the fact that \(A\) is the disjoint union of \(A - A \cap B\) and \(A \cap B\), and the analogous result for \(B\). Note that the possibility that \(m(A \cap B) = \infty \) is allowed for within
this proof.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> \(m(A \cup B) \leq m(A \cup B) + m(A \cap B) = m(A) + m(B)\) follows immediately from (a) as \(m(A \cap B) \geq 0\). The general case is proved by induction.
Weâ€™ve just established \(n=2\). Now suppose the result holds for some \(n\). Then
</p>
<p>
\[\begin {aligned} m\left (\bigcup _{i=1}^{n+1}A_{i}\right ) &amp; = m\left (\bigcup _{i=1}^{n}A_{i} \cup A_{n+1}\right )\\ &amp; \leq m\left (\bigcup _{i=1}^{n}A_{i}\right ) +
m(A_{n+1})\\ &amp; \leq \sum _{i=1}^{n}m(A_{i}) +m(A_{n+1}) = \sum _{i=1}^{n+1}m(A_{i}).                              \end {aligned}\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-1.html#ps:uniform_measure"><b>1.4</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We have that \((km)(\emptyset )=km(\emptyset )=0\) because \(m(\emptyset )=0\).
</p>
<p>
If \((A_n)_{n\in \N }\) is a sequence of disjoint measurable sets then
</p>
<p>
\[\sum \limits _{n=1}^\infty (km)(A_n)=k\sum \limits _{n=1}^\infty m(A_n)=k m\l (\bigcup _{n=1}^\infty A_n\r )=(km)\l (\bigcup _{n=1}^\infty A_n\r ).\]
</p>
<p>
For the second equality we use that \(m\) is \(\sigma \)-additive. Thus \(km\) is \(\sigma \)-additive.
</p>
<p>
Thus \(km\) is a measure.
</p>
<p>
If \(m\) is a finite measure, then by taking \(k=\frac {1}{m(S)}\) it follows immediately that \(\P (\cdot )=\frac {m(\cdot )}{m(S)}\) is a measure. Noting that \(\P (S)=\frac {m(S)}{m(S)}=1\), \(\P \)
is a probability measure.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> We have that \((m+n)(\emptyset )=m(\emptyset )+n(\emptyset )=0+0=0\).
</p>
<p>
If \((A_j)_{j\in \N }\) is a sequence of disjoint measurable sets then
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                                         âˆž
                                                                                         X                            J
                                                                                                                      X
                                                                                               (m + n)(Aj ) = lim           m(Aj ) + n(Aj )
                                                                                                               Jâ†’âˆž
                                                                                         j=1                          j=1
                                                                                                                      J
                                                                                                                      X                    J
                                                                                                                                           X
                                                                                                           = lim            m(Aj ) +             n(Aj )
                                                                                                               Jâ†’âˆž
                                                                                                                      j=1                  j=1
                                                                                                               âˆž
                                                                                                               X                    âˆž
                                                                                                                                    X
                                                                                                           =         m(Aj ) +             n(Aj )
                                                                                                               j=1                  j=1
                                                                                                                 ï£«              ï£¶         ï£«           ï£¶
                                                                                                                      âˆž
                                                                                                                      [                       âˆž
                                                                                                                                              [
                                                                                                           = mï£­            Aj ï£¸ + n ï£­               Aj ï£¸
                                                                                                                     j=1                      j=1
                                                                                                                            ï£«             ï£¶
                                                                                                                                âˆž
                                                                                                                                [
                                                                                                           = (m + n) ï£­                Aj ï£¸ .
                                                                                                                                j=1



-->


<p>


\begin{align*}
\sum \limits _{j=1}^\infty (m+n)(A_j) &amp;=\lim \limits _{J\to \infty } \sum \limits _{j=1}^J m(A_j)+n(A_j) \\ &amp;=\lim \limits _{J\to \infty } \sum \limits _{j=1}^J m(A_j)+
\sum \limits _{j=1}^J n(A_j) \\ &amp;=\sum \limits _{j=1}^\infty m(A_j)+\sum \limits _{j=1}^\infty n(A_j) \\ &amp;=m\l (\bigcup _{j=1}^\infty A_j\r )+n\l (\bigcup _{j=1}^\infty
A_j\r ) \\ &amp;=(m+n)\l (\bigcup _{j=1}^\infty A_j\r ).
\end{align*}
Here, the second follows because the sums are finite, and the third line follows because both series are increasing (and hence their limits both exist). The fourth line follows by \(\sigma \)-additivity of \(m\) and
\(n\).
</p>
<p>
Thus \(m+n\) is a measure.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-1.html#ps:conditional_measure"><b>1.5</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We have \(m_B(\emptyset )=m(\emptyset \cap B)=m(\emptyset )=0\).
</p>
<p>
If \((A_n)_{n\in \N }\) is a sequence of disjoint measurable sets then \((A_n\cap B)_{n\in \N }\) are also disjoint and measurable, hence
</p>
<p>
\[\sum \limits _{n=1}^\infty m_B(A_n)=\sum \limits _{n=1}^\infty m(A_n\cap B)=m\l (\bigcup _{n=1}^\infty A_n\cap B\r )=m\l (\l (\bigcup _{n=1}^\infty A_n\r )\cap B\r )=m_B(\bigcup
_{n=1}^\infty A_n).\]
</p>
<p>
Here to deduce the second equality we use the \(\sigma \)-additivity of \(m\).
</p>
<p>
Thus \(m_B\) is a measure.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Applying <a href="Exercises-1.html#ps:uniform_measure"><b>1.4</b></a> part (a) to \(m_B\), it is immediate that \(\P _B\) is a probability measure.
</p>
<p>
If \(m\) itself is a probability measure, say we write \(m=\P \), then \(\P _B\) is the conditional distribution of \(\P \) given that the event \(B\) occurs.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-1.html#ps:prob_measure"><b>1.6</b></a> The easiest way to see that \(m\) is a measure is to first use <a href="Exercises-1.html#ps:uniform_measure"><b>1.4</b></a> (a) and (c)
and induction to show that if \(m_{1},m_{2} \ldots , m_{n}\) are measures and \(c_{1}, c_{2}, \ldots , c_{n}\) are non-negative numbers then \(c_{1}m_{1} + c_{2}m_{2} + \cdots + c_{n}m_{n}\) is
a measure. Now apply this with \(m_{j} = \delta _{x_{j}} (1 \leq j \leq n)\). To get a probability measure we need \(\sum _{j=1}^{n}c_{j} = 1\) for then, as \(\delta _{x_{j}}\) is a probability
measure for all \(1 \leq j \leq n\), we have
</p>
<p>
\[ m(S) = \sum _{j=1}^{n}c_{j}\delta _{x_{j}}(S) = \sum _{j=1}^{n}c_{j} = 1.\]
</p>
<p>
<i>Itâ€™s also possible to check the definition directly, but it is a little more work that way.</i>
</p>
</li>
<li>


<p>
<a href="Exercises-1.html#ps:uniform_dist"><b>1.7</b></a> The uniform distribution \(m\) on \(([a,b],\mc {B}([a,b]))\) is given by
</p>
<p>
\[m(A)=\frac {\lambda (A)}{b-a}\]
</p>
<p>
where \(\lambda \) denotes Lebesgue measure.
</p>
</li>
<li>


<p>
<a href="Exercises-1.html#ps:borel_closed"><b>1.8</b></a> By definition \((a,b) \in {\cal B}(\R )\). Weâ€™ve shown in the notes that \(\{a\}, \{b\} \in {\cal B}(\R )\) and so by S(ii), \([a,b] =
\{a\} \cup (a,b) \cup \{b\} \in {\cal B}(\R )\).
</p>
</li>
<li>


<p>
<a href="Exercises-1.html#ps:meas_decreasing"><b>1.9</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(i)</span> We have that \(A\cap A^c=\emptyset \) and \(A\cup A^c=S\), so \(m(A)+m(A^c)=m(S)=M\). Because \(m(S)&lt;\infty \) we have also that \(m(A)&lt;\infty \),
hence we may subtract \(m(A)\) and obtain \(m(A^c)=M-m(A)\).
</p>
</li>
<li>


<p>
<span class="textnormal">(ii)</span> Let \((A_n)\) be a decreasing sequence of sets. Then \(B_n=S\sc A_n\) defines an increasing sequence of sets, so by the first part of Theorem <a
href="Measure-limits.html#thm:monotone_meas">1.7.1</a> we have \(m(B_n)\to m(B)\) where \(B=\cup _j B_j\).
</p>
<p>
By part (a) we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--



                                                                                  m(Bn ) = m(S \ An ) = m(S) âˆ’ m(An )

                                                                                   m(B) = m(âˆªj S \ An ) = m(S \ âˆ©j Aj ) = m(S) âˆ’ m(âˆ©j Aj )



-->


<p>


\begin{align*}
m(B_n)&amp;=m(S\sc A_n)=m(S)-m(A_n)\\ m(B)&amp;=m(\cup _j S\sc A_n)=m(S\sc \cap _j A_j)=m(S)-m(\cap _j A_j)
\end{align*}
Thus \(m(S)-m(A_n)\to m(S)-m(\cap _j A_j)\). Since \(m(S)&lt;\infty \) we may subtract it, and after multiplying by \(-1\) we obtain that \(m(A_n)\to m(\cap _j A_j)\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Let \(S=\R \), \(\Sigma =\mc {B}(\R )\) and \(m=\lambda \) be Lebesgue measure on \(\R \). Set \(A_n=(-\infty ,-n]\). Note that \(\cap _n A_n=\emptyset \)
so \(\lambda (\cap _n A_n)=0\). However, \(m(A_n)=\infty \) for all \(n\), so \(m(A_n)\nrightarrow m(\cap _n A_n)\) in this case.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-1.html#ps:atomic_sigma_field"><b>1.10</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Note that each element of \(\Pi \) is a subset of \(S\). Hence \(\Pi \) itself is a subset of the power set \(\mc {P}(S)\) of \(S\). Since \(S\) is a finite set, \(\mc
{P}(S)\) is also a finite set, hence \(\Pi \) is also finite.
</p>
<p>
<i>Part (b) requires you to keep a very clear head. To solve a question like this you have to explore what you have deduced from what else, with lots of thinking â€˜if I knew this then I would also know thatâ€™ and then
trying to fit a bigger picture together, connecting your start point to your desired end point. Analysis can often be like this. </i>
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(i)</span> Suppose \(\Pi _i\cap \Pi _j \neq \emptyset \). Note that \(\Pi _i \cap \Pi _j\) is a subset of both \(\Pi _i\) and \(\Pi _j\).
</p>
<p>
By definition of \(\Pi \), any subset of \(\Pi _i\) is either equal to \(\Pi _i\) or is equal to \(\emptyset \). Since we assume that \(\Pi _i\cap \Pi _j \neq \emptyset \), we therefore have \(\Pi _i=\Pi
_i\cap \Pi _j\). Similarly, \(\Pi _j=\Pi _i\cap \Pi _j\).
</p>
<p>
Hence \(\Pi _i=\Pi _j\), but this contradicts the fact that the \(\Pi _i\) are distinct from each other. Thus we have a contradiction and in fact we must have \(\Pi _i\cap \Pi _j = \emptyset \).
</p>
</li>
<li>


<p>
<span class="textnormal">(ii)</span> By definition of \(\Pi \) we have \(\cup _{i=1}^k \Pi _i \sw S\). Suppose \(\cup _{i=1}^k \Pi _i \neq S\). Then \(C=S\sc \cup _{i=1}^k \Pi _i\) is a
non-empty set in \(\Sigma \).
</p>
<p>
Since \(C\) is disjoint from all the \(\Pi _i\), we must have \(C\notin \Pi \). Noting that \(C\in \Sigma \), by definition of \(\Pi \) this implies that there is some<sup>1</sup> \(B_1\subset C\) such that
\(B_1\neq \emptyset \).
</p>
<p>
We have that \(B_1\) is disjoint from all the \(\Pi _i\), so we must have \(B_1\notin \Pi \). Thus by the same reasoning (as we gave for \(C\)) there exists \(B_2\subset B_1\) such that \(B_2\neq \emptyset
\). Iterating, we construct an infinite decreasing sequence of sets \(C\supset B_1\supset B_2 \supset B_3\ldots \) each strictly smaller than the previous one, none of which are empty. However, this is
impossible because \(C\sw S\) is a finite set.
</p>
</li>
<li>


<p>
<span class="textnormal">(iii)</span> Let \(i\in I\). So \(\Pi _i\cap A\neq \emptyset \). Noting that \(\Pi _i\cap A\sw \Pi _i\), by definition of \(\Pi \) we must have \(\Pi _i\cap A=\Pi _i\).
That is, \(\Pi _i\sw A\). Since we have this for all \(i\in I\), we have \(\cup _{i\in I} \Pi _i\sw A\).
</p>
<p>
Now suppose that \(A\sc \cup _{i\in I} \Pi _i\neq \emptyset \). Since by (ii) we have \(S=\cup _{i=1}^k \Pi _i\), and the union is disjoint by (i), this means that there is some \(\Pi _j\) with
\(j\notin I\) such that \(A\cap \Pi _j\neq \emptyset \). However \(A\cap \Pi _j\sw \Pi _j\) so by definition of \(\Pi \) we must have \(\Pi _j\cap A=\Pi _j\). That is \(\Pi _j\sw A\), but then we
would have \(j \in I\), which is a contraction.
</p>
<p>
Thus \(A\sc \cup _{i\in I} \Pi _i\) must be empty, and we conclude that \(A=\cup _{i\in I} \Pi _i\).
</p>
<p>


</p>
</li>
</ul>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-1.html#ps:cantor_false"><b>1.11</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Recall that \(C_n\) is the union of \(2^n\) disjoint closed intervals, each with length \(3^{-n}\), and that \(C=\cap _n C_n\), with notation as in Example <a
href="Measure-Spaces.html#ex:cantor">1.1.1</a>.
</p>
<p>
Suppose, for a contradiction, that \((a,b)\sw C\) with \(a&lt;b\). Then \((a,b)\sw C_n\) for all \(n\). Choose \(n\) such that \((\frac {2}{3})^{-n}&lt;\frac 12(b-a)\). Let us write the \(2^n\) disjoint
closed intervals making up \(C_n\) as \(I_1,\ldots ,I_{2^n}\). The point \(c=\frac {a+b}{2}\) must fall into precisely one of these intervals, say \(I_j\). Since \(I_j\) has length \((\frac {2}{3})^{-n}\),
which is less than \(\frac 12(b-a)\), we must have \(I_j\sw (a,b)\) (draw a picture!). However, \(C_{n+1}\) does not contain all of \(I_j\), because the middle part of \(I_j\) will be removed â€“ so we cannot
have \((a,b)\sw C_{n+1}\). Thus we have reached a contradiction.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> For a counterexample, consider a variant of the construction of the Cantor set, where instead of removing the middle thirds at stage \(n\), we instead remove the middle
\(1-e^{-1/n^2}\) (from each component of \(C_n\)). Then, by the same argument as in the proof of Lemma <a href="Lebesgue-Measure.html#lem:cantor_zero_meas">1.5.5</a>, we would have
</p>
<p>
\[\lambda (C)=\lim _n \lambda (C_n)=\lim _{n\to \infty } e^{-1}e^{-1/4}e^{-1/9}\ldots e^{-1/n^2}=\lim _{n\to \infty } \exp \l (-\sum _1^n \frac {1}{i^2}\r ) =\exp \l (-\sum
_1^\infty \frac {1}{n^2}\r ).          \]
</p>
<p>
We have that \(\lambda (C)\) is positive because \(\sum _1^\infty \frac {1}{n^2}&lt;\infty \).
</p>
<p>
A similar argument as in part (a) applies here, and shows that \(C\) does not contain any open intervals. The length of each interval within \(C_{n+1}\) is less than half the length of the intervals in \(C_{n}\)
(because each interval of \(C_n\) has a middle part removed to become two intervals in \(C_{n+1}\)). Thus, by a trivial induction, each of the \(2^{n}\) disjoint closed intervals in \(C_n\) has length \(\leq
(\frac 12)^{n}\). You can check that we can apply the same argument as in (v), but replacing \((\frac 23)^n\) with \((\frac 12)^{n}\).
</p>
</li>
</ul>
</li>
</ul>
<div role="note" class="footnotes">

<a id="notes-autopage-250"></a>

<p>
<sup>1</sup>&nbsp;\(X\subset Y\) means that \(X\sw Y\) and \(X\neq Y\) i.e.&nbsp;\(X\) is <i>strictly</i> smaller than the set \(Y\)
</p>



</div>
<!--
......   subsection Chapter <a href=Measurable-Functions.html#chap:measurable_funcs>2</a> ......
-->
<h5 id="autosec-251">Chapter <a href="Measurable-Functions.html#chap:measurable_funcs">2</a></h5>
<a id="notes-autopage-251"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-2.html#ps:indicator_funcs"><b>2.1</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> If \(x \in A\) and \(x \in B\), lhs \(=1\) and rhs\(=1 + 1 -1 = 1\),
</p>
<p>
If \(x \in A\) and \(x \notin B\), lhs \(=1\) and rhs\(=1 + 0 -0 = 1\),
</p>
<p>
If \(x \notin A\) and \(x \in B\), lhs \(=1\) and rhs\(=0 + 1 -0 = 1\),
</p>
<p>
If \(x \notin A\) and \(x \notin B\), lhs \(=0\) and rhs\(=0 + 0 - 0 = 0\), and so we have equality of lhs and rhs in all possible cases.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> If \(x \in A, x \notin A^{c}\) so lhs \(=1\) and rhs \(=1-0 = 1\),
</p>
<p>
if \(x \notin A, x \in A^{c}\) so lhs \(=0\) and rhs \(=1-1 = 0\).
</p>
</li>
<li>


<p>
<span class="textnormal">(c)</span> Since \(A = B \cup (A-B)\) and \(B \cap (A-B) = \emptyset \), we can apply (a) to find that \({\1}_{A} = {\1}_{B} + {\1}_{A-B}\).
</p>
</li>
<li>


<p>
<span class="textnormal">(d)</span> The lhs and rhs are both non-zero only in the case where \(x \in A\) and \(x \in B\) when both lhs and rhs are \(1\).
</p>
<p>


</p>
</li>
</ul>
<p>
For the last part, if \(x \notin A\) then \(x \notin A_{n}\) for all \(\nN \) and so lhs \(=\) rhs \(= 0\). If \(x \in A\) then \(x \in A_{n}\) for one and only one \(\nN \) and so lhs \(=\) rhs \(= 1\).
</p>
</li>
<li>


<p>
<a href="Exercises-2.html#ps:lils"><b>2.2</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Since for all \(\nN , \sup _{k \geq n}a_{k} = -\inf _{k \geq n}(-a_{k})\), we have
</p>
<p>
\[ \limsup _{n \rightarrow \infty } a_{n} = \lim _{n \rightarrow \infty }\sup _{k \geq n}a_{k} = \lim _{n \rightarrow \infty }\left (-\inf _{k \geq n}(-a_{k})\right ) = -\liminf
_{n \rightarrow \infty }(-a_{n}).\]
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Since for all \(\nN , \sup _{k \geq n}(a_{k} + b_{k}) \leq \sup _{k \geq n}a_{k} + \sup _{k \geq n}b_{k}\), the result is obtained similarly to (a) by taking
limits on both sides.
</p>
</li>
<li>


<p>
<span class="textnormal">(c)</span> Argue as in (b) noting that the inequality is reversed for \(\inf \), or use (a) and (b) to argue that
</p>
<p>
\[\begin {aligned} \li (a_{n} + b_{n}) &amp; = -\ls (-a_{n} - b_{n})\\ &amp; \geq -\ls (- a_{n}) - \ls (-b_{n})\\ &amp; = \li a_{n} + \li b_{n}.                           \end {aligned}\]
</p>
</li>
<li>


<p>
<span class="textnormal">(d)</span> Use the fact that for all \(\nN , \sup _{k \geq n}(a_{k}b_{k}) \leq \left (\sup _{k \geq n}a_{k}\right )\left (\sup _{k \geq n}b_{k}\right )\) and
argue as in (b).
</p>
</li>
<li>


<p>
<span class="textnormal">(e)</span> Use the fact that for all \(\nN , \inf _{k \geq n}(a_{k}b_{k}) \geq \left (\inf _{k \geq n}a_{k}\right )\left (\inf _{k \geq n}b_{k}\right )\) and
argue as in (d).
</p>
</li>
<li>


<p>
<span class="textnormal">(f)</span> Since \(0 \leq \li |a_{n}| \leq \ls |a_{n}| = 0\), we must have \(\li |a_{n}| = 0\) and so \(0 = \li |a_{n}| = \ls |a_{n}|\) from which it follows that
\(\lim _{n \rightarrow \infty } |a_{n}| = 0\) and hence \(\lim _{n \rightarrow \infty } a_{n} = 0\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-2.html#ps:constants_meas"><b>2.3</b></a> Fix \(a\in \R \). If \(a &lt; c, f^{-1}((a, \infty )) = S \in \Sigma \) and if \(a \geq c, f^{-1}((a, \infty )) = \emptyset
\in \Sigma \).
</p>
</li>
<li>


<p>
<a href="Exercises-2.html#ps:open_int_meas"><b>2.4</b></a> If \(f\) is measurable then, writing \((a,b)=\R \sc ((-\infty ,a]\cup [b,\infty ))\) and using the properties of pre-images,
</p>
<p>
\[f^{-1}((a,b))=f^{-1}(\R )\sc \l (f^{-1}((-\infty ,a])\cup f^{-1}([b,\infty )\r ),\]
</p>
<p>
which by Theorem <a href="Measurable-Functions-Basic-Concepts.html#thm:meas_halfints">2.2.1</a> shows that \(f^{-1}((a,b))\in \Sigma \). Note that if either \(a\) or \(b\) are infinite, the
corresponding half interval above will be the empty set (which has empty pre-image).
</p>
<p>
Conversely, suppose that we have \(f^{-1}((a,b))\in \Sigma \) for all \(-\infty \leq a&lt;b\le \infty \). Taking \(b=\infty \), we have that \(f^{-1}((a,\infty ))\in \Sigma \), which shows that
\(f\) is measurable.
</p>
</li>
<li>


<p>
<a href="Exercises-2.html#ps:add_mult_const_meas_func"><b>2.5</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> For any \(a\in \R \), we have \((f+c)^{-1}((a,\infty ))=\{x\in \R \-f(x)+c&gt;a\}=\{x\in \R \-f(x)&gt;a-c\}=f^{-1}((a-c,\infty ))\in \Sigma \) by
measurability of \(f\).
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> First note that if \(k=0\) then \((kf)(x)= 0\) for all \(x\), so in this case \(f\) is measurable by <a href="Exercises-2.html#ps:constants_meas"><b>2.3</b></a>.
</p>
<p>
Consider when \(k&gt;0\). For any \(a\in \R \), we have \((kf)^{-1}((a,\infty ))=\{x\in \R \-kf(x)&gt;a\}=\{x\in \R \-f(x)&gt;a/k\}=f^{-1}((a/k,\infty ))\in \Sigma \) by measurability of
\(f\).
</p>
<p>
For \(k&lt;0\), we can write \(kf=-(-kf)\). The function \(-kf\) is measurable by the above, because \(-k&gt;0\). Multiplying by \(-1\) to obtain \(-(-kf)\) preserves measurability by Theorem <a
href="Algebra-Measurable-Functions.html#thm:meas_plus_etc">2.4.3</a> , where we use that the constant function \(g\equiv -1\) is measurable.
</p>
<p>
<i>Follow-up exercise: Prove the \(k&lt;0\) case without using Theorem <a href="Algebra-Measurable-Functions.html#thm:meas_plus_etc">2.4.3</a>.</i>
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-2.html#ps:compose_meas"><b>2.6</b></a> \((g \circ f)^{-1}((a, \infty )) = f^{-1}(g^{-1}(a, \infty )\). Now \(g\) is Borel measurable and so \(g^{-1}((a, \infty )) = A
\in {\cal B}(\R )\). Hence by Theorem <a href="Measurable-Functions-Basic-Concepts.html#thm:meas_borel">2.2.5</a>, \(f^{-1}(A) \in \Sigma \). So we conclude that \((g \circ f)^{-1}((a,
\infty )) \in \Sigma \) and so \(g \circ f\) is measurable.
</p>
<p>
If \(X:\Omega \rightarrow \R \) is a random variable then it is a measurable function from \((\Omega , {\cal F})\) to \((\R , {\cal B}(\R ))\). If \(g:           \R \rightarrow \R \) is Borel measurable then
\(g(X) = g \circ X\) is again a random variable by what we have just shown. If \(g\) is not Borel measurable then we must be wary of interpreting \(g(X)\) as a random variable, unless we can directly prove
that it is measurable using some other technique.
</p>
</li>
<li>


<p>
<a href="Exercises-2.html#ps:translate_meas"><b>2.7</b></a> For any \(a&gt;0\), we have
</p>
<p>
\[h^{-1}((a,\infty ))=\{x\in \R \- f(x+y)&gt;(a,\infty )\}=\{z-y\in \R \-f(z)&gt;a\}=(f^{-1}((a,\infty )))_{-y}.\]
</p>
<p>
Here we use the notation \(A_y=\{a+y\-a\in A\}\) from Section <a href="The-Borel-field.html#sec:borel_field">1.4</a>. Using that \(A_y\in \mc {B}(\R )\) whenever \(A\in \mc {B}(\R )\), we
have that \(h^{-1}((a,\infty ))\in \mc {B}(\R )\), and hence \(h\) is measurable.
</p>
<p>
<i>Alternative:</i> Write \(h = f \circ \tau _{y}\) where \(\tau _{y}(x) = x + y\). The mapping \(\tau _{y}\) is continuous and hence measurable and so \(h\) is measurable by Corollary <a
href="Examples-Measurable-Functions.html#cor:meas_cts">2.3.2</a>.
</p>
</li>
<li>


<p>
<a href="Exercises-2.html#ps:abs_meas"><b>2.8</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> If \(f(x)&gt;0\) then \(f_+(x)=f(x)\) and \(f_-(x)=0\). If \(f(x)&lt;0\) then \(f_+(x)=0\) and \(f_-(x)=-f(x)\). If \(f(x)=0\) then \(f_+(x)=f_-(x)=0\). In all
cases we have \(f(x)=f_+(x)-f_-(x)\).
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> Using the same cases as in (a), in all cases we have \(|f(x)|=f_+(x)+f_-(x)\).
</p>


</li>
<li>


<p>
<span class="textnormal">(c)</span> By Theorem <a href="Algebra-Measurable-Functions.html#thm:meas_min_max_etc">2.4.1</a>, \(f_+\) and \(f_-\) are measurable whenever \(f\) is. By Theorem
<a href="Algebra-Measurable-Functions.html#thm:meas_plus_etc">2.4.3</a>, the sum of measurable function is measurable, hence \(|f|=f_++f_-\) is measurable.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-2.html#ps:diff_meas"><b>2.9</b></a> If \(f\) is differentiable then it is continuous and so measurable by Corollary <a
href="Examples-Measurable-Functions.html#cor:meas_cts">2.3.2</a>. For each \(x \in \R , f^{\prime }(x) = \lim _{h \rightarrow 0}\frac {f(x + h) - f(x)}{h}\). Now \(x \rightarrow
f(x+h)\) is measurable by Problem <a href="Exercises-2.html#ps:translate_meas"><b>2.7</b></a>, and \(x \rightarrow \frac {f(x + h) - f(x)}{h}\) is measurable by Theorem <a
href="Algebra-Measurable-Functions.html#thm:meas_plus_etc">2.4.3</a> and Problem <a href="Exercises-2.html#ps:compose_meas"><b>2.6</b></a>(b). Finally \(f^{\prime }\) is measurable by
Theorem <a href="Algebra-Measurable-Functions.html#thm:meas_lim">2.4.4</a>.
</p>
</li>
<li>


<p>
<a href="Exercises-2.html#ps:mono_meas"><b>2.10</b></a> Recall that we have shown all intervals (i.e.&nbsp;sets of the form \((a,b)\),\([a,b)\) and so on) are Borel sets. Another way to describe intervals
is that \(I\sw \R \) is an interval if, whenever \(a,b\in \R \) and \(a&lt;c&lt;b\) we have \(c\in I\).
</p>
<p>
Suppose \(f\) is monotone increasing. Fix \(c\in \R \) and consider \(I=f^{-1}((c,\infty ))\). We want to show that \(I\) is an interval. Let \(a,b\in I\), so that we have \(f(a),f(b)&gt;c\), and let
\(a&lt;d&lt;b\). We have \(a\leq d\) so, by monotonicity of \(f\) we have \(f(a)\leq f(d)\). Thus \(f(d)&gt;c\), so \(d\in I\). Hence \(I\) is an interval, so \(I\in \mc {B}(\R )\). Hence \(f\) is
measurable.
</p>
</li>
<li>


<p>
<a href="Exercises-2.html#ps:lims_meas"><b>2.11</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Let \(A\) be the set of measure zero on which \(f_{n}\) fails to converge to \(f\). Then \(\lim _{n \rightarrow } f_{n}(x) = f(x)\) for all \(x \in S-A\). But then
by algebra of limits \(\lim _{n \rightarrow } f_{n}(x)^{2} = f(x)^{2}\) for all \(x \in S-A\).
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> \(A\) be the set of measure zero on which \(f_{n}\) fails to converge to \(f\) and \(B\) be the set of measure zero on which \(g_{n}\) fails to converge to \(g\). Now
\(m(A \cup B) \leq m(A) + m(B) = 0\) and by algebra of limits \(\lim _{n \rightarrow } (f_{n}(x) + g_{n})(x) = f(x) + g(x)\) for all \(x \in S-(A \cup B)\).
</p>


</li>
<li>


<p>
<span class="textnormal">(c)</span> This follows by writing \(f_{n}g_{n} = \frac {1}{4}[(f_{n} + g_{n})^{2} - (f_{n} - g_{n})^{2}]\) and using the results of (a) and (b).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-2.html#ps:open_sets"><b>2.12</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Let \(x\in O_1\cup O_2\). Consider if \(x\in O_1\), then there is an open interval \(I_1\) containing \(x\). Thus \(I_1\) is an open interval within \(O_1\cup O_2\)
containing \(x\). We can do the same for \(x\in O_2\), then with \(x\in I_2\sw O_2\), hence \(O_1\cup O_2\) is open.
</p>
<p>
Now let \(x\in O_1\cap O_2\). Then for each \(i=1,2\) we have an open interval \(I_i\sw O_i\) containing x. Let us write \(I_1=(a_1,b_1), I_2=(a_2,b_2)\), and \(c_1=\max (a_1,b_1)\), \(c_2=\min
(a_2,b_2)\). Then \((c_1,c_2)=I_1\cap I_2\), and since \(x\in I_1\cap I_2\) we have \(x\in (c_1,c_2)\). In particular this means \(c_1&lt;c_2\), so \(I_1\cap I_2\) is an open interval. Also
\(I_1\cap I_2\sw O_1\cap O_2\), so \(O_1\cap O_2\) is open.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
1. This is true. We can use exactly the same method as in part (a): let \(x\in \cup _n O_n\), and the assume \(x\in O_1\) (or use \(O_i\) in place of \(O_1\)), then we have an open interval \(I_1\sw O_1\)
containing \(x\), then \(I_1\sw \cup _n O_n\), and we are done.
</p>


</li>
<li>


<p>
2. This is false. A counterexample is given by \(O_n=(\frac {-1}{n},1+\frac {1}{n})\), for which \(\cap _n O_n=[0,1]\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="textnormal">(c)</span> Let \((C_n)_{n\in \N }\) be a sequence of closed sets. Then \(\R \sc C_n\) is open, for each \(n\). Using set operations we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--



                                                                                          R \ (C1 âˆª C2 ) = (R \ C1 ) âˆ© (R \ C2 )

                                                                                          R \ (C1 âˆ© C2 ) = (R \ C1 ) âˆª (R \ C2 )
                                                                                                          !
                                                                                                [                 \
                                                                                           R\        Cn       =       (R \ Cn )
                                                                                                 n                n
                                                                                                          !
                                                                                                \                 [
                                                                                           R\        Cn       =       (R \ Cn )
                                                                                                 n                n



-->


<p>


\begin{align*}
\R \sc (C_1\cup C_2)&amp;=(\R \sc C_1)\cap (\R \sc C_2)\\ \R \sc (C_1\cap C_2)&amp;=(\R \sc C_1)\cup (\R \sc C_2)\\ \R \sc \l (\bigcup _n C_n\r )&amp;=\bigcap _n\l (\R \sc C_n\r
)\\ \R \sc \l (\bigcap _n C_n\r )&amp;=\bigcup _n\l (\R \sc C_n\r )
\end{align*}
The first two equations combined with part (a) tell us that both the results of part (a) carry over to closed sets: both \(C_1\cap C_2\) and \(C_1\cup C_2\) are closed.
</p>
<p>
From the fourth equation, since \(\R \sc C_n\) is open (for all \(n\)), using (b)(i) we see that \(\R \sc \l (\bigcup _n C_n\r )\) is also open, hence \(\bigcap _n C_n\) is closed.
</p>
<p>
However, we canâ€™t do the same for the third equation, because (b)(ii) was false. Instead, we can take complements of our counterexample in (b)(ii) to find a counterexample here, giving \(C_n=\R \sc (\frac
{-1}{n},1+\frac {1}{n})=(-\infty ,\frac {-1}{n}]\cup [1+\frac {1}{n},\infty )\). Then \(\cup _nC_n=(-\infty ,0)\cup (1,\infty )\) which is not closed (because its complement \([0,1]\) is
not open).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-2.html#ps:upper_sc"><b>2.13</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Its sufficient to consider the case where \(x = a\). Then for any \(\epsilon &gt; 0\) and arbitrary \(\delta , f(a - \delta ) = 0 &lt; f(a) + \epsilon = 1 +
\epsilon \) and \(f(a + \delta ) = f(a) &lt; f(a) + \epsilon = 1 + \epsilon \).
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> Its sufficient to consider the case \(x = n\) for some integer \(n\). Again for any \(\epsilon &gt; 0\) and arbitrary \(\delta , f(n-\delta ) = n-1 &lt; f(n) +
\epsilon = n + \epsilon \) and \(f(n+\delta ) = n &lt; f(n) + \epsilon = n + \epsilon \).
</p>


</li>
<li>


<p>
<span class="textnormal">(c)</span> Let \(U = f^{-1}((-\infty , a))\). We will show that \(U\) is open. Then it is a Borel set and \(f\) is measurable. Fix \(x \in U\) and let \(\epsilon = a -
f(x)\). Then there exists \(\delta &gt; 0\) so that \(|x - y| &lt; \delta \Rightarrow f(y) &lt; f(x) + \epsilon = a\) and so \(y \in U\). We have shown that for each \(x \in U\) there exists an
open interval (of radius \(\delta \)) so that if \(y\) is in this interval then \(y \in U\). Hence \(U\) is open.
</p>
</li>
</ul>
</li>
</ul>
<!--
......    subsection Chapter <a href=Lebesgue-Integration.html#chap:lebesbgue_integration>3</a> ......
-->
<h5 id="autosec-252">Chapter <a href="Lebesgue-Integration.html#chap:lebesbgue_integration">3</a></h5>
<a id="notes-autopage-252"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-3.html#ps:sf_int"><b>3.1</b></a> \(f = {\1}_{[-2,-1)} + 2{\1}_{[0,1)} + {\1}_{[1,2)}\).
</p>
<p>
\(\int _{\R }f(x)dx = 1 + 2 + 1 = 4\).
</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:sf_int_finite"><b>3.2</b></a> If \(f = \sum _{i=1}^{n}c_{i}{\1}_{A_{i}}\), then
</p>
<p>
\[f{\1}_{A} = \sum _{i=1}^{n}c_{i}{\1}_{A_{i}}{\1}_{A} = \sum _{i=1}^{n}c_{i}{\1}_{A_{i}\cap A}=\sum _{i=1}^{n}c_{i}{\1}_{A_{i}\cap A}+0\1_{S\sc A},\]
</p>
<p>
by Problem <a href="Exercises-2.html#ps:indicator_funcs"><b>2.1</b></a>(d). Note that \(\{A\cap A_1,\ldots ,A\cap A_n,S\sc A\}\) are disjoint sets, with union \(S\).
</p>
<p>
If \(f \geq 0, c_{i} \geq 0 (1 \leq i \leq n)\) and so \(f{\1}_{A} \geq 0\).
</p>
<p>
If we assume that \(m(A) &lt; \infty \), then \(I_{A}f = \sum _{i=1}^{n}c_{i}{\1}m(A_{i}\cap A)&lt; \infty \), because in this case \(m(A_{i}\cap A) &lt; m(A) &lt; \infty \) for all \(1 \leq i
\leq n\).
</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:f+f-"><b>3.3</b></a> We have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--



                                                                                           f+ = 1[âˆ’1,0) + 31[1,2) ,

                                                                                           fâˆ’ = 1[âˆ’2,âˆ’1) + 21[0,1) ,
                                                                                 Z                Z                        Z
                                                                                     f (x)dx =         f+ (x)dx âˆ’              fâˆ’ (x)dx = (1 + 3) âˆ’ (1 + 2) = 1.
                                                                                 R                 R                       R



-->


<p>


\begin{align*}
f_{+} &amp;= {\1}_{[-1,0)} + 3{\1}_{[1,2)}, \\ f_{-} &amp;= {\1}_{[-2,-1)} + 2{\1}_{[0,1)}, \\ \int _{\R }f(x)dx &amp;= \int _{\R }f_{+}(x)dx - \int _{\R }f_{-}(x)dx = (1+3) -
(1+2) = 1.
\end{align*}


</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:T331"><b>3.4</b></a> We have already proved part (1) of Theorem <a href="The-Lebesgue-Integral-Non-negative-Measurable-Functions.html#thm:bprops">3.3.1</a>.
It remains to prove parts (2)-(4).
</p>
<ul style="list-style-type:none">


<li>
<p>
(2) Let \(\alpha &gt;0\). We have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                                       Z                       

\begin{align*}
\int _{S}\alpha f dm &amp; = \sup \l \{\int _{S}s \,dm \- s\text { is simple, } 0 \leq s \leq \alpha f\r \}\\ &amp; = \sup \l \{\int _{S}s \,dm \- s\text { is simple, } 0\leq
\tfrac {1}{\alpha }s \leq f\r \}\\ &amp; = \sup \l \{\int _{S}\alpha r \,dm \- r\text { is simple, } 0\leq r \leq f\r \}\\ &amp; = \alpha \sup \l \{\int _{S} r \,dm \- r\text {
is simple, } 0\leq r \leq f\r \}\\ &amp; = \alpha \int _{S}f dm.
\end{align*}
Here, to deduce the third line we use that \(s\) is simple if and only if \(r=\frac {1}{\alpha }s\) is simple.
</p>


</li>
<li>


<p>
(3) Since \(A\sw B\) we have \(\1_A\leq \1_B\), which means \(\1_Af\leq \1_Bf\). This part now follows from part (1).
</p>


</li>
<li>


<p>
(4) We have \(m(A)=0\). Suppose \(s\) is a non-negative simple function such that \(0\leq s\leq \1_Af\), and write \(s=\sum _{i=1}^n c_i\1_{A_i}\). Hence, for any given \(i\), either \(c_i=0\) or we must
have \(A_i\sw A\), implying \(m(A_i)=0\). Thus \(\int _S s\,dm=\sum _{i=1}^n c_im(A_i)=0\). Thus \(\int _S f\,dm=0\).
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:chebychev"><b>3.5</b></a> Imitating the proof of Lemma <a href="The-Lebesgue-Integral-Non-negative-Measurable-Functions.html#lem:markov_ineq">3.3.2</a>, let
\(A = \{x \in S; |f(x)| \geq c\}\). Then
</p>
<p>
\[ \int _{S}f^{2}dm \geq \int _{A}f^{2}dm \geq c^{2}m(A),\]
</p>
<p>
and so \(m(A) \leq \frac {1}{c^{2}}\int _{S}f^{2}dm\).
</p>
<p>
The generalisation to \(p \geq 1\) is
</p>
<p>
\[ m(\{x \in S; |f(x)| \geq c\}) \leq \frac {1}{c^{p}}\int _{S}|f|^{p}dm,\]
</p>
<p>
and it is proved similarly. Note that when \(p\) is odd, we need to replace \(f\) by \(|f|\) inside the integral to ensure non-negativity.
</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:series_int"><b>3.6</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We can write
</p>
<p>
\[a^{(N)}_n=\sum \limits _{i=1}^N a_i \1_{\{i\}}(n).\]
</p>
<p>
Noting that \(\{i\}\) is measurable and \(a_i\in \R \), this is a simple function. By step 2 of the construction of the Lebesgue integral, noting that \(\#(\{i\})=1\),
</p>
<p>
\[\int _\N a^{(N)}\,d\#=\sum \limits _{i=1}^N a_i.\]
</p>
<p>
We have \(a^{(N)}\to a\) pointwise as \(N\to \infty \), and \(0\leq a^{(N)}\leq a^{(N+1)}\), so by the monotone convergence theorem
</p>
<p>
\[\int _\N a\,d\#=\lim _{N\to \infty }\int _\N a^{(N)}\,d\#=\sum \limits _{i=1}^\infty a_i,\]
</p>
<p>
as required.
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> We have that \(a\) is integrable if and only if \(|a|\) is integrable, from part (a) occurs if and only if \(\sum _n |a_n|&lt;\infty \). That is, \(a\) is integrable if and
only if it is absolutely convergent.
</p>
<p>
Lastly, writing \(a=a_+-a_-\) we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                                             Z              Z                    Z
                                                                                                  a d# =            a+ d# âˆ’           aâˆ’ d#
                                                                                              N             N                     N
                                                                                                            âˆž
                                                                                                            X                          âˆž
                                                                                                                                       X
                                                                                                        =           max(an , 0) âˆ’            max(âˆ’an , 0)
                                                                                                            n=1                        n=1
                                                                                                            Xâˆž
                                                                                                        =           max(an , 0) âˆ’ max(âˆ’an , 0)
                                                                                                            n=1
                                                                                                            Xâˆž
                                                                                                        =           an .
                                                                                                            n=1



-->


<p>


\begin{align*}
\int _\N a\,d\# &amp;=\int _\N a_+\,d\#-\int _N a_-\,d\# \\ &amp;=\sum \limits _{n=1}^\infty \max (a_n,0)-\sum \limits _{n=1}^\infty \max (-a_n,0) \\ &amp;=\sum \limits
_{n=1}^\infty \max (a_n,0)-\max (-a_n,0) \\ &amp;=\sum \limits _{n=1}^\infty a_n.
\end{align*}
Here, the third line follows from the second using absolute convergence, which allows us to rearrange infinite series.
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:T351"><b>3.7</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
(1) Using Theorem 3.3.1 (2), if \(c \geq 0\),
</p>
<p>
\[\int _{S}cf dm = \int _{S}cf_{+}dm - \int _{S}cf_{-}dm = c\int _{S}f_{+}dm - c\int _{S}f_{-}dm = c\int _{S}f dm.\]
</p>
<p>
If \(c = -1, (-f)_{+} = f_{-}\) and \((-f)_{-} = f_{+}\) and so
</p>
<p>
\[ \int _{S}(-f)dm = \int _{S}f_{-}dm - \int _{S}f_{+}dm = -\left (\int _{S}f_{+}dm - \int _{S}f_{-}dm\right ) = -\int _{S}fdm.\]
</p>
<p>
Finally if \(c &lt; 0 (c \neq -1)\) write \(c = -d\) where \(d &gt; 0\) and use the two cases weâ€™ve just proved.
</p>
</li>
<li>


<p>
(3) If \(f \leq g\) then \(g - f \geq 0\) so by Theorem 3.3.1 (1), \(\int _{S}(g-f)dm \geq 0\). But by (1) and (2) this is equivalent to \(\int _{S}gdm - \int _{S}fdm \geq 0\), i.e. \(\int _{S}gdm
\geq \int _{S}fdm\), as required.
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:abs_int"><b>3.8</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Noting \(f_+\) and \(f_-\) are both non-negative, with non-negative integrals, we have
</p>
<p>
\[\l |\int _S f\,dm\r | = \l |\int _S f_+\,dm - \int _S f_-\,dm\r | \leq \int _S f_+\,dm + \int _S f_-\,dm = \int _S |f|\,dm.\]
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> By the triangle inequality we have \(|f(x)+g(x)|\leq |f(x)|+|g(x)|\). Thus by monotonicity and linearity (from Theorem <a
href="Lebesgue-Integrability.html#thm:basicsli">3.6.2</a>) we obtain
</p>
<p>
\[\int _S |f+g|\,dm\leq \int _S |f|+|g|\,dm = \int _S |f|\,dm + \int _S |g|\,dm.\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:ae_equiv"><b>3.9</b></a> Reflexivity is obvious as \(f(x) = f(x)\) for all \(x \in S\). So is symmetry, because \(f(x) = g(x)\) almost everywhere if and only if \(g(x) =
f(x)\) almost everywhere. For transitivity, let \(A = \{x \in S; f(x) \neq g(x)\}, B = \{x \in S; g(x) \neq h(x)\}\) and \(C = \{x \in S; f(x) \neq h(x)\}\). Then \(C \subseteq A \cup B\)
and so \(m(C) \leq m(A) + m(B) = 0\). Thus if \(f=g\) a.e.&nbsp;and \(g=h\) a.e.&nbsp;we have \(f=h\) a.e.
</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:int_nonconv_1"><b>3.10</b></a> Let \(x \in \R \) be arbitrary. Then we can find \(n_{0} \in \mathbb {N}\) so that \(\frac {1}{n_{0}} &lt; |x|\) and then for all \(n
\geq n_{0}, f_{n}(x) = n{\1}_{(0, 1/n)}(x) = 0\). So we have proved that \(\lim _{n \rightarrow }f_{n}(x) = 0\). But for all \(\nN \)
</p>
<p>
\[ \int _{\R }|f_{n}(x) - 0|dx = n\int _{\R }{\1}_{(0, 1/n)}(x)dx = n.\frac {1}{n} = 1,\]
</p>
<p>
and so we cannot find any function in the sequence that gets arbitrarily close to \(0\) in the \({\cal L}_{1}\) sense.
</p>
<p>
The MCT does not apply here because \((f_n)\) is not monotone. The DCT does not apply because, if it did, then the DCT would give \(\int _\R f_n \to \int _\R f=0\) which is not true! We conclude that
there is no dominating integrable function for \((f_n)\), because all the other conditions of the DCT hold. Fatouâ€™s Lemma does apply, and would give
</p>
<p>
\[\int _\R \liminf _n f_n=\int _\R 0 \leq \liminf _n\int _\R f_n = \liminf _n 1=1\]
</p>
<p>
which we already knew because we could calculate the integrals explicitly in this case.
</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:cos_int"><b>3.11</b></a> Integrability follows easily from the facts that \(|\cos (\alpha x)| \leq 1\) and \(|\sin (\beta x)| \leq 1\) for all \(x \in \R \). As
\(|\cos (x/n)f(x)| \leq |f(x)|\) for all \(x \in \R \) and \(f\) is integrable, we may use the dominated convergence theorem to deduce that
</p>
<p>
\[ \lim _{n \rightarrow \infty }\int _{\R }\cos (x/n)f(x)dx = \int _{\R } \lim _{n \rightarrow \infty }\cos (x/n)f(x)dx = \int _{\R }f(x)dx,\]
</p>
<p>
since \(\lim _{n \rightarrow \infty }\cos (x/n) = \cos (0) = 1\) for all \(x \in \R \).
</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:int_on_incr_set"><b>3.12</b></a> First suppose that \(f{\1}_{A}\) is integrable. Then for all \(\nN , |f|{\1}_{A_{n}} \leq |f|{\1}_{A}\) and so \(f{\1}_{A_{n}}\) is
integrable by monotonicity. It follows that
</p>
<p>
\[ \sum _{r=1}^{n}\int _{S}|f|{\1}_{A_{r}}dm = \int _{S}|f|{\1}_{\bigcup _{r=1}^{n}A_{r}}dm &lt; \infty .\]
</p>
<p>
Now \(|f|{\1}_{\bigcup _{r=1}^{n}A_{r}}\) increases to \(|f|{\1}_{A}\) as \(n \rightarrow \infty \) and so by the monotone convergence theorem,
</p>
<p>
\[ \sum _{r=1}^{\infty }\int _{S}|f|{\1}_{A_{r}}dm = \lim _{n \rightarrow \infty }\int _{S}|f|{\1}_{\bigcup _{r=1}^{n}A_{r}}dm = \int _{S}|f|{\1}_{A}dm &lt; \infty .\]
</p>
<p>
Conversely if \(f{\1}_{A_{n}}\) is integrable for each \(\nN \) and \(\sum _{n=1}^{\infty }\int _{A_{n}}|f|dm &lt; \infty \), we have by Theorem <a
href="The-Lebesgue-Integral-Non-negative-Measurable-Functions.html#thm:fmeas">3.3.5</a> that
</p>
<p>
\[\begin {aligned} \int _{S}|f|{\1}_{A}dm &amp; = \int _{S}|f|{\1}_{\bigcup _{n=1}^{\infty }A_{n}}dm\\ &amp; = \sum _{n=1}^{\infty }\int _{A_{n}}|f|dm &lt; \infty .                             \end
{aligned}\]
</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:rev_fatou"><b>3.13</b></a> \(f - f_{n} \geq 0\) for all \(\nN \) so by Fatouâ€™s lemma:
</p>
<p>
\[ \li \int _{S} (f - f_{n})dm \geq \int _{S} \li (f - f_{n})dm.\]
</p>
<p>
\[\mbox {i.e.}~\int _{S}f dm + \li \int _{S}(-f_{n}) dm \geq \int _{S}f dm + \int _{S}\li (- f_{n})dm,\]
</p>
<p>
\[\mbox {and so}~ \li -\left (\int _{S}f_{n}dm\right ) \geq \int _{S}\li (- f_{n})dm.\]
</p>
<p>
Multiplying both sides by \(-1\) reverses the inequality to yield
</p>
<p>
\[ -\li -\left (\int _{S}f_{n}dm\right ) \leq \int _{S}\left (-\li (- f_{n})\right )dm.\]
</p>
<p>
But then by definition of \(\ls \) we have
</p>
<p>
\[ \ls \int _{S}f_{n}dm \leq \int _{S}\ls f_{n} dm.\]
</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:ints_cts"><b>3.14</b></a> Define \(f_{n}(x) = f(t_{n}, x)\) for each \(\nN , x \in S\). Then \(|f_{n}(x)| \leq g(x)\) for all \(x \in S\). Since \(g\) is integrable,
by dominated convergence
</p>
<p>
\[\begin {aligned} \lim _{n \rightarrow \infty }\int _{S}f(t_{n}, x)dm(x) &amp; = \int _{S}\lim _{n \rightarrow \infty }f_{n}(x)dm(x)\\ &amp; = &amp;\int _{S}\lim _{n \rightarrow
\infty }f(t_{n}, x) dm(x)\\ &amp; = \int _{S}f(t, x)dm(x), \end {aligned}\]
</p>
<p>
where we used the continuity assumption (ii) in the last step.
</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:diff_under_int"><b>3.15</b></a> Let \((h_n)\) be an arbitrary sequence such that \(h_n\to 0\) and define \(a_{n,t}(x)=\frac {f(t_n+h,x)-f(t,x)}{h_n}\).
</p>
<p>
Since \(\frac {\p f}{\p t}\) exists we have \(a_{n,t}(x)\to \frac {\p f}{\p t}(x,t)\) as \(n\to \infty \) for all \(x\). By the mean value theorem there exists \(\theta _n\in [0,1]\) such that
\(a_{n,t}(x)=\frac {\p f}{\p t}(t+\theta _n h,x)\), hence \(|f_n(x)|\leq h(x)\). Thus by dominated convergence \(\int _S a_{n,t}(x)\,dm(x)\to \int _S \frac {\p f}{\p t}(t,x)\,dm(x)\).
</p>
<p>
By linearity of the integral we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--


                                                                        âˆ‚                           1
                                                                             Z                                       Z                                 Z                
                                                                               f (t, x) dm(x) = lim                            f (t + hn , x) dm(x) âˆ’       f (t, x) dm(x)
                                                                        âˆ‚t   S                 nâ†’âˆž hn                      S                            S
                                                                                                                Z
                                                                                                      = lim          an,t (x) dm(x)
                                                                                                       nâ†’âˆž S




-->


<p>


\begin{align*}
\frac {\p }{\p t}\int _S f(t,x)\,dm(x) &amp;=\lim _{n\to \infty }\frac {1}{h_n}\l (\int _S f(t+h_n,x)\,dm(x)-\int _Sf(t,x)\,dm(x)\r )\\ &amp;=\lim _{n\to \infty }\int _S
a_{n,t}(x)\,dm(x)
\end{align*}
and the result follows.
</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:nonconv_int"><b>3.16</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> For each \(x \in \R , \nN \), the expression for \(f_{n}(x)\) is a telescopic sum. If you begin to write it out, you see that terms cancel in pairs and you obtain
</p>
<p>
\[ f_{n}(x) = -2xe^{-x^{2}} + 2(n+1)^{2}xe^{-(n+1)^{2}x^{2}}.\]
</p>
<p>
Using the fact that \(\lim _{N \rightarrow \infty }N^{2}e^{-yN^{2}} = 0\), for all \(y \in \R \) we find that
</p>
<p>
\[ \lim _{n \rightarrow \infty }f_{n}(x) = f(x) = -2xe^{-x^{2}}.\]
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> The functions \(f\) and \(f_{n}\) are continuous and so Riemann integrable over the closed interval \([0,a]\). We can calculate (which is left for you) that \(\int
_{0}^{a}f(x)dx = -2\int _{0}^{a}xe^{-x^{2}}dx = e^{-a^{2}} - 1\). But on the other hand
</p>
<p>
\[\begin {aligned} \int _{0}^{a}f_{n}(x)dx &amp; = \sum _{r=1}^{n}\int _{0}^{a}[-2r^{2}xe^{-r^{2}x^{2}} + 2(r+1)^{2}xe^{-(r+1)^{2}x^{2}}]dx\\ &amp; = \sum _{r=1}^{n}(e^{-r^{2}a} -
e^{-(r+1)^{2}a})\\ &amp; = e^{-a^{2}} - e^{-(n+1)^{2}a} \rightarrow e^{-a^{2}}~\mbox {as}~n \rightarrow \infty .                                                    \end {aligned}\]
</p>
<p>
So we conclude that \(\int _{0}^{a}f(x)dx \neq \lim _{n \rightarrow \infty }\int _{0}^{a}f_{n}(x)dx\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:fourier_linear"><b>3.17</b></a> Using the fact that \(|e^{-ixy}| \leq 1\), we get by Theorem 3.5.1,
</p>
<p>
\[ |\widehat {f}(y)| \leq \int _{\R }|e^{-ixy}|.|f(x)|dx \leq \int _{\R }|f(x)|dx &lt; \infty .\]
</p>
<p>
For the linearity, we have
</p>
<p>
\[\begin {aligned} \widehat {af + bg}(y) &amp; = \int _{\R }e^{-ixy}(a f(x) + b g(x)) dx \\ &amp; = a\int _{\R }e^{-ixy}f(x)dx + b\int _{\R }e^{-ixy}g(x) dx \\ &amp; = a \widehat
{f}(y) + b \widehat {g}(y).            \end {aligned}\]
</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:fourier_jump"><b>3.18</b></a> \(x \rightarrow {\1}_{\mathbb {Q}}(x)\cos (nx)\) is integrable as \(|{\1}_{\mathbb {Q}}(x)\cos (nx)| \leq |\cos (nx)|\) for all
\(x \in \R \) and \(x \rightarrow \cos (nx)\) is integrable. Similarly \(x \rightarrow {\1}_{\mathbb {Q}}(x)\sin (nx)\) is integrable. So the Fourier coefficients \(a_{n}\) and \(b_{n}\) are
well-defined as Lebesgue integrals. As \(|\cos (nx)| \leq 1\), we have \(a_{n} = 0\) for all \(n \in \mathbb {Z}_{+}\) since,
</p>
<p>
\[\begin {aligned} |a_{n}| &amp; \leq &amp; \frac {1}{\pi }\int _{-\pi }^{\pi }{\1}_{\mathbb {Q}}(x)|\cos (nx)|dx \\ &amp; \leq &amp; \frac {1}{\pi }\int _{-\pi }^{\pi
}{\1}_{\mathbb {Q}}(x)dx = 0.            \end {aligned}\]
</p>
<p>
By a similar argument, \(b_{n} = 0\) for all \(\nN \). So it is possible to associate a Fourier series to \({\1}_{\mathbb {Q}}\), but this Fourier series will converges to zero!
</p>
<p>
<i>This illustrates that pointwise convergence is not the right tool for examining convergence of Fourier series!</i>
</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:fourier_translate"><b>3.19</b></a> First observe that \(f_{a}\) is measurable, by Problem <a href="Exercises-2.html#ps:translate_meas"><b>2.7</b></a> (take \(y
= -a\) there). For integrability, we use
</p>
<p>
\[ \int _{\R }|f_{a}(x)|dx = \int _{\R }|f(x - a)|dx = \int _{\R }|f(x)|dx &lt; \infty .\]
</p>
<p>
Then
</p>
<p>
\[ \widehat {f_{a}}(y) = \int _{\R }e^{-ixy}f(x-a) dx,\]
</p>
<p>
and the result follows on making a change of variable \(u = x-a\).
</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:fourier_cts"><b>3.20</b></a> Let \(y \in \R \) and \((y_{n})\) be an arbitrary sequence converging to \(y\) as \(n \rightarrow \infty \). We need to show that the
sequence \((f(y_{n}))\) converges to \(f(y)\). We have
</p>
<p>
\[\begin {aligned} |\widehat {f}(y_{n}) - \widehat {f}(y)| &amp; = \left |\int _{\R }e^{-ixy_{n}}f(x)dx - \int _{\R }e^{-ixy}f(x)dx\right |\\ &amp; \leq &amp; \int _{\R
}|e^{-ixy_{n}}- e^{-ixy}|.|f(x)|dx.            \end {aligned}\]
</p>
<p>
Now \(|e^{-ixy_{n}}- e^{-ixy}| \leq |e^{-ixy_{n}}| + |e^{-ixy}| = 2\) and the function \(x \rightarrow 2f(x)\) is integrable. Also the mapping \(y \rightarrow e^{-ixy}\) is continuous, and so
\(\lim _{n \rightarrow \infty }|e^{-ixy_{n}}- e^{-ixy}| = 0\). The result follows from these two facts, and the use of Lebesgueâ€™s dominated convergence theorem.
</p>
</li>
<li>


<p>
<a href="Exercises-3.html#ps:fourier_xfx"><b>3.21</b></a> To prove that \(y \rightarrow \widehat {f}(y)\) is differentiable, we need to show that
</p>
<p>
\(\lim _{h \rightarrow 0}(\widehat {f}(y + h) - \widehat {f}(y))/h\) exists for each \(y \in \R \). We have
</p>
<p>
\[\begin {aligned} \frac {\widehat {f}(y + h) - \widehat {f}(y)}{h} &amp; = \frac {1}{h}\int _{\R }(e^{-ix(y + h)} - e^{-ixy})f(x)dx \\ &amp; = \int _{\R }e^{-ixy}\left (\frac
{e^{-ihx} - 1}{h}\right )f(x)dx.            \end {aligned}\]
</p>
<p>
Since \(|e^{-ixy}| \leq 1\), and using the hint with \(b = hx\), we get
</p>
<p>
\[\begin {aligned} \left |\frac {\widehat {f}(y + h) - \widehat {f}(y)}{h}\right | &amp; \leq &amp; \int _{\R }\left |\frac {e^{-ihx} - 1}{h}\right |.|f(x)|dx \\ &amp; \leq &amp;
\int _{\R }|x||f(x)|dx &lt; \infty .            \end {aligned}\]
</p>
<p>
Then we can use Lebesgueâ€™s dominated convergence theorem to get
</p>
<p>
\[\begin {aligned} \lim _{h \rightarrow 0}\frac {\widehat {f}(y + h) - \widehat {f}(y)}{h} &amp; = \int _{\R }e^{-ixy}\lim _{h \rightarrow 0}\left (\frac {e^{-ihx} - 1}{h}\right
)f(x)dx \\ &amp; = -i\int _{\R }e^{-ixy}xf(x)dx = -i\widehat {g}(y), \end {aligned}\]
</p>
<p>
and the result is proved. In the last step we used
</p>
<p>
\[ \lim _{h \rightarrow 0}\frac {e^{-ihx} - 1}{h} = \left .\frac {d}{dy}e^{-ixy}\right |_{y=0} = -ix.\]
</p>
<p>


</p>
</li>
</ul>
<!--
......   subsection Chapter <a href=Probability-Measure.html#chap:prob_with_meas>4</a> ......
-->
<h5 id="autosec-253">Chapter <a href="Probability-Measure.html#chap:prob_with_meas">4</a></h5>
<a id="notes-autopage-253"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-4.html#ps:cdf_thm_bit"><b>4.1</b></a> Let \((a_{n})\) be an increasing sequence that tends to \(\infty \). Define \(A_{n} = \{\omega \in \Omega ; X(\omega ) \leq a_{n}\}\).
Then \((A_{n})\) increases to \(\Omega \) and by Theorem <a href="Probability-Measure.html#thm:monotone_events_P">4.1.1</a>,
</p>
<p>
\[\lim _{x \rightarrow \infty }F(x) = \lim _{n \rightarrow \infty }\P (A_{n}) = \P (\Omega ) = 1.\]
</p>
<p>
Next let \(B_{n} = \{\omega \in \Omega ; X(\omega ) \leq - a_{n}\}\). Then \((B_{n})\) decreases to \(\emptyset \) and by Theorem <a
href="Probability-Measure.html#thm:monotone_events_P">4.1.1</a>,
</p>
<p>
\[\lim _{x \rightarrow -\infty }F(x) = \lim _{n \rightarrow \infty }\P (B_{n}) = \P (\emptyset ) = 0.\]
</p>
</li>
<li>


<p>
<a href="Exercises-4.html#ps:prob_conv_thms"><b>4.2</b></a> Monotone Convergence Theorem. Let \((X_{n})\) be an increasing sequence of non-negative random variables which converges pointwise to a
random variable \(X\), i.e. \(\lim _{n \rightarrow \infty }X_{n}(\omega ) = X(\omega )\) for all \(\omega \in \Omega \) (*). Then
</p>
<p>
\[ \lim _{n \rightarrow \infty }\E (X_{n}) = \E (X).\]
</p>
<p>
Fatouâ€™s Lemma. Let \((X_{n})\) be a sequence of non-negative random variables, then
</p>
<p>
\[ \li \E (X_{n}) \geq \E \left (\li X_{n}\right ).\]
</p>
<p>
Dominated Convergence Theorem. Let \((X_{n})\) be a sequence of random variables which converges pointwise (*) to a random variable \(X\). Suppose that there exists an integrable, non-negative random
variable \(Y\) so that \(|X_{n}(\omega )| \leq Y(\omega )\) for all \(\nN \) and all \(\omega \in \Omega \). Then \(X\) is integrable and
</p>
<p>
\[ \lim _{n \rightarrow \infty }\E (X_{n}) = \E (X).\]
</p>
<p>
(*) We can in fact replace pointwise convergence by convergence almost everywhere, i.e. \(\lim _{n \rightarrow \infty }X_{n}(\omega ) = X(\omega )\) for all \(\omega \in \Omega - A\), where \(A \in
{\cal F}\) and \(\P (A) = 0\).
</p>
</li>
<li>


<p>
<a href="Exercises-4.html#ps:EmaxXa"><b>4.3</b></a> Using the usual notation, \(a \vee b = \max \{a,b\},\)
</p>
<p>
\[\E (\max \{X,a\}) = \int _{\R } (x \vee a) dp_{X}(x).\]
</p>
<p>
Since \(x \vee a \geq x\) and \(x \vee a \geq a\), by monotonicity
</p>
<p>
\(\int _{\R } (x \vee a) dp_{X}(x) \geq \int _{\R } x dp_{X}(x) = \E (X)\) and \(\int _{\R } (x \vee a) dp_{X}(x) \geq \int _{\R } a dp_{X}(x) = a\), and so
</p>
<p>
\[ \E (\max \{X,a\}) \geq \max \{\E (X), a\}.\]
</p>
</li>
<li>


<p>
<a href="Exercises-4.html#ps:chebychev_2"><b>4.4</b></a> This follows immediately from the result of exercise <a href="Exercises-3.html#ps:chebychev"><b>3.5</b></a>, when you replace \(f\) by
\(X - \mu \) and rewrite in probability notation.
</p>
</li>
<li>


<p>
<a href="Exercises-4.html#ps:indep_uncor"><b>4.5</b></a> We have \(XY=U^2V(1-V)=0\) because \(V(1-V)=0\), hence \(\E [XY]=0\). Note that \(\E [U]=0\). By independence of \(U\) and \(V\), \(\E
[X]=\E [U]\E [V]=0\) and \(\E [Y]=\E [U]\E [1-V]=0\). Hence \(\E [XY]=\E [X]\E [Y]\).
</p>
<p>
To see that \(X\) and \(Y\) are not independent, note that \(\{X=0\}=\{V=0\}\) and \(\{Y=0\}=\{V=1\}\). Thus \(\P [X=Y=0]=0\) but that \(\P [X=0]=\P [Y=0]\frac 12\), so \(\P [X=Y=0]\neq \P
[X=0]\P [Y=0]\).
</p>
</li>
<li>


<p>
<a href="Exercises-4.html#ps:indep_inf"><b>4.6</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Define \(B_n=\cap _{i=1}^n A_i\). Then \((B_n)\) is a decreasing sequence of sets and, since \(\P \) is a finite measure, by Theorem <a
href="Probability-Measure.html#thm:monotone_events_P">4.1.1</a> we have \(\P [B_n]\to \P [\cap _{i=1}^\infty B_i]\) as \(n\to \infty \). Since \(\cap _{i=1}^\infty A_i=\cap
_{i=1}^\infty B_i\) we thus have \(\P [\cap _{i=1}^\infty A_i]=\lim _{n\to \infty }\P [\cap _{i=1}^n A_i]\). Using independence on the right hand side, we obtain
</p>
<p>
\[\P [\cap _{i=1}^\infty A_i]=\lim _{n\to \infty }\P [A_1]\P [A_2]\ldots \P [A_n]=\prod _{i=1}^\infty \P [A_i]\]
</p>
<p>
as required. Note that the limit on the right hand side exists because \(\P [A_1]\P [A_2]\ldots \P [A_n]\) is decreasing as \(n\) increases.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> If \(\P (A_{n}) &lt; 1-\kappa \) for infinitely many \(n\), where \(\kappa &gt;0\) does not depend on \(n\), then \(\prod _{n=1}^{\infty }\P (A_{n}) = 0\) so
\(\P \left (\bigcap _{n=1}^{\infty }A_{n}\right ) = \prod _{n=1}^{\infty }\P (A_{n})\) would hold in, for example, the case where all the \((A_n)\) were disjoint. Disjoints events are always
<i>dependent</i> (because if one occurs then all the others do not!), so clearly this â€˜alternativeâ€™ definition is not what want.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-4.html#ps:indep_exts"><b>4.7</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We have \(\P [A\cap B]=\P [A]\cap \P [B]\). Noting that \(A^c\cap B^c=(A\cup B)^c\), we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--



                                                                                 P[Ac âˆ© B c ] = P[(A âˆª B)c ] = 1 âˆ’ P[A âˆª B]

                                                                                                           = 1 âˆ’ P[A] âˆ’ P[B] + P[A âˆ© B]

                                                                                                           = 1 âˆ’ P[A] âˆ’ P[B] âˆ’ P[A]P[B]

                                                                                                           = (1 âˆ’ P[A])(1 âˆ’ P[B])

                                                                                                           = P[Ac ]P[B c ].



-->


<p>


\begin{align*}
\P [A^c\cap B^c]=\P [(A\cup B)^c] &amp;=1-\P [A\cup B]\\ &amp;=1-\P [A]-\P [B]+\P [A\cap B]\\ &amp;=1-\P [A]-\P [B]-\P [A]\P [B]\\ &amp;=(1-\P [A])(1-\P [B])\\ &amp;=\P [A^c]\P
[B^c].
\end{align*}
Hence \(A^c\) and \(B^c\) are independent.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> If \(A,B \in {\cal B}(\R )\) and \(f,g\) are Borel measurable, then \(f^{-1}(A), g^{-1}(B) \in {\cal B}(\R )\) and so
</p>
<p>
\[\begin {aligned} \P (f(X) \in A, g(Y) \in B) &amp; = \P (X \in f^{-1}(A), Y \in g^{-1}(B))\\ &amp; = \P (X \in f^{-1}(A))\P (Y \in g^{-1}(B))\\ &amp; = \P (f(X) \in A)\P (g(Y)
\in B). \end {aligned}\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-4.html#ps:BC2_baby_version"><b>4.8</b></a> Since \(\P [E_n]\geq \eps \) we have \(\P [\Omega \sc E_n]\leq 1-\eps \). Hence for all \(N\in \N \) we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--


                                                                                                           h            i
                                                                                             P[âˆªn En ] â‰¥ P âˆªN
                                                                                                            n=1 En
                                                                                                                h                 i
                                                                                                       = 1 âˆ’ P âˆ©N
                                                                                                                n=1 â„¦ \ En
                                                                                                               N
                                                                                                               Y
                                                                                                       =1âˆ’           P[â„¦ \ En ]
                                                                                                               n=1

                                                                                                       â‰¥ 1 âˆ’ (1 âˆ’ Ïµ)N .



-->


<p>


\begin{align*}
\P [\cup _n E_n] &amp;\geq \P \l [\cup _{n=1}^N E_n\r ] \\ &amp;= 1-\P \l [\cap _{n=1}^N \Omega \sc E_n\r ] \\ &amp;= 1-\prod _{n=1}^N\P [\Omega \sc E_n] \\ &amp;\geq 1-(1-\eps
)^N.
\end{align*}
In the above the third line is obtained from the second using independence of the \((E_n)\). As this holds for all \(N\in \N \) we obtain that \(\P \l [\cup _{n} E_n\r ]=1.\)
</p>
</li>
<li>


<p>
<a href="Exercises-4.html#ps:E_P_tail_bound"><b>4.9</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> If \(X=k\) and \(k\in \N \) then
</p>
<p>
\[\sum \limits _{n=1}^\infty \1_{\{X\geq n\}}=\sum \limits _{n=1}^\infty \1_{\{k\geq n\}}=k=X\]
</p>
<p>
because the first \(k\) terms of the sum are \(1\) and the rest are \(0\). Since \(X\) only takes values in \(\N \), we have
</p>
<p>
\[X=\sum _{n=1}^\infty \1_{\{X\geq n\}}.\]
</p>
<p>
By monotone convergence
</p>
<p>
\[ \E [X] = \E \l [\sum _{n=1}^{\infty }{\1}_{\{X \geq n\}}\r ] = \sum _{n=1}^{\infty }\E [{\1}_{\{X \geq n\}}] = \sum _{n=1}^{\infty }\P [X \geq n].\]
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Let \(X_1=\lfloor Y\rfloor \) and \(X_2=\lceil Y\rceil \), that is \(Y\) rounded up and down (respectively) to the nearest integer. We can apply part (a) to both
\(X_1\) and \(X_2\), since they take values in \(\N \cup \{0\}\).
</p>
<p>
Note that for \(n\in \N \) we have \(X_1\geq n\) if and only if \(Y\geq n\). Hence
</p>
<p>
\[\sum \limits _{n=1}^\infty \P [Y\geq n] =\sum \limits _{n=1}^\infty \P [X_1\geq n] =\E [X_1] \leq \E [Y].\]
</p>
<p>
Here, the last line follows by monotonicity, since \(X_1\leq Y\).
</p>
<p>
For \(X_2\) we need to be slightly more careful. We have \(Y\leq X_2\leq Y+1\), hence \(\P [X_2\geq k]\leq \P [Y+1\geq k]\). Hence
</p>
<p>
\[ \E [Y]\leq \E [X_2] =\sum \limits _{n=1}^\infty \P [X_2\geq n] \leq \sum \limits _{n=1}^\infty \P [Y+1\geq n] = \1_{\{Y\geq 0\}}+\sum \limits _{n=1}^\infty \P [Y\geq n] =
1+\sum \limits _{n=1}^\infty \P [Y\geq n].            \]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-4.html#ps:countable_atoms"><b>4.10</b></a> From Theorem <a href="The-Cumulative-Distribution-Function.html#thm:cdf">4.2.1</a> we have that \(x\mapsto F_X(x)=\P
[X\leq x]\) is right-continuous and monotone increasing. At each \(x\) such that \(\P [X=x]&gt;0\) the function \(x\mapsto F_X(x)\) has an upwards jump. The region it jumps through is
\(Q_x=(F_X(x-),F_X(x))\), which is non-empty open interval, and in particular contains a rational number \(q_x\) (in fact, infinitely many rationals, but one will do).
</p>
<p>
Hence, for each \(x\) with \(\P [X=x]\) we have a rational number \(q_x\), and because \(F\) is increasing we have \(q_x\neq q_y\) whenever \(x\neq y\). Therefore \(x\mapsto q_x\) is an injective map from
\(\{x\in \R \-\P [X=x]&gt;0\}\) to \(\Q \). Since \(\Q \) is countable, so is \(\{x\in \R \-\P [X=x]&gt;0\}\).
</p>
</li>
<li>


<p>
<a href="Exercises-4.html#ps:cauchy_schwarz"><b>4.11</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> By linearity, the quadratic function \(g(t) = \E (X^{2}) + 2t\E (XY) + t^{2}\E (Y^{2}) \geq 0\) for all \(t \in \R \). A non-negative quadratic function has at
most one real root, and hence has a non-positive discriminant (i.e.&nbsp;\(b^2-4ac\leq 0\)). Hence \(4\E (XY)^{2} - 4\E (X^{2})\E (Y^{2})\leq 0\) and the result follows.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Put \(Y=1\) in the Cauchy-Schwarz inequality from (a) to get \(\E (|X|) \leq \E (X^{2})^{\frac {1}{2}} &lt; \infty \). So \(X\) is integrable. By Problem <a
href="Exercises-3.html#ps:abs_int"><b>3.8</b></a> part (a) \(|\E (X)| \leq \E (|X|)\). Combining our two inequalities gives \(|\E (X)|^{2} \leq \E (X^{2})\).
</p>
</li>
<li>


<p>
<span class="textnormal">(c)</span> If \(\E [X^2]&lt;\infty \) then by part (b) \(X\) is also integrable, so by linearity we have
</p>
<p>
\[ \var (X) = \E [(X - \mu )^{2}] = \E [X^{2}] - 2\mu \E [X] + \mu ^{2} = \E [X^{2}] - \mu ^{2}.\]
</p>
<p>
Hence \(\var (X)&lt;\infty \).
</p>
<p>
Conversely, suppose that \(\var (X)&lt;\infty \), and note that by assumption we also have \(\E [X]&lt;\infty \). We can write \(X^2=(X-\E [X])^2+2X\E [X]-\E [X]^2\) and note that all terms here are
integrable by our assumptions, thus
</p>
<p>
\[\E [X^2]=\var (X)+2\E [X]\E [X]-\E [X]^2=\var (X)-\E [X]^2.\]
</p>
<p>
Hence \(\E [X^2]\) is finite.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-4.html#ps:exp_moments"><b>4.12</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Since \(e^{-ax} \leq 1\) for all \(x \geq 0\) we have
</p>
<p>
\[ \E (e^{-aX}) = \int _{0}^{\infty }e^{-ax}dp_{X}(x) \leq \int _{0}^{\infty }dp_{X}(x) = 1.\]
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span>
</p>
<p>
\[\begin {aligned} \E (e^{a|X|} &amp; = \frac {1}{\sqrt {2 \pi }}\int _{\R }e^{u|y|}e^{-\frac {1}{2}y^{2}}dy \\ &amp; = \frac {1}{\sqrt {2 \pi }}\int _{-\infty
}^{0}e^{-uy}e^{-\frac {1}{2}y^{2}}dy + \frac {1}{\sqrt {2 \pi }}\int _{0}^{\infty }e^{uy}e^{-\frac {1}{2}y^{2}}dy \\ &amp; = 2.\frac {1}{\sqrt {2 \pi }}\int _{0}^{\infty
}e^{uy}e^{-\frac {1}{2}y^{2}}dy \\ &amp; = 2e^{\frac {1}{2}a^{2}}\frac {1}{\sqrt {2 \pi }}\int _{0}^{\infty }e^{-\frac {1}{2}(y-a)^{2}}dy \\ &amp; = 2e^{\frac {1}{2}a^{2}}\frac
{1}{\sqrt {2 \pi }}\int _{-a}^{\infty }e^{-\frac {1}{2}y^{2}}dy \\ &amp; = 2e^{\frac {1}{2}a^{2}}\P (X &gt; -a) &lt; \infty .                         \end {aligned}\]
</p>
<p>
[Note that the same argument can be used to establish that \(X\) has the moment generating function \(E(e^{aX}) = e^{\frac {1}{2}a^{2}}\).
</p>
</li>
<li>


<p>
<span class="textnormal">(c)</span> Using the fact that \(e^{a|x|} = \sum _{n=0}^{\infty }\frac {a^{n}|x|^{n}}{n!}\) for all \(x \in \R \) we see that for each \(\nN , |x|^{n} \leq \frac
{n!}{a^{n}}e^{a|x|}\) and so by monotonicity:
</p>
<p>
\[ \E (|X|^{n}) \leq \frac {n!}{a^{n}}\E (e^{a|X|}) &lt; \infty .\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-4.html#ps:E_int"><b>4.13</b></a> If \(f\) is an indicator function: \(f = {\1}_{A}\) for some \(A \in {\cal B}(\R )\):
</p>
<p>
\[ \int _{\Omega }{\1}_{A}(X(\omega ))d\P (\omega ) = \P (X \in A) = p_{X}(A) = \int _{\R }{\1}_{A}(x)p_{X}(dx),\]
</p>
<p>
and so the result holds in this case. It extends to simple functions by linearity. If \(f\) is non-negative and bounded
</p>
<p>
\[\begin {aligned} \int _{\Omega }f(X(\omega ))d\P (\omega ) &amp; = \sup \left \{\int _{\Omega }g(\omega )d\P (\omega ); g~\mbox {simple on}~\Omega , 0 \leq g \leq f \circ
X\right \}\\ &amp; = \sup \left \{\int _{\Omega }h(X(\omega ))d\P (\omega ); h~\mbox {simple on}~\R , 0 \leq h \circ X \leq f \circ X\right \}\\ &amp; = \sup \left \{\int _{\R
}h(x)p_{X}(dx); h~\mbox {simple},~0 \leq h \leq f\right \}\\ &amp; = \int _{\R }f(x)dp_{X}(x).\end {aligned}\]
</p>
<p>
In the general case write \(f = f_{+} - f_{-}\) and similarly for \(X\) (details left for you).
</p>
<p>
If \(f\) is non-negative but not necessarily bounded, the result still holds but both \(\int _\Omega f(X(\omega )\,d\P (\omega )\) and \(\int _\R f(x)\,dp_X(x)\) may be (simultaneously) infinite.
</p>
</li>
<li>


<p>
<a href="Exercises-4.html#ps:Omega_uncountable"><b>4.14</b></a> Suppose that there exists \(\omega \in \Omega \) such that \(\P [\omega ]&gt;0\). Define a sequence of events \((E&apos;_n)\) by
setting \(E&apos;_n=E_n\) if \(\omega \notin E_n\) and \(E&apos;_n=\Omega \sc E_n\) if \(\omega \in E_n\). Clearly \(\omega \notin \cup _{n\in \N } E&apos;_n\). By part (a) exercise <a
href="Exercises-4.html#ps:indep_exts"><b>4.7</b></a> the events \((E&apos;_n)_{n\in \N }\) are independent of one another. We have \(\P [E&apos;_n]\in (\eps ,1-\eps )\) for all \(n\in \N \) so
from exercise <a href="Exercises-4.html#ps:BC2_baby_version"><b>4.8</b></a> we have that \(\P [\cup _{n\in \N } E&apos;_n]=1\). However \(\omega \notin \cup _{n\in \N } E&apos;_n\) and
\(\P [\omega ]&gt;0\), so this is a contradiction to \(\P [\Omega ]=1\). Hence \(\P [\omega ]=0\) for all \(\omega \in \Omega \).
</p>
<p>
For the last part, if \(\Omega \) was countable then we could write \(\Omega =\{\omega _1,\omega _2,\ldots \}\) and by definition of a measure we would have \(1=\P [\Omega ]=\sum _{i\in \N }\P
[\omega _i]\). Hence at least one \(\omega _i\) must satisfy \(\P [\omega _i]&gt;0\), but we have already shown that this may not happen.
</p>
</li>
</ul>
<!--
......     subsection Chapter <a href=Sequences-random-variables.html#chap:rv_sequences>5</a> ......
-->
<h5 id="autosec-254">Chapter <a href="Sequences-random-variables.html#chap:rv_sequences">5</a></h5>
<a id="notes-autopage-254"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-5.html#ps:D_not_P"><b>5.1</b></a> The sequence is assumed independent and identically distributed, which means that \(\P [X_n\leq x]=\P [X_1\leq x]\) for all \(x\), and in
particular \(\P [X_n\leq x]\to \P [X_1\leq x]\). Thus \(X_n\stackrel {d}{\to } X\) in distribution.
</p>
<p>
Let \(a\in (0,1]\). Since \(X_n\) only takes the value \(0\) and \(1\), \(|X_n-X_1|\) only takes the values \(0\) and \(1\). Thus \(\{|X_n-X|&gt;a\}=\{|X_n-X|=1\}=\{X_n=1,X_1=0\}\cup
\{X_n=0,X_1=1\}\). For \(n&gt;1\), since \(X_n\) and \(X_1\) are independent we thus have
</p>
<p>
\[\P [|X_n-X|&gt;a]=\P [X_n=1,X_1=0]+\P [X_n=0,X_1=1]=\frac 12\frac 12+\frac 12\frac 12=\frac 12\]
</p>
<p>
which does not tend to zero as \(n\to \infty \). Thus \(X_n\) does not converge to \(X\) in probability.
</p>
</li>
<li>


<p>
<a href="Exercises-5.html#ps:as_borel_cantelli"><b>5.2</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We have
</p>
<p>
\[\E [|X_n-0|]=\E [X_n]=n\frac {1}{n^2}+0\l (1-\frac 1{n^2}\r )=\frac {1}{n^2}\to 0\]
</p>
<p>
so \(X_n\stackrel {L^1}{\to }0\). Since \(\sum \frac {1}{n^2}&lt;\infty \), by the second Borel-Cantelli lemma we have \(\P [X_n=n\text { i.o.}]=0\). Since \(X_n\) is either equal to \(n^2\) or
\(0\), this means that \(\P [X_n=0\text { e.v.}]=1\). Thus \(X_n\stackrel {a.s.}{\to } 0\).
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> We have
</p>
<p>
\[\E [|X_n-0|]=\E [X_n]=n\frac {1}{n}+0\l (1-\frac 1{n}\r )=1\]
</p>
<p>
which does not tend to zero, so \(X_n\) does not converge to \(0\) in \(L^1\). Since \(\sum \frac {1}{n}=\infty \) and the \(X_n\) are independent, by the second Borel-Cantelli lemma we have \(\P
[X_n=n\text { i.o.}]=1\). Thus \(X_n\) does not convergence almost surely to \(0\).
</p>


</li>
<li>


<p>
<span class="textnormal">(c)</span> We have
</p>
<p>
\[\E [|X_n-0|]=\E [X_n]=n^2\frac {1}{n^2}+0\l (1-\frac 1{n^2}\r )=1\]
</p>
<p>
which does not tend to zero, so \(X_n\) does not converge to \(0\) in \(L^1\). Since \(\sum \frac {1}{n^2}&lt;\infty \), by the second Borel-Cantelli lemma we have \(\P [X_n=n^2\text { i.o.}]=0\).
Since \(X_n\) is either equal to \(n^2\) or \(0\), this means that \(\P [X_n=0\text { e.v.}]=1\). Thus \(X_n\stackrel {a.s.}{\to } 0\).
</p>


</li>
<li>


<p>
<span class="textnormal">(d)</span> We have
</p>
<p>
\[\E [|X_n-0|]=\E [X_n]=\sqrt {n}\frac {1}{n}+0\l (1-\frac 1{n}\r )=\frac {1}{\sqrt {n}}\to 0\]
</p>
<p>
so \(X_n\stackrel {L^1}{\to }0\). Since \(\sum \frac {1}{n}=\infty \) and the \(X_n\) are independent, by the second Borel-Cantelli lemma we have \(\P [X_n=\sqrt {n}\text { i.o.}]=1\). Thus
\(X_n\) does not convergence almost surely to \(0\).
</p>


</li>
<li>


<p>
<span class="textnormal">(e)</span> In cases (a), (c) and (d) this follows from Lemma <a href="Convergence-Random-Variables.html#lem:conv_modes">5.2.1</a>. For case (b), since \(X_n\) only takes
the values \(0\) and \(n\) we have that \(\{|X_n-0|&gt;a\}=\{X_n=n\}\) whenever \(a&lt;n\), in which case \(\P [|X_n-0|&gt;a]=\P [X_n=n]=\frac {1}{n}\to 0\) as \(n\to \infty \). Thus
\(X_n\stackrel {\P }{\to } 0\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-5.html#ps:P_not_as"><b>5.3</b></a> Let us first show that \(X_n\stackrel {\P }{\to }0\). Given any \(\epsilon &gt; 0\) and \(c &gt; 0\) we can find \(m\in \N \) such that
\(\frac {1}{2^{m}c} &lt; \epsilon \). The key point is that for \(n&gt;2^m\) the length of the interval \(A_n\) is less than or equal to \(\frac {1}{2^m}\), and since our probability measure is Lebesgue
measure this gives \(\E [1_{A_n}]\leq \frac {1}{2^m}\). Hence, for all \(n &gt; 2^m\), by Markovâ€™s inequality
</p>
<p>
\[ \P [|X_{n} - 0| &gt; c] = \P [{\1}_{A_{n}} &gt; c] \leq \frac {\E [{\1}_{A_{n}}]}{c} &lt; \frac {1}{2^{m}c} &lt; \epsilon .\]
</p>
<p>
On the other hand \((X_{n})\) cannot converge to \(0\) almost surely since given any \(\nN \), we can find \(m &gt; n\) so that \(A_{m}\) and \(A_{n}\) are disjoint, in which case for any \(\omega \in
\Omega \) we have \(X_n(\omega )-X_m(\omega )={\1}_{A_{n}}(\omega ) - {\1}_{A_{m}}(\omega )\) is equal to either \(1-0\) or \(0-1\). In either case, \(|X_n(\omega )-X_m(\omega )|=1\). Since \(n\)
was arbitrary and \(m\geq n\), this means \(X_n(\omega )\) cannot converge (to anything) as \(n\to \infty \). In particular, there is no almost sure convergence to zero.
</p>
<p>
<i>The best way to think about this question is to rewrite it in terms of probability. Lebesgue measure on \([0,1]\) is the distribution of a uniform random variable \(U\). Then \(X_n=\1_{A_n}\) is equal to \(1\)
if that uniform random variable falls into \(A_n\), and zero otherwise. Fix some sampled value for \(U\), and then think about how the sequence \(X_n\) will behave. </i>
</p>
</li>
<li>


<p>
<a href="Exercises-5.html#ps:indep_coin_runs"><b>5.4</b></a> Let \(E_{m}\) be the event that starting at the \(m\)th toss, \(k\) consecutive heads appear. Then \(\P [E_{m}] = 1/2^{k}\). Set
\(A_n=E_{m+kn}\) and then the \((A_n)\) are independent. Moreover, \(\sum _{r=1}^{\infty }\P [A_n] = \infty \), so by the second Borel-Cantelli lemma \(\P [A_n\text { i.o.}]=1\).
</p>
</li>
<li>


<p>
<a href="Exercises-5.html#ps:ev_io"><b>5.5</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> You might reasonably think that this is obvious - if \((A_n)\) occurs eventually then it occurs for all \(n\) after some \(N\), and of course there are infinitely many such
\(n\) so then \((A_n)\) occurs infinitely often. Letâ€™s give a proof anyway.
</p>
<p>
Suppose \(\omega \in \{A_n\text { e.v.}\}=\bigcup _m\bigcap _{n\geq m} A_n\). Then, for at least one value of \(m\), we have \(\omega \in A_n\) for all \(n\geq m\). Take any \(k\in \N \) and pick
some \(n\geq \max (m,k)\). Then \(\omega \in \bigcup _{i\geq k}A_i\), but this holds for all \(k\), which implies \(\omega \in \bigcap _k\bigcup _{i\geq k} A_i=\{A_i\text { i.o.}\}\).
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> By the laws of set algebra we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                                   ï£«       ï£¶    ï£«   ï£«      ï£¶ï£¶
                                                                                    \ [       [       [         [ \
                                                               â„¦ \ {An i.o.} = â„¦ \ ï£­    An ï£¸ = ï£­â„¦ \ ï£­   An ï£¸ï£¸ =     â„¦ \ An = {â„¦ \ An e.v.}.
                                                                                       m nâ‰¥m           m          nâ‰¥m           m nâ‰¥m



-->


<p>


\begin{align*}
\Omega \sc \{A_n\text { i.o.}\} =\Omega \sc \l (\bigcap _m\bigcup _{n\geq m}A_n\r ) =\bigcup _m\l (\Omega \sc \l (\bigcup _{n\geq m}A_n\r )\r ) =\bigcup _m\bigcap _{n\geq
m}\Omega \sc A_n =\{\Omega \sc A_n\text { e.v.}\}.
\end{align*}
It follows immediately that \(1-\P [A_n\text { i.o.}]=\P [A_n^c\text { e.v.}].\)
</p>
</li>
<li>


<p>
<span class="textnormal">(c)</span> Define \(B_m=\cap _{n\geq m} A_n\). Note that \(B_m\) is increasing. Note that \(\P [B_m]\leq \P [A_m]\) because \(B_m\sw A_m\). Thus by Theorem <a
href="Probability-Measure.html#thm:monotone_events_P">4.1.1</a> we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>
<!--


        P[An e.v.] = P[âˆªm Bm ] = lim P[Bm ] = lim inf P[Bm ] â‰¤ lim inf P[Am ].        (B.1)                                                                                        --><a id="eq:evbound"></a><!--
                                 mâ†’âˆž            mâ†’âˆž              mâ†’âˆž

-->
<p>


\begin{equation}
\label {eq:evbound} \P [A_n\text { e.v.}]=\P [\cup _m B_m]=\lim _{m\to \infty }\P [B_m]=\liminf _{m\to \infty }\P [B_m]\leq \liminf _{m\to \infty }\P [A_m].
\end{equation}


</p>
<p>
In the above, we must switch from \(\lim \) to \(\liminf \) before using \(\P [B_m]\leq \P [A_m]\), because we cannot be sure if \(\lim _n\P [A_n]\) exists (and in general it will not).
</p>
<p>
Using (b), we then have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{1}\)</span>
<!--


                                                       P[An i.o.] = 1 âˆ’ P[Acn e.v.] â‰¥ 1 âˆ’ lim inf P[Acm ] = 1 âˆ’ lim inf (1 âˆ’ P[Am ]) = âˆ’ lim inf âˆ’P[Am ] = lim sup P[Am ].
                                                                                              mâ†’âˆž               mâ†’âˆž                      mâ†’âˆž                mâ†’âˆž
                                                                                      (B.2)                                                                                       --><a id="eq:iobound"></a><!--
-->
<p>


\begin{equation}
\label {eq:iobound} \P [A_n\text { i.o.}]=1-\P [A_n^c\text { e.v.}]\geq 1-\liminf _{m\to \infty }\P [A^c_m]=1-\liminf _{m\to \infty }(1-\P [A_m])=-\liminf _{m\to \infty }-\P
[A_m]=\limsup _{m\to \infty }\P [A_m].
\end{equation}


</p>
<p>
Note that \(\liminf _{m\to \infty }\P [A_m]\leq \limsup _{m\to \infty }\P [A_m]\) holds automatically, from <span class="textup">(<a href="Measurable-Functions.html#li">2.1</a>)</span>
and <span class="textup">(<a href="Measurable-Functions.html#prop:ls">2.2</a>)</span>. Putting <span class="textup">(<a href="Solutions-exercises.html#eq:iobound">B.2</a>)</span> and
<span class="textup">(<a href="Solutions-exercises.html#eq:iobound">B.2</a>)</span> together completes the argument.
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-5.html#ps:wlln_uncor"><b>5.6</b></a> Without loss of generality (as in the argument given for the general case) we may assume that \(\E (X_{n}) = 0\) for all \(\nN \). If this is not
the case, we consider \(X_{n} - \mu \) in place of \(X_{n}\).
</p>
<p>
The proof proceeds in exactly the same way as when the random variables are independent, but needs the following extra calculation:
</p>
<p>
\[\begin {aligned} \var (\overline {X}) &amp; = \frac {1}{n^{2}}\E \left (\left (\sum _{i=1}^{n}X_{i}\right )^{2}\right )\\ &amp; = \frac {1}{n^{2}}\sum _{i=1}^{n}\sum
_{j=1}^{n}\E (X_{i}X_{j})\\ &amp; = \frac {1}{n^{2}}\sum _{i=1}^{n}\E (X_{i}^{2})\\ &amp; = \frac {\sigma ^{2}}{n}.                                \end {aligned}\]
</p>
</li>
<li>


<p>
<a href="Exercises-5.html#ps:unique_limit"><b>5.7</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We need to show that \(X\) and \(Y\) have the same distribution (i.e.&nbsp;they have the same distribution functions \(F_X(x)=F_Y(x)\)).
</p>
<p>
If \(x\in \R \) is such that \(\P [X=x]=0\) then we have both \(\P [X_n\leq x]\to \P [X\leq x]\) and \(\P [X_n\leq x]\to \P [Y\leq x]\), so by uniqueness of limits for real sequences we have \(\P
[X\leq x]=\P [Y\leq x]\).
</p>
<p>
By exercise <a href="Exercises-4.html#ps:countable_atoms"><b>4.10</b></a> there are at most countably many \(x\in \R \) such that \(\P [X=x]&gt;0\). Therefore, for all but a countable set of \(x\in
\R \), we have \(F_X(x)=F_Y(x)\). From Theorem <a href="The-Cumulative-Distribution-Function.html#thm:cdf">4.2.1</a> we have that both \(F_X\) and \(F_Y\) are right continuous. Hence, in fact,
\(F_X(x)=F_Y(y)\) at all \(x\in \R \).
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> By definition of convergence in probability, for any \(a&gt;0\), for any \(\epsilon &gt;0\) there exists \(N\in \N \) such that, for all \(n\geq N\),
</p>
<p>
\[\P [|X_n-X|&gt;a]&lt;\epsilon \hspace {1pc}\text { and }\hspace {1pc}\P [|X_n-Y|&gt;a]&lt;\epsilon .\]
</p>
<p>
By the triangle inequality we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{2}\)</span>
<!--


                                                                     P[|X âˆ’ Y | > 2a] = P[|X âˆ’ Xn + Xn âˆ’ Y | > 2a] â‰¤ P[|X âˆ’ Xn | + |Xn âˆ’ Y | > 2a].
                                                                              (B.3)                                                                                          --><a id="eq:ps_uniq_limit"></a><!--
-->
<p>


\begin{equation}
\label {eq:ps_uniq_limit} \P [|X-Y|&gt;2a]=\P [|X-X_n+X_n-Y|&gt;2a]\leq \P [|X-X_n|+|X_n-Y|&gt;2a].
\end{equation}


</p>
<p>
If \(|X-X_n|+|X_n-Y|&gt;2a\) then \(|X-X_n|&gt;a\) or \(|X_n-Y|&gt;a\) (or possibly both). Hence, continuing <span class="textup">(<a
href="Solutions-exercises.html#eq:ps_uniq_limit">B.3</a>)</span>,
</p>
<p>
\[\P [|X-Y|&gt;2a]\leq \P [|X_n-X|&gt;a]+\P [|X_n-Y|&gt;a]\leq 2\epsilon .\]
</p>
<p>
Since this is true for any \(\epsilon &gt;0\) and any \(a&gt;0\), we have \(\P [X=Y]=1\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-5.html#ps:conv_prob_vs_L1"><b>5.8</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Write
</p>
<p>
\[\min (1,X)=\min (1,X)\1_{\{X&lt;a\}} + \min (1,X)\1_{\{X\geq a\}}\]
</p>
<p>
and take expectations, giving
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{3}\)</span>


<!--



                                                                                 E[min(1, X)] = E[min(1, X)1{X<a} ] + E[min(1, X)1{Xâ‰¥a} ]

                                                                                                â‰¤ E[a] + E[1{Xâ‰¥a} ]

                                                                                                = a + P[X â‰¥ a].



-->


<p>


\begin{align*}
\E [\min (1,X)] &amp;= \E [\min (1,X)\1_{\{X&lt;a\}}] + \E [\min (1,X)\1_{\{X\geq a\}}] \\ &amp;\leq \E [a] + \E [\1_{\{X\geq a\}}] \\ &amp;=a + \P [X\geq a].
\end{align*}
To deduce the second line of the above we use monotonicity of \(\E \).
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> \((\Leftarrow ):\) Suppose that \(\E [\min (1,X)]\to 0\). For \(a\in (0,1]\) we have
</p>
<p>
\[\P [X_n\geq a]=\P [\min (1,X_n)\geq a]\leq \frac {1}{a}\E [\min (1,X_n)]\]
</p>
<p>
which tends to zero as \(n\to \infty \). Here we use Markovâ€™s inequality.
</p>
<p>
For \(a\geq 1\) we have \(\P [X\geq a]\leq \P [X\geq 1]\to 0\).
</p>
<p>
\((\Rightarrow ):\) Suppose that \(X_n\stackrel {\P }{\to }0\). Let \(a\in (0,1]\). Then \(\P [X_n\geq a]\to 0\).
</p>
<p>
From part (a) we have
</p>
<p>
\[0\leq \E [\min (1,X_n)]\leq a+\P [X_n\geq a].\]
</p>
<p>
Let \(\epsilon &gt;0\). Choose \(a=\frac {\epsilon }{2}\) and let \(N\in \N \) be large enough that \(\P [X_n\geq a]\leq \frac {\epsilon }{2}\) for all \(n\geq \N \). Then \(0\leq \E [\min
(1,X_n)]\leq \frac {\epsilon }{2}+\frac {\epsilon }{2}=\epsilon \) for all \(n\geq N\). Hence \(\E [\min (1,X_n)]\to 0\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-5.html#ps:conv_const"><b>5.9</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> For the deterministic random variable \(X=c\), the only discontinuity of its distribution function is at the value \(c\), where it jumps from \(0\) to \(1\). Therefore, from
\(X_n\stackrel {d}{\to } c\) we have that for all \(\eps &gt;0\), \(\P [X_n\leq c-\eps ]\to \P [c&lt;c-\eps ]=0\) and \(\P [X_n\leq c+\eps ]\to \P [c\leq c+\eps ]=1\), as \(n\to \infty \).
From the second statement we may deduce that \(\P [X_n\geq c+\eps ]\to 0\) for all \(\eps &gt;0\). We thus have
</p>
<p>
\[\P [|X_n-c|\geq \eps ]] = \P [X_n\leq c-\eps ]+\P [X_n\geq c+\eps ]\to 0 \]
</p>
<p>
as \(n\to \infty \), which is to say that \(X_n\stackrel {\P }{\to } c\).
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Let \((X_n)\) be a sequence of independent random variables such that \(X_n\stackrel {\P }{\to } X\). We will argue by contradiction: suppose that \(X\) is not
almost equal to a constant. There there exists \(c\in \R \) and \(\eps ,\de &gt;0\) such that \(\P [X\leq c-\eps ]\geq \de \) and \(\P [X\geq c+\eps ]\geq \de \).
</p>
<p>
By Lemma <a href="Convergence-Random-Variables.html#lem:P_as_sub">5.2.4</a> there is a subsequence \((Y_n)\) of \((X_n)\) such that \(Y_n\stackrel {a.s.}{\to } X\). By Lemma <a
href="Convergence-Random-Variables.html#lem:conv_modes">5.2.1</a> we have that \(Y_n\stackrel {\P }{\to } X\).
</p>
<p>
Since \(Y_n\stackrel {\P }{\to } X\), there exists \(N\in \N \) such that for all \(n\geq N\) we have \(\P [|Y_n-X|\geq \eps /2]\leq \de /2\). For \(n\geq N\) we thus have \(\P [Y_n\leq c-\eps
/2]\geq \de /2\) and \(\P [Y_n\geq c+\eps /2]\geq \de /2\). Hence also
</p>
<p>
\[ \sum _n\P [Y_n\leq c-\eps /2]=\infty \quad \quad \text { and }\quad \quad \sum _n\P [Y_n\geq c+\eps /2]=\infty .                                \]
</p>
<p>
The \((X_n)\) are independent, hence so are the elements of the subsequence \((Y_n)\). From the second Borel-Cantelli lemma we obtain that
</p>
<p>
\[\P [Y_n\leq c-\eps /2\text { infinitely often, and }Y_n\geq c+\eps /2\text { infinitely often}]=1.\]
</p>
<p>
However, this contradicts the fact that \(Y_n\stackrel {a.s}{\to }X\).
</p>
<p>
We have therefore reached a contradiction, so in fact there exists some \(c\in \R \) such that \(\P [X=c]=1\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-5.html#ps:complete_conv"><b>5.10</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We first show that complete convergence implies almost sure convergence. This part does not require independence. Let \(A_\epsilon =\{|X_n-X|\leq \epsilon
\;\text { e.v.}\}\) and note that \(A_{1/m}\) is a decreasing sequence of sets (as \(m\in \N \) increases), and that
</p>
<p>
\[\bigcap _{m\in \N }A_{1/m}=\bigcap _{\epsilon &gt;0}A_\epsilon =\{X_n\to X\}.\]
</p>
<p>
If \(X_n\) converges completely to \(X\) then, by the first Borel-Cantelli lemma, \(\P [|X_n-X|\geq \epsilon \;\text { i.o.}]=0\) which implies that \(\P [A_{1/m}]=1\) for all \(m\in \N \). Since
\((A_{1/m})\) is decreasing we obtain that \(\P [\cap _{m\in \N }A_{1/m}]=1\), and hence \(\P [X_n\to X]=1\), so we have almost sure convergence.
</p>
<p>
Let us now show that if the \((X_n)\) are independent then almost sure convergence implies complete convergence. By part (b) of exercise <a href="Exercises-5.html#ps:conv_const"><b>5.9</b></a> \(X\)
we have that \(X_n\stackrel {a.s.}{\to } X=c\) where \(c\in \R \) is deterministic. For any \(\epsilon &gt;0\), the sequence of events \(E_n(\epsilon )=\{|X_n-c|\geq \epsilon \}\) are independent.
The fact that \(X_n\stackrel {a.s.}{\to } c\) means that \(\P [E_n(\epsilon )\text { i.o.}]=0\). Hence by the second Borel-Cantelli lemma (here we use independence) we must have \(\sum _n \P
[E_n(\epsilon )]&lt;\infty \), as required.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Let \(U\) be a uniform random variable on \([0,1]\) and define
</p>
<p>
\[X_n= \begin {cases} 1 &amp; \text { if }U\leq \frac 1n \\ 0 &amp; \text { otherwise.} \end {cases} \]
</p>
<p>
Then \(\P [X_n\to 0]=\P [U&gt;0]=1\), so \(X_n\stackrel {a.s.}{\to } 0\).
</p>
<p>
For \(\eps \in (0,1]\) we have \(\P [|X_n-0|\geq \eps ]=\P [X_n=1]=\frac 1n\), so \(\sum _n\P [|X_n-0|\geq \eps ]=\infty \), hence \(X_n\) does not converge completely to \(0\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-5.html#ps:C_int_abs"><b>5.11</b></a> Suppose \(f = f_{1} + if_{2}\) is integrable. Then both \(f_{1}\) and \(f_{2}\) are integrable. The integrability of \(|f| = \sqrt
{f_{1}^{2} + f_{2}^{2}}\) follows immediately from the inequality \(\sqrt {f_{1}^{2} + f_{2}^{2}} \leq |f_{1}| + |f_{2}|\). For the converse use \(|f_{1}| \leq \sqrt {f_{1}^{2} + f_{2}^{2}}\)
and \(|f_{2}| \leq \sqrt {f_{1}^{2} + f_{2}^{2}}\).
</p>
</li>
<li>


<p>
<a href="Exercises-5.html#ps:N_char_func"><b>5.12</b></a> First suppose that we have established the case for \(Y\sim N(0,1)\) i.e.&nbsp;we know that \(\Phi _{Y}(u) = e^{-\frac {1}{2}u^{2}}\)
for all \(u \in \R \). Then since \(X = \mu + \sigma Y\), we have
</p>
<p>
\[\begin {aligned} \Phi _{X}(u) &amp; = \E (e^{iu (\mu + \sigma Y)})\\ &amp; = e^{iu\mu }\E (e^{i(u\sigma )Y}) = e^{i\mu u - \frac {1}{2}\sigma ^{2}u^{2}},\end {aligned}\]
</p>
<p>
as was required.
</p>
<p>
It remains to establish the result for \(Y\). To this end we write
</p>
<p>
\[\begin {aligned} \Phi _{Y}(u) &amp; = \frac {1}{\sqrt {2 \pi }}\int _{\R }e^{iuy}e^{-\frac {1}{2}y^{2}}dy \\ &amp; = \frac {1}{\sqrt {2 \pi }}\int _{\R }\cos (uy)e^{-\frac
{1}{2}y^{2}}dy + i \frac {1}{\sqrt {2 \pi }}\int _{\R }\sin (uy)e^{-\frac {1}{2}y^{2}}dy.                         \end {aligned}\]
</p>
<p>
As \(|\cos (uy)ye^{-\frac {1}{2}u^{2}}| \leq |y|e^{-\frac {1}{2}y^{2}}\) and \(|\sin (uy)ye^{-\frac {1}{2}u^{2}}| \leq |y|e^{-\frac {1}{2}y^{2}}\) and \(y \rightarrow |y|e^{-\frac
{1}{2}y^{2}}\) is integrable on \(\R \), we may apply Problem <a href="Exercises-3.html#ps:diff_under_int"><b>3.15</b></a> to deduce that \(u \rightarrow \Phi _{Y}(u)\) is differentiable and its
derivative at \(u \in \R \) is
</p>
<p>
\[ \Phi _{Y}^{\prime }(u) = \frac {i}{\sqrt {2 \pi }}\int _{\R }e^{iuy}ye^{-\frac {1}{2}y^{2}}dy.\]
</p>
<p>
Now integrate by parts to find that
</p>
<p>
\[\begin {aligned} \Phi _{Y}^{\prime }(u) &amp; = \frac {i}{\sqrt {2 \pi }}\left [-e^{iuy}e^{-\frac {1}{2}y^{2}}\right ]_{-\infty }^{\infty } - \frac {1}{\sqrt {2 \pi }}\int
_{-\infty }^{\infty }ue^{iuy}e^{-\frac {1}{2}y^{2}}dy\\ &amp; = -u \Phi _{Y}(u).                       \end {aligned}\]
</p>
<p>
So we have the initial value problem \(\ds \frac {d \Phi _{Y}(u)}{du} = -u \Phi _{Y}(u)\) with initial condition, \(\Phi _{Y}(0) = 1\) and the result follows by using the standard separation of variables
technique.
</p>
</li>
<li>


<p>
<a href="Exercises-5.html#ps:gen_moments"><b>5.13</b></a> First note that by Problem <a href="Exercises-4.html#ps:cauchy_schwarz"><b>4.11</b></a>, for all \(1 \leq m \leq n, \E
(|X|^{m})\) is finite and so the mapping \(y \rightarrow y^{m}\) is \(p_{X}\) integrable. We also have that for all \(u,y \in \R , |i^{m}y^{m}e^{iuy}| \leq |y|^{m}\) Hence we can apply Problem <a
href="Exercises-3.html#ps:diff_under_int"><b>3.15</b></a> to differentiate up to and including \(n\) times under the integral sign to obtain
</p>
<p>
\[ \frac {d^{n}}{du^{n}}\Phi _{X}(u) = \int _{\R }i^{n}y^{n}e^{iuy}dp_{X}(y).\]
</p>
<p>
Now let \(u=0\) to find that
</p>
<p>
\[ \left .\frac {d^{n}}{du^{n}}\Phi _{X}(u)\right |_{u=0} = i^{n}\int _{\R }y^{n}dp_{X}(y) = i^{n}\E (X^{n}).\]
</p>
</li>
<li>


<p>
<a href="Exercises-5.html#ps:clt_bernoulli"><b>5.14</b></a> In this case \(\mu = p\) and \(\sigma = \sqrt {p(1-p)}\) and so we can write
</p>
<p>
\[ \lim _{n \rightarrow \infty }\P \left (\frac {S_{n} - np}{\sqrt {np(1-p)}} \leq a \right ) = \frac {1}{\sqrt {2\pi }}\int _{-\infty }^{a}e^{-\frac {1}{2}y^{2}}dy.\]
</p>
<p>
The random variable \(S_{n}\) is the sum of \(n\) i.i.d. Bernoulli random variables and so is binomial with mean \(np\) and variance \(np(1-p)\) and so for large \(n\) it is approximately normal in the precise
sense given above.
</p>
<p>


</p>
</li>
</ul>
<!--
......   subsection Chapter <a href=Product-Measures-Fubini-Theorem.html#chap:product_meas>6</a> ......
-->
<h5 id="autosec-255">Chapter <a href="Product-Measures-Fubini-Theorem.html#chap:product_meas">6</a></h5>
<a id="notes-autopage-255"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-6.html#ps:Ex"><b>6.1</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span>
</p>
<p>
\[\begin {aligned} (E \cap F)_{x} &amp; = \{y \in S_{2}; (x, y) \in E \cap F\}\\ &amp; = \{y \in S_{2}; (x,y) \in E\} \cap \{y \in S_{2}; (x,y) \in F\}\\ &amp; = E_{x} \cap F_{x}.
\end {aligned}\]
</p>
<p>
\[\begin {aligned} (E^{c})_{x} &amp; = \{y \in S_{2}; (x, y) \in E^{c}\}\\ &amp; = \{y \in S_{2}; (x, y) \notin E\}\\ &amp; = (E_{x})^{c}.                             \end {aligned}\]
</p>
<p>
\[\begin {aligned} \left (\bigcup _{n=1}^{\infty }E_{n}\right )_{x} &amp; = \left \{y \in S_{2}; (x, y) \in \bigcup _{n=1}^{\infty }E_{n}\right \}\\ &amp; = \bigcup _{n=1}^{\infty
}\{y \in S_{2}; (x, y) \in E_{n}\}\\ &amp; = \bigcup _{n=1}^{\infty }(E_{n})_{x}.                     \end {aligned}\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-6.html#ps:prod_sigma_finite_meas"><b>6.2</b></a> We can write \(S_{1} = \bigcup _{n=1}^{\infty }A_{n}\) where \(m_{1}(A_{n}) &lt; \infty \) for all \(\nN \) and \(S_{2}
= \bigcup _{r=1}^{\infty }B_{r}\) where \(m_{2}(B_{r}) &lt; \infty \) for all \(r \in \mathbb {N}\). We then have
</p>
<p>
\[S_{1} \times S_{2} = \bigcup _{n=1}^{\infty }\bigcup _{r=1}^{\infty }A_{n} \times B_{r},\]
</p>
<p>
and for all \(r,n \in \mathbb {N}\),
</p>
<p>
\[(m_{1} \times m_{2})(A_{n} \times B_{r}) = m_{1}(A_{n})m_{2}(B_{r}) &lt; \infty .\]
</p>
<p>
(You can, of course, write \(S_{1} \times S_{2}\) as just a single union, by using the countability of \(\N \times \N \).)
</p>
</li>
<li>


<p>
<a href="Exercises-6.html#ps:prod_meas"><b>6.3</b></a> Let \(E = A \times B\). Then if \(x \in S_{1}\),
</p>
<p>
\[ E_{x} = \left \{\begin {array}{cc} B &amp; \mbox {if}~ x \in A\\ \emptyset &amp; \mbox {if}~ x \notin A \end {array} \right .                           \]
</p>
<p>
So \(\phi _{E}(x) = m_{2}(B){\1}_{A}(x)\), and hence
</p>
<p>
\[\begin {aligned} (m_{1} \times m_{2})(A \times B) &amp; = \int _{S_{1}}\phi _{E}(x)dm_{1}(x) \\ &amp; = m_{2}(B) \int _{S_{1}}{\1}_{A}(x)dm_{1}(x)\\ &amp; = m_{1}(A)m_{2}(B).
\end {aligned}\]
</p>
</li>
<li>


<p>
<a href="Exercises-6.html#ps:prod_meas_2"><b>6.4</b></a> Suppose that \(\mu \) is a measure that takes the same value as \(m_{1} \times m_{2}\) on finite product sets. Define
</p>
<p>
\[ {\cal E} = \{E \in \Sigma _{1} \otimes \Sigma _{2}; \mu (E) = (m_{1} \times m_{2})(E)\}.\]
</p>
<p>
By definition of \(\mu \), the collection \(\cal P\) of all finite product sets is in \(\cal E\). Since
</p>
<p>
\[ (A_{1} \times A_{2}) \cap (B_{1} \times B_{2}) = (A_{1} \cap B_{1}) \times (A_{2} \cap B_{2}),\]
</p>
<p>
it follows that \(\cal P\) is a \(\pi \)-system. Using basic properties of measures, it is not hard to show that \(\cal E\) is a \(\lambda \)-system (use the solution to Problem <a
href="Exercises-6.html#ps:Ex"><b>6.1</b></a> to establish (L1)). By \(\sigma \)-finiteness, it follows that \(\sigma ({\cal P}) = \Sigma _{1} \otimes \Sigma _{2}\) and by Dynkinâ€™s \(\pi -\lambda
\) lemma, \(\sigma ({\cal P}) \subseteq {\cal E}\). The result follows.
</p>
</li>
<li>


<p>
<a href="Exercises-6.html#ps:prod_fgh"><b>6.5</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Method 1.&nbsp;First let \(f = {\1}_{A}\) and let \(g = {\1}_{B}\) . Since \(h = {\1}_{A}{\1}_{B} = {\1}_{A \times B}\), it is clear that \(h\) is measurable in
this case. Next use linearity, to extend to the case where \(f\) and \(g\) are non-negative simple functions. Next let \(f\) and \(g\) be arbitrary non-negative measurable functions. Then by Theorem 2.4.1, there is
a sequence of non-negative simple functions \((s_{n})\) converging pointwise to \(f\), and a corresponding sequence \((t_{m})\) converging pointwise to \(g\). Taking limits as \(m\) and \(n\) go to infinity,
proves that \(f\) and \(g\) are measurable in this case. Finally let \(f\) and \(g\) be arbitrary measurable functions. Write \(f = f_{+} - f_{-}\) and \(g = g_{+} - g_{-}\). Then
</p>
<p>
\[ fg = (f_{+}g_{+} + f_{-}g_{-}) - (f_{-}g_{+} + f_{+}g_{-}),\]
</p>
<p>
is measurable as it is a sum of products of measurable functions.
</p>
<p>
Method 2.&nbsp; For \(B \in \Sigma _{2}\), define \(\tilde {f}_{B}(x, y) = f(x){\1}_{B}(y)\) for all \(x \in S_{1}, y \in S_{2}\). The mapping \(\tilde {f}:                     S_{1} \times S_{2} \rightarrow
\R \) is measurable since for all \(a \in \R \), \(\tilde {f}^{-1}((a, \infty )) = f^{-1}((a, \infty )) \times B \in \Sigma _{1} \times \Sigma _{2}\). In particular, \(\tilde {f}_{S_{2}}\) is
measurable; however \(\tilde {f}_{S_{2}}(x, y) = f(x)\) for all \(x \in S_{1}, y \in S_{2}\); so \(h = \tilde {f}_{S_{2}}\tilde {g}_{S_{1}}\) is the product of measurable functions, hence is
measurable.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Follows easily from Fubiniâ€™s theorem (2).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-6.html#ps:pve_sum_swap"><b>6.6</b></a> Let \(m\) be counting measure on \((\N , {\cal P}(\N ))\). Then \(a(i, j) = a_{ij}\) defines a non-negative measurable function from
\((\N ^{2}, {\cal P}(\N ^{2}))\) to \((\R , {\cal B}(\R ))\), where we note that \({\cal P}(\N ^{2}) = {\cal P}(\N ) \otimes {\cal P}(\N )\). We have
</p>
<p>
\[ \int _{\N ^{2}}a~d(m \times m) = \sum _{(i, j ) \in \N ^{2}}a_{i,j},\]
</p>
<p>
\[ \int _{\N } \left (\int _{\N }a(i,j)dm(i)\right )dm(j) = \sum _{j=1}^{\infty }\sum _{i=1}^{\infty }a_{ij},\]
</p>
<p>
\[ \int _{\N } \left (\int _{\N }a(i,j)dm(j)\right )dm(i) = \sum _{i=1}^{\infty }\sum _{j=1}^{\infty }a_{ij},\]
</p>
<p>
and the result follows by Fubiniâ€™s theorem 1.
</p>
</li>
<li>


<p>
<a href="Exercises-6.html#ps:Leb_marginal"><b>6.7</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span>
</p>
<p>
\[\begin {aligned} A_{f}^{c} &amp; =\{(x,t) \in S \times \R ; 0 \leq f(x) &lt; t\}\\ &amp; = \bigcup _{q \in \mathbb {Q}}\{(x,t) \in S \times \R ; 0 \leq f(x) &lt; q, t \geq q\}\\
&amp; = \bigcup _{q \in \mathbb {Q}}f^{-1}([0, q)) \times [q, \infty ),\end {aligned}\]
</p>
<p>
which is a countable union of measurable sets, and so is measurable. Hence \(A_{f} = (A_{f}^{c})^{c}\) is measurable.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> We use the definition (as a Lebesgue integral) of product measure. Fix \(x \in S\). Then the \(x\)-slice \((A_{f})_{x}\) is just the interval \([0, f(x)]\). Its Lebesgue
measure is \(f(x)\) and so
</p>
<p>
\[\begin {aligned} (m \times \lambda )(A_{f}) &amp; = \int _{S}\lambda [(A_{f})_{x}]dm(x)\\ &amp; = \int _{S}f(x)dm(x).\end {aligned}\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-6.html#ps:sinxx"><b>6.8</b></a> First fix \(T &gt; 0\) and use the hint:
</p>
<p>
\[ \int _{0}^{T} \ds \frac {\sin (x)}{x}dx = \int _{0}^{T} \sin (x)\left (\int _{0}^{\infty }e^{-xy}dy\right )dx.\]
</p>
<p>
Now \(f(x,y) = e^{-xy}\sin (x)\) is continuous, and so Riemann integrable (and hence Lebesgue integrable) on \([0, t] \times [0, N]\). By Fubiniâ€™s theorem:
</p>
<p>
\[\begin {aligned} \int _{0}^{T} \sin (x)\left (\int _{0}^{N}e^{-xy}dy\right )dx &amp; = \int _{0}^{N} \left (\int _{0}^{T}e^{-xy}\sin (x)dx\right )dy\\ &amp; = -\int _{0}^{N}
\left (\frac {y}{1 + y^{2}}e^{-yT}\sin (T)\right .\\ &amp; + &amp; \left .\frac {1}{1 + y^{2}}(e^{-yT}\cos (T) - 1)\right )dy, \end {aligned}\]
</p>
<p>
using integration by parts.
</p>
<p>
On the other hand, \(\int _{0}^{N}e^{-xy}dy = \frac {1}{x}(1 - e^{-Ny})\) and so
</p>
<p>
\[ \left |\int _{0}^{N}e^{-xy}dy\right | \leq \frac {2}{x}.\]
</p>
<p>
Since \(x \rightarrow \frac {\sin (x)}{x}\) is continuous, and hence integrable, on \([0, T]\) we can use dominated convergence to assert that
</p>
<p>
\[\begin {aligned} \int _{0}^{T} \ds \frac {\sin (x)}{x}dx &amp; = \int _{0}^{T} \sin (x)\left (\int _{0}^{\infty }e^{-xy}dy\right )dx \\ &amp; = -\int _{0}^{\infty } \left (\frac
{y}{1 + y^{2}}e^{-yT}\sin (T) + \frac {1}{1 + y^{2}}(e^{-yT}\cos (T) - 1)\right )dy.                      \end {aligned}\]
</p>
<p>
Now use monotonicity in the first integral (since \(y/1 + y^{2} \leq 1\)), and dominated convergence in the second (since \(|e^{-yT}\cos (T) - 1| \leq 2\)) to deduce that
</p>
<p>
\[ \lim _{T \rightarrow \infty }\int _{0}^{T} \ds \frac {\sin (x)}{x}dx = \int _{0}^{\infty }\frac {1}{1+y^{2}}dy = \frac {\pi }{2}.\]
</p>
</li>
<li>


<p>
<a href="Exercises-6.html#ps:fub_counterex"><b>6.9</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Both integrals vanish by elementary calculus arguments.
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> Let \(S = \{(x,y) \in \R ^{2}; - 1 \leq x \leq 1, -1 \leq y \leq 1\}\) and \(A = \{(x,y) \in \R ^{2}; 0 \leq x \leq 1, 0 \leq y \leq 1\}\). We
require \(\int _{S}|f(x,y)|dx dy &lt; \infty \). Note that \(\int _{S}|f(x,y)|dx dy \geq \int _{A}|f(x,y)|dx dy\). Now if \(f\) were integrable over \(A\), we could use Fubiniâ€™s theorem to write it
as repeated integral. But consider
</p>
<p>
\[ \int _{0}^{1}x \left (\int _{0}^{1}\frac {y}{(x^{2} + y^{2})^{2}}dy \right )dx = \frac {1}{2}\int _{0}^{1}\left (\frac {1}{x} - \frac {x}{x^{2} + 1}\right )dx.\]
</p>
<p>
Since \(x \rightarrow \frac {1}{x}\) is not integrable over \([0, 1]\), the result follows.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-6.html#ps:fourier_conv"><b>6.10</b></a> First observe that by Problems <a href="Exercises-2.html#ps:translate_meas"><b>2.7</b></a> and <a
href="Exercises-1.html#ps:conditional_measure"><b>1.5</b></a> part (a) the mapping \((x, y) \rightarrow f(x-y)g(y)\) is measurable. Let \(K = \sup _{x \in \R }|g(x)| &lt; \infty \), since
\(g\) is bounded. Then since \(f\) is integrable
</p>
<p>
\[ |(f*g)(x)| \leq \int _{\R }|f(x - y)|.|g(y)|dy \leq K \int _{\R }|f(x - y)|dy = K \int _{\R }|f(y)|dy &lt; \infty .\]
</p>
<p>
We also have by Fubiniâ€™s theorem
</p>
<p>
\[\begin {aligned} \int _{\R }\int _{\R }|(f(x-y)g(y)|dydx &amp; \leq &amp; \int _{\R }\left (\int _{\R }|f(x - y)|.|g(y)|dy \right )dx\\ &amp; = \int _{\R }\left (\int _{\R
}|f(x-y)|dx \right )|g(y)|dy \\ &amp; = \int _{\R }|f(x)|dx \int _{\R }|g(y)|dy &lt; \infty , \end {aligned}\]
</p>
<p>
from which it follows that \(f*g\) is both measurable, and integrable.
</p>
<p>
By a similar argument using Fubiniâ€™s theorem, we have that
</p>
<p>
\[\begin {aligned} \widehat {f * g}(y) &amp; = \int _{\R }e^{-ixy}\int _{\R }f(x -z)g(z)dz dx \\ &amp; = \int _{\R }\left (\int _{\R }e^{-iy(u + z)}f(u)du \right )g(z)dz \\ &amp;
= \int _{\R }e^{-iyu}f(u)du.         \int _{\R }e^{-iyz}g(z)dz\\ &amp; = \widehat {f}(y)\widehat {g}(y), \end {aligned}\]
</p>
<p>
where we used that change of variable \(x = u + z\).
</p>
<p>


</p>
</li>
</ul>
<a id="notes-autofile-last"></a>
</section>

</main>

</div>

<footer>

<p>
Copyright Nic Freeman, Sheffield University, last updated March 23, 2023
</p>

</footer>



<nav class="botnavigation"><a href="notes.html" class="linkhome" >
Home</a></nav>

</body>
</html>
