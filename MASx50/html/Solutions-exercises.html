<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="UTF-8" />
<meta name="author" content="Nic Freeman" />
<meta name="generator" content="LaTeX Lwarp package" />
<meta name="description" content="MAS350/61022 Probability with Measure, Sheffield University, September 19, 2023." />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<!--[if lt IE 9]>
<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
<title>MAS350/61022 — Solutions to exercises</title>
<link rel="stylesheet" type="text/css" href="sans-serif-lwarp-sagebrush.css" />
<script>
// Lwarp MathJax emulation code
//
// Based on code by Davide P. Cervone.
// Equation numbering: https://github.com/mathjax/MathJax/issues/2427
// Starred and ifnextchar macros: https://github.com/mathjax/MathJax/issues/2428
// \left, \right delimiters: https://github.com/mathjax/MathJax/issues/2535
//
// Modified by Brian Dunn to adjust equation numbering and add subequations.
//
// LaTeX can use \seteqnumber{subequations?}{section}{number} before each equation.
// subequations? is 0 usually, 1 if inside subequations.
// section is a string printed as-is, or empty.
// number is auto-incremented by MathJax between equations.
//
MathJax = {
  subequations: "0",
  section: "",
  loader: {
    load: ['[tex]/tagformat', '[tex]/textmacros'],
  },
  startup: {
    ready() {
      // These would be replaced by import commands if you wanted to make
      // a proper extension.
      const Configuration = MathJax._.input.tex.Configuration.Configuration;
      const CommandMap = MathJax._.input.tex.SymbolMap.CommandMap;
      const Macro = MathJax._.input.tex.Symbol.Macro;
      const TexError = MathJax._.input.tex.TexError.default;
      const ParseUtil = MathJax._.input.tex.ParseUtil.default;
      const expandable = MathJax._.util.Options.expandable;

        // Insert the replacement string into the TeX string, and check
        // that there haven't been too many maxro substitutions (prevents
        // infinite loops).
        const useArgument = (parser, text) => {
          parser.string = ParseUtil.addArgs(parser, text, parser.string.slice(parser.i));
          parser.i = 0;
          if (++parser.macroCount > parser.configuration.options.maxMacros) {
            throw new TexError('MaxMacroSub1',
            'MathJax maximum macro substitution count exceeded; ' +
            'is there a recursive macro call?');
          }
        }

        // Create the command map for:
        //     \ifstar, \ifnextchar, \ifblank, \ifstrequal, \gsub, \seteqnumber
        new CommandMap('Lwarp-macros', {
          ifstar: 'IfstarFunction',
          ifnextchar: 'IfnextcharFunction',
          ifblank: 'IfblankFunction',
          ifstrequal: 'IfstrequalFunction',
          gsubstitute: 'GsubstituteFunction',
          seteqnumber: 'SeteqnumberFunction'
        }, {
          // This function implements an ifstar macro.
          IfstarFunction(parser, name) {
             const resultstar = parser.GetArgument(name);
             const resultnostar = parser.GetArgument(name);
             const star = parser.GetStar();                 // true if there is a *
             useArgument(parser, star ? resultstar : resultnostar);
          },

          // This function implements an ifnextchar macro.
          IfnextcharFunction(parser, name) {
            let whichchar = parser.GetArgument(name);
            if (whichchar.match(/^(?:0x[0-9A-F]+|[0-9]+)$/i)) {
              // $ syntax highlighting
              whichchar = String.fromCodePoint(parseInt(whichchar));
            }
            const resultnextchar = parser.GetArgument(name);
            const resultnotnextchar = parser.GetArgument(name);
            const gotchar = (parser.GetNext() === whichchar);
            useArgument(parser, gotchar ? resultnextchar : resultnotnextchar);
          },

          // This function implements an ifblank macro.
          IfblankFunction(parser, name) {
            const blankarg = parser.GetArgument(name);
            const resultblank = parser.GetArgument(name);
            const resultnotblank = parser.GetArgument(name);
            const isblank = (blankarg.trim() == "");
            useArgument(parser, isblank ? resultblank : resultnotblank);
          },

          // This function implements an ifstrequal macro.
          IfstrequalFunction(parser, name) {
            const strequalfirst = parser.GetArgument(name);
            const strequalsecond = parser.GetArgument(name);
            const resultequal = parser.GetArgument(name);
            const resultnotequal = parser.GetArgument(name);
            const isequal = (strequalfirst == strequalsecond);
            useArgument(parser, isequal ? resultequal : resultnotequal);
          },

          // This function implements a gsub macro.
          GsubstituteFunction(parser, name) {
            const gsubfirst = parser.GetArgument(name);
            const gsubsecond = parser.GetArgument(name);
            const gsubthird = parser.GetArgument(name);
            let gsubresult=gsubfirst.replace(gsubsecond, gsubthird);
            useArgument(parser, gsubresult);
          },

          // This function modifies the equation numbers.
          SeteqnumberFunction(parser, name) {
              // Get the macro parameters
              const star = parser.GetStar();                  // true if there is a *
              const optBrackets = parser.GetBrackets(name);   // contents of optional brackets
              const newsubequations = parser.GetArgument(name); // the subequations argument
              const neweqsection = parser.GetArgument(name); // the eq section argument
              const neweqnumber = parser.GetArgument(name);   // the eq number argument
              MathJax.config.subequations=newsubequations ;   // a string with boolean meaning
              MathJax.config.section=neweqsection ;           // a string with numeric meaning
              parser.tags.counter = parser.tags.allCounter = neweqnumber ;
          }

        });

        // Create the Lwarp-macros package
        Configuration.create('Lwarp-macros', {
          handler: {macro: ['Lwarp-macros']}
        });

        MathJax.startup.defaultReady();

        // For forward references:
        MathJax.startup.input[0].preFilters.add(({math}) => {
          if (math.inputData.recompile){
              MathJax.config.subequations = math.inputData.recompile.subequations;
              MathJax.config.section = math.inputData.recompile.section;
          }
        });
        MathJax.startup.input[0].postFilters.add(({math}) => {
          if (math.inputData.recompile){
              math.inputData.recompile.subequations = MathJax.config.subequations;
              math.inputData.recompile.section = MathJax.config.section;
          }
        });

          // For \left, \right with unicode-math:
          const {DelimiterMap} = MathJax._.input.tex.SymbolMap;
          const {Symbol} = MathJax._.input.tex.Symbol;
          const {MapHandler} = MathJax._.input.tex.MapHandler;
          const delimiter = MapHandler.getMap('delimiter');
          delimiter.add('\\lBrack', new Symbol('\\lBrack', '\u27E6'));
          delimiter.add('\\rBrack', new Symbol('\\rBrack', '\u27E7'));
          delimiter.add('\\lAngle', new Symbol('\\lAngle', '\u27EA'));
          delimiter.add('\\rAngle', new Symbol('\\rAngle', '\u27EB'));
          delimiter.add('\\lbrbrak', new Symbol('\\lbrbrak', '\u2772'));
          delimiter.add('\\rbrbrak', new Symbol('\\rbrbrak', '\u2773'));
          delimiter.add('\\lbag', new Symbol('\\lbag', '\u27C5'));
          delimiter.add('\\rbag', new Symbol('\\rbag', '\u27C6'));
          delimiter.add('\\llparenthesis', new Symbol('\\llparenthesis', '\u2987'));
          delimiter.add('\\rrparenthesis', new Symbol('\\rrparenthesis', '\u2988'));
          delimiter.add('\\llangle', new Symbol('\\llangle', '\u2989'));
          delimiter.add('\\rrangle', new Symbol('\\rrangle', '\u298A'));
          delimiter.add('\\Lbrbrak', new Symbol('\\Lbrbrak', '\u27EC'));
          delimiter.add('\\Rbrbrak', new Symbol('\\Rbrbrak', '\u27ED'));
          delimiter.add('\\lBrace', new Symbol('\\lBrace', '\u2983'));
          delimiter.add('\\rBrace', new Symbol('\\rBrace', '\u2984'));
          delimiter.add('\\lParen', new Symbol('\\lParen', '\u2985'));
          delimiter.add('\\rParen', new Symbol('\\rParen', '\u2986'));
          delimiter.add('\\lbrackubar', new Symbol('\\lbrackubar', '\u298B'));
          delimiter.add('\\rbrackubar', new Symbol('\\rbrackubar', '\u298C'));
          delimiter.add('\\lbrackultick', new Symbol('\\lbrackultick', '\u298D'));
          delimiter.add('\\rbracklrtick', new Symbol('\\rbracklrtick', '\u298E'));
          delimiter.add('\\lbracklltick', new Symbol('\\lbracklltick', '\u298F'));
          delimiter.add('\\rbrackurtick', new Symbol('\\rbrackurtick', '\u2990'));
          delimiter.add('\\langledot', new Symbol('\\langledot', '\u2991'));
          delimiter.add('\\rangledot', new Symbol('\\rangledot', '\u2992'));
          delimiter.add('\\lparenless', new Symbol('\\lparenless', '\u2993'));
          delimiter.add('\\rparengtr', new Symbol('\\rparengtr', '\u2994'));
          delimiter.add('\\Lparengtr', new Symbol('\\Lparengtr', '\u2995'));
          delimiter.add('\\Rparenless', new Symbol('\\Rparenless', '\u2996'));
          delimiter.add('\\lblkbrbrak', new Symbol('\\lblkbrbrak', '\u2997'));
          delimiter.add('\\rblkbrbrak', new Symbol('\\rblkbrbrak', '\u2998'));
          delimiter.add('\\lvzigzag', new Symbol('\\lvzigzag', '\u29D8'));
          delimiter.add('\\rvzigzag', new Symbol('\\rvzigzag', '\u29D9'));
          delimiter.add('\\Lvzigzag', new Symbol('\\Lvzigzag', '\u29DA'));
          delimiter.add('\\Rvzigzag', new Symbol('\\Rvzigzag', '\u29DB'));
          delimiter.add('\\lcurvyangle', new Symbol('\\lcurvyangle', '\u29FC'));
          delimiter.add('\\rcurvyangle', new Symbol('\\rcurvyangle', '\u29FD'));
          delimiter.add('\\Vvert', new Symbol('\\Vvert', '\u2980'));
    }     // ready
  },      // startup

  tex: {
    packages: {'[+]': ['tagformat', 'Lwarp-macros', 'textmacros']},
    tags: "ams",
         tagformat: {
             number: function (n) {
                 if(MathJax.config.subequations==0)
                     return(MathJax.config.section + n);
                 else
                     return(MathJax.config.section + String.fromCharCode(96+n));
             },
         },
  }
}
</script>

<script
    id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"
></script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-J4222H8D03"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-J4222H8D03');
</script>
<!-- Google tag (gtag.js) -->


</head>
<body>



<a id="notes-autopage-293"></a>
<nav class="topnavigation"><a href="notes.html" class="linkhome" >
Home</a></nav>

<header>

<p>
last updated: September 19, 2023
</p>

</header>



<div class="bodyandsidetoc">
<div class="sidetoccontainer">



<nav class="sidetoc">



<div class="sidetoctitle">

<p>
<span class="sidetocthetitle">Probability with Measure</span>
</p>

<p>
Contents
</p>
</div>



<div class="sidetoccontents">

<p>
<a href="notes.html" class="linkhome" >
Home</a>
</p>

<p>
<a href="Introduction.html#autosec-5" class="tocchapter" >
<span class="sectionnumber">0</span>&#x2003;Introduction</a>
</p>



<p>
<a href="Introduction.html#autosec-6" class="tocsection" >
<span class="sectionnumber">0.1</span>&#x2003;Organization</a>
</p>



<p>
<a href="Preliminaries.html#autosec-13" class="tocsection" >
<span class="sectionnumber">0.2</span>&#x2003;Preliminaries</a>
</p>



<p>
<a href="Measure-Spaces.html#autosec-16" class="tocchapter" >
<span class="sectionnumber">1</span>&#x2003;Measure Spaces</a>
</p>



<p>
<a href="Measure-Spaces.html#autosec-17" class="tocsection" >
<span class="sectionnumber">1.1</span>&#x2003;What is measure theory?</a>
</p>



<p>
<a href="Sigma-fields.html#autosec-20" class="tocsection" >
<span class="sectionnumber">1.2</span>&#x2003;Sigma fields</a>
</p>



<p>
<a href="Measure.html#autosec-31" class="tocsection" >
<span class="sectionnumber">1.3</span>&#x2003;Measure</a>
</p>



<p>
<a href="The-Borel-field.html#autosec-37" class="tocsection" >
<span class="sectionnumber">1.4</span>&#x2003;The Borel \(\sigma \)-field</a>
</p>



<p>
<a href="Lebesgue-measure.html#autosec-41" class="tocsection" >
<span class="sectionnumber">1.5</span>&#x2003;Lebesgue measure</a>
</p>



<p>
<a href="An-example-non-measurable-set.html#autosec-48" class="tocsection" >
<span class="sectionnumber">1.6</span>&#x2003;An example of a non-measurable set \((\star )\)</a>
</p>



<p>
<a href="Measures-limits.html#autosec-53" class="tocsection" >
<span class="sectionnumber">1.7</span>&#x2003;Measures and limits</a>
</p>



<p>
<a href="Null-sets.html#autosec-57" class="tocsection" >
<span class="sectionnumber">1.8</span>&#x2003;Null sets</a>
</p>



<p>
<a href="Product-measures.html#autosec-61" class="tocsection" >
<span class="sectionnumber">1.9</span>&#x2003;Product measures</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-c-measure_spaces.html#autosec-66" class="tocsection" >
<span class="sectionnumber">1.10</span>&#x2003;Exercises on Chapter 1</a>
</p>



<p>
<a href="Real-Analysis.html#autosec-70" class="tocchapter" >
<span class="sectionnumber">2</span>&#x2003;Real Analysis</a>
</p>



<p>
<a href="Real-Analysis.html#autosec-71" class="tocsection" >
<span class="sectionnumber">2.1</span>&#x2003;The extended reals</a>
</p>



<p>
<a href="Liminf-limsup.html#autosec-77" class="tocsection" >
<span class="sectionnumber">2.2</span>&#x2003;Liminf and limsup</a>
</p>



<p>
<a href="Convergence-functions.html#autosec-84" class="tocsection" >
<span class="sectionnumber">2.3</span>&#x2003;Convergence of functions</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-c-real_analysis.html#autosec-90" class="tocsection" >
<span class="sectionnumber">2.4</span>&#x2003;Exercises on Chapter 2</a>
</p>



<p>
<a href="Measurable-Functions.html#autosec-94" class="tocchapter" >
<span class="sectionnumber">3</span>&#x2003;Measurable Functions</a>
</p>



<p>
<a href="Measurable-Functions.html#autosec-95" class="tocsection" >
<span class="sectionnumber">3.1</span>&#x2003;Overview</a>
</p>



<p>
<a href="Borel-measurable-functions.html#autosec-102" class="tocsection" >
<span class="sectionnumber">3.2</span>&#x2003;Borel measurable functions</a>
</p>



<p>
<a href="Measurable-functions-open-sets.html#autosec-106" class="tocsection" >
<span class="sectionnumber">3.3</span>&#x2003;Measurable functions and open sets \((\Delta )\)</a>
</p>



<p>
<a href="Algebra-measurable-functions.html#autosec-114" class="tocsection" >
<span class="sectionnumber">3.4</span>&#x2003;Algebra of measurable functions \((\Delta )\)</a>
</p>



<p>
<a href="Simple-functions.html#autosec-120" class="tocsection" >
<span class="sectionnumber">3.5</span>&#x2003;Simple functions</a>
</p>



<p>
<a href="Extended-real-functions.html#autosec-124" class="tocsection" >
<span class="sectionnumber">3.6</span>&#x2003;Extended real functions</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-c-measurable_funcs.html#autosec-127" class="tocsection" >
<span class="sectionnumber">3.7</span>&#x2003;Exercises on Chapter 3</a>
</p>



<p>
<a href="Lebesgue-Integration.html#autosec-131" class="tocchapter" >
<span class="sectionnumber">4</span>&#x2003;Lebesgue Integration</a>
</p>



<p>
<a href="Lebesgue-Integration.html#autosec-133" class="tocsection" >
<span class="sectionnumber">4.1</span>&#x2003;The Lebesgue integral for simple functions</a>
</p>



<p>
<a href="The-Lebesgue-integral-non-negative-measurable-functions.html#autosec-140" class="tocsection" >
<span class="sectionnumber">4.2</span>&#x2003;The Lebesgue integral for non-negative measurable functions</a>
</p>



<p>
<a href="The-monotone-convergence-theorem.html#autosec-149" class="tocsection" >
<span class="sectionnumber">4.3</span>&#x2003;The monotone convergence theorem</a>
</p>



<p>
<a href="Integration-as-measure.html#autosec-153" class="tocsection" >
<span class="sectionnumber">4.4</span>&#x2003;Integration as a measure</a>
</p>



<p>
<a href="The-Lebesgue-integral.html#autosec-157" class="tocsection" >
<span class="sectionnumber">4.5</span>&#x2003;The Lebesgue integral</a>
</p>



<p>
<a href="The-dominated-convergence-theorem.html#autosec-163" class="tocsection" >
<span class="sectionnumber">4.6</span>&#x2003;The dominated convergence theorem</a>
</p>



<p>
<a href="Calculations-with-Lebesgue-integral.html#autosec-169" class="tocsection" >
<span class="sectionnumber">4.7</span>&#x2003;Calculations with the Lebesgue integral</a>
</p>



<p>
<a href="Lebesgue-integration-complex-valued-functions.html#autosec-176" class="tocsection" >
<span class="sectionnumber">4.8</span>&#x2003;Lebesgue integration of complex valued functions \((\Delta )\)</a>
</p>



<p>
<a href="Multiple-integrals-function-spaces.html#autosec-180" class="tocsection" >
<span class="sectionnumber">4.9</span>&#x2003;Multiple integrals and function spaces \((\star )\)</a>
</p>



<p>
<a href="Riemann-integration.html#autosec-185" class="tocsection" >
<span class="sectionnumber">4.10</span>&#x2003;Riemann integration \((\star )\)</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#autosec-195" class="tocsection" >
<span class="sectionnumber">4.11</span>&#x2003;Exercises on Chapter 4</a>
</p>



<p>
<a href="Probability-with-Measure.html#autosec-203" class="tocchapter" >
<span class="sectionnumber">5</span>&#x2003;Probability with Measure</a>
</p>



<p>
<a href="Probability-with-Measure.html#autosec-205" class="tocsection" >
<span class="sectionnumber">5.1</span>&#x2003;Probability</a>
</p>



<p>
<a href="The-cumulative-distribution-function.html#autosec-208" class="tocsection" >
<span class="sectionnumber">5.2</span>&#x2003;The cumulative distribution function</a>
</p>



<p>
<a href="Discrete-continuous-random-variables.html#autosec-214" class="tocsection" >
<span class="sectionnumber">5.3</span>&#x2003;Discrete and continuous random variables</a>
</p>



<p>
<a href="Independence.html#autosec-218" class="tocsection" >
<span class="sectionnumber">5.4</span>&#x2003;Independence</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#autosec-222" class="tocsection" >
<span class="sectionnumber">5.5</span>&#x2003;Exercises on Chapter 5</a>
</p>



<p>
<a href="Inequalities-Random-Variables.html#autosec-228" class="tocchapter" >
<span class="sectionnumber">6</span>&#x2003;Inequalities for Random Variables \((\Delta )\)</a>
</p>



<p>
<a href="Inequalities-Random-Variables.html#autosec-229" class="tocsection" >
<span class="sectionnumber">6.1</span>&#x2003;Chernoff bounds \((\Delta )\)</a>
</p>



<p>
<a href="The-Paley-Zygmund-inequality.html#autosec-233" class="tocsection" >
<span class="sectionnumber">6.2</span>&#x2003;The Paley-Zygmund inequality \((\Delta )\)</a>
</p>



<p>
<a href="Jensen-inequality.html#autosec-237" class="tocsection" >
<span class="sectionnumber">6.3</span>&#x2003;Jensen’s inequality \((\Delta )\)</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-c-ineqs.html#autosec-244" class="tocsection" >
<span class="sectionnumber">6.4</span>&#x2003;Exercises on Chapter 6 \((\Delta )\)</a>
</p>



<p>
<a href="Sequences-Random-Variables.html#autosec-248" class="tocchapter" >
<span class="sectionnumber">7</span>&#x2003;Sequences of Random Variables</a>
</p>



<p>
<a href="Sequences-Random-Variables.html#autosec-249" class="tocsection" >
<span class="sectionnumber">7.1</span>&#x2003;The Borel-Cantelli lemmas</a>
</p>



<p>
<a href="Convergence-random-variables.html#autosec-253" class="tocsection" >
<span class="sectionnumber">7.2</span>&#x2003;Convergence of random variables</a>
</p>



<p>
<a href="Laws-large-numbers.html#autosec-262" class="tocsection" >
<span class="sectionnumber">7.3</span>&#x2003;Laws of large numbers</a>
</p>



<p>
<a href="Characteristic-functions.html#autosec-268" class="tocsection" >
<span class="sectionnumber">7.4</span>&#x2003;Characteristic functions (\(\Delta \))</a>
</p>



<p>
<a href="The-central-limit-theorem.html#autosec-278" class="tocsection" >
<span class="sectionnumber">7.5</span>&#x2003;The central limit theorem (\(\Delta \))</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-c-rv_sequences.html#autosec-284" class="tocsection" >
<span class="sectionnumber">7.6</span>&#x2003;Exercises on Chapter 7</a>
</p>



<p>
<a href="Advice-revision-exams.html#autosec-290" class="tocchapter" >
<span class="sectionnumber">A</span>&#x2003;Advice for revision/exams</a>
</p>



<p>
<a href="Solutions-exercises.html#autosec-294" class="tocchapter" >
<span class="sectionnumber">B</span>&#x2003;Solutions to exercises</a>
</p>



</div>

</nav>

</div>



<main class="bodycontainer">



<section class="textbody">

<h1>Probability with Measure</h1>

<!--MathJax customizations:-->



<div class="hidden">

\(\newcommand{\footnotename}{footnote}\)

\(\def \LWRfootnote {1}\)

\(\newcommand {\footnote }[2][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand {\footnotemark }[1][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\let \LWRorighspace \hspace \)

\(\renewcommand {\hspace }{\ifstar \LWRorighspace \LWRorighspace }\)

\(\newcommand {\mathnormal }[1]{{#1}}\)

\(\newcommand \ensuremath [1]{#1}\)

\(\newcommand {\LWRframebox }[2][]{\fbox {#2}} \newcommand {\framebox }[1][]{\LWRframebox } \)

\(\newcommand {\setlength }[2]{}\)

\(\newcommand {\addtolength }[2]{}\)

\(\newcommand {\setcounter }[2]{}\)

\(\newcommand {\addtocounter }[2]{}\)

\(\newcommand {\arabic }[1]{}\)

\(\newcommand {\number }[1]{}\)

\(\newcommand {\noalign }[1]{\text {#1}\notag \\}\)

\(\newcommand {\cline }[1]{}\)

\(\newcommand {\directlua }[1]{\text {(directlua)}}\)

\(\newcommand {\luatexdirectlua }[1]{\text {(directlua)}}\)

\(\newcommand {\protect }{}\)

\(\def \LWRabsorbnumber #1 {}\)

\(\def \LWRabsorbquotenumber &quot;#1 {}\)

\(\newcommand {\LWRabsorboption }[1][]{}\)

\(\newcommand {\LWRabsorbtwooptions }[1][]{\LWRabsorboption }\)

\(\def \mathchar {\ifnextchar &quot;\LWRabsorbquotenumber \LWRabsorbnumber }\)

\(\def \mathcode #1={\mathchar }\)

\(\let \delcode \mathcode \)

\(\let \delimiter \mathchar \)

\(\def \oe {\unicode {x0153}}\)

\(\def \OE {\unicode {x0152}}\)

\(\def \ae {\unicode {x00E6}}\)

\(\def \AE {\unicode {x00C6}}\)

\(\def \aa {\unicode {x00E5}}\)

\(\def \AA {\unicode {x00C5}}\)

\(\def \o {\unicode {x00F8}}\)

\(\def \O {\unicode {x00D8}}\)

\(\def \l {\unicode {x0142}}\)

\(\def \L {\unicode {x0141}}\)

\(\def \ss {\unicode {x00DF}}\)

\(\def \SS {\unicode {x1E9E}}\)

\(\def \dag {\unicode {x2020}}\)

\(\def \ddag {\unicode {x2021}}\)

\(\def \P {\unicode {x00B6}}\)

\(\def \copyright {\unicode {x00A9}}\)

\(\def \pounds {\unicode {x00A3}}\)

\(\let \LWRref \ref \)

\(\renewcommand {\ref }{\ifstar \LWRref \LWRref }\)

\( \newcommand {\multicolumn }[3]{#3}\)

\(\require {textcomp}\)

\(\newcommand {\intertext }[1]{\text {#1}\notag \\}\)

\(\let \Hat \hat \)

\(\let \Check \check \)

\(\let \Tilde \tilde \)

\(\let \Acute \acute \)

\(\let \Grave \grave \)

\(\let \Dot \dot \)

\(\let \Ddot \ddot \)

\(\let \Breve \breve \)

\(\let \Bar \bar \)

\(\let \Vec \vec \)

\(\DeclareMathOperator {\var }{var}\)

\(\DeclareMathOperator {\cov }{cov}\)

\(\newcommand {\nN }{n \in \mathbb {N}}\)

\(\newcommand {\Br }{{\cal B}(\R )}\)

\(\newcommand {\F }{{\cal F}}\)

\(\newcommand {\ds }{\displaystyle }\)

\(\newcommand {\st }{\stackrel {d}{=}}\)

\(\newcommand {\uc }{\stackrel {uc}{\rightarrow }}\)

\(\newcommand {\la }{\langle }\)

\(\newcommand {\ra }{\rangle }\)

\(\newcommand {\li }{\liminf _{n \rightarrow \infty }}\)

\(\newcommand {\ls }{\limsup _{n \rightarrow \infty }}\)

\(\newcommand {\limn }{\lim _{n \rightarrow \infty }}\)

\(\def \to {\rightarrow }\)

\(\def \iff {\Leftrightarrow }\)

\(\def \ra {\Rightarrow }\)

\(\def \sw {\subseteq }\)

\(\def \mc {\mathcal }\)

\(\def \mb {\mathbb }\)

\(\def \sc {\setminus }\)

\(\def \v {\textbf }\)

\(\def \E {\mb {E}}\)

\(\def \P {\mb {P}}\)

\(\def \R {\mb {R}}\)

\(\def \C {\mb {C}}\)

\(\def \N {\mb {N}}\)

\(\def \Q {\mb {Q}}\)

\(\def \Z {\mb {Z}}\)

\(\def \B {\mb {B}}\)

\(\def \~{\sim }\)

\(\def \-{\,;\,}\)

\(\def \qed {$\blacksquare $}\)

\(\def \1{\unicode {x1D7D9}}\)

\(\def \cadlag {c\&grave;{a}dl\&grave;{a}g}\)

\(\def \p {\partial }\)

\(\def \l {\left }\)

\(\def \r {\right }\)

\(\def \Om {\Omega }\)

\(\def \om {\omega }\)

\(\def \eps {\epsilon }\)

\(\def \de {\delta }\)

\(\def \ov {\overline }\)

\(\def \sr {\stackrel }\)

\(\def \Lp {\mc {L}^p}\)

\(\def \Lq {\mc {L}^p}\)

\(\def \Lone {\mc {L}^1}\)

\(\def \Ltwo {\mc {L}^2}\)

\(\def \toae {\sr {\rm a.e.}{\to }}\)

\(\def \toas {\sr {\rm a.s.}{\to }}\)

\(\def \top {\sr {\mb {\P }}{\to }}\)

\(\def \tod {\sr {\rm d}{\to }}\)

\(\def \toLp {\sr {\Lp }{\to }}\)

\(\def \toLq {\sr {\Lq }{\to }}\)

\(\def \eqae {\sr {\rm a.e.}{=}}\)

\(\def \eqas {\sr {\rm a.s.}{=}}\)

\(\def \eqd {\sr {\rm d}{=}}\)

\(\def \Sa {(S1)}\)

\(\def \Sb {(S2)}\)

\(\def \Sc {(S3)}\)

\(\def \Scp {(S3&apos;)}\)

\(\def \Ma {(M1)}\)

\(\def \Mb {(M2)}\)

\(\def \La {(L1)}\)

\(\def \Lb {(L2)}\)

\(\def \Lc {(L3)}\)

\(\def \Ld {(L4)}\)

\(\def \Le {(L5)}\)

</div>

<p>
<!--
......     chapter Solutions to exercises ......
-->
<h3 id="autosec-294">Chapter&nbsp;<span class="sectionnumber">B&#x2003;</span>Solutions to exercises</h3>
<a id="notes-autopage-294"></a>
<a id="notes-autofile-51"></a>

<a id="s:solutions"></a>
<!--
......    subsection Chapter <a href=Measure-Spaces.html#c:measure_spaces>1</a> ......
-->
<h5 id="autosec-295">Chapter <a href="Measure-Spaces.html#c:measure_spaces">1</a></h5>
<a id="notes-autopage-295"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-c-measure_spaces.html#ps:not_sigma_field"><b>1.1</b></a> For example, \(\{3\}\cup \{4\}=\{3,4\}\), which is not an element of \(A\), but \(\sigma \)-fields
are closed under taking (finite and countable) unions.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measure_spaces.html#ps:sigma_union"><b>1.2</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> To show \(\Sigma _{1} \cap \Sigma _{2}\) is a \(\sigma \)-field we must verify (S1) to (S3).
</p>
<p>
(S1) Since \(S \in \Sigma _{1}\) and \(S \in \Sigma _{2}\), \(S \in \Sigma _{1} \cap \Sigma _{2}\).
</p>
<p>
(S2) Suppose \((A_{n})\) is a sequence of sets in \(\Sigma _{1} \cap \Sigma _{2}\). Then \(A_{n} \in \Sigma _{1}\) for all \(\nN \) and so \(\bigcup _{n=1}^{\infty }A_{n} \in \Sigma _{1}\). But
also \(A_{n} \in \Sigma _{2}\) for all \(\nN \) and so \(\bigcup _{n=1}^{\infty }A_{n} \in \Sigma _{2}\). Hence \(\bigcup _{n=1}^{\infty }A_{n} \in \Sigma _{1} \cap \Sigma _{2}\).
</p>
<p>
(S3) If \(A \in \Sigma _{1} \cap \Sigma _{2}, A^{c} \in \Sigma _{1}\) and \(A^{c} \in \Sigma _{2}\). Hence \(A^{c} \in \Sigma _{1} \cap \Sigma _{2}\).
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> \(\Sigma _{1} \cup \Sigma _{2}\) is not in general a \(\sigma \)-field, because if \(A \in \Sigma _{1}\) and \(B \in \Sigma _{2}\) there is no reason why \(A
\cup B \in \Sigma _{1} \cup \Sigma _{2}\). For example let \(S = \{1,2,3\}, \Sigma _{1} = \{\emptyset , \{1\}, \{2,3\}, S\}, \Sigma _{2} = \{\emptyset , \{2\}, \{1,3\}, S\}, A =
\{1\}, B = \{2\}\). Then \(A \cup B = \{1,2\}\) is neither in \(\Sigma _{1}\) nor \(\Sigma _{2}\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measure_spaces.html#ps:sigma_restrict"><b>1.3</b></a> We check S1-S3. Note that we need to show that \(\Sigma _X\) is a \(\sigma \)-field on \(X\) (and not a
\(\sigma \)-field on \(S\)).
</p>
<p>
(S1) Taking \(A=\emptyset \in \Sigma \) we have \(\emptyset \cap X=\emptyset \in \Sigma _X\). Taking \(A=S\in \Sigma \), we have \(S\cap X = X\in \Sigma _X\).
</p>
<p>
(S2) If \(A\in \Sigma _X\) then \(A\in \Sigma \), and \(X\sc (X\cap A)=X\sc A= X\cap (S\sc A)\in \Sigma _X\) because \(S\sc A\in \Sigma \).
</p>
<p>
(S3) If \(A_n\cap X\in \Sigma _X\) for all \(n\in \N \) then \(A_n\in \Sigma \), and \(\bigcup _{n=1}^\infty A_n\cap X=X\cap \l (\bigcup _{n=1}^\infty A_n\r )\in \Sigma _X\) because
\(\bigcup _{n=1}^\infty A_n\in \Sigma \).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measure_spaces.html#ps:measure_basic"><b>1.4</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We can write \(A \cup B = (A \sc B) \cup (B \sc A) \cup (A \cap B)\) as a disjoint union (draw a diagram!). Hence using finite additivity of measures we obtain
</p>
<p>
\[ m(A \cup B) = m(A\sc B) + m(B\sc A) + m(A \cap B).\]
</p>
<p>
Hence
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--



                                                                        m(A ∪ B) + m(A ∩ B) = m(A \ B) + m(B \ A) + 2m(A ∩ B)

                                                                                                    = [m(A \ B) + m(A ∩ B)] + [m(B \ A) + m(A ∩ B)]

                                                                                                    = m(A) + m(B),



-->


<p>


\begin{align*}
m(A \cup B) + m(A \cap B) &amp; = m(A\sc B) + m(B\sc A ) + 2m(A\cap B)\\ &amp; = [m(A \sc B) + m(A \cap B)] + [m(B \sc A) + m(A \cap B)]\\ &amp; = m(A) + m(B),
\end{align*}
where we use the fact that \(A\) is the disjoint union of \(A \sc B\) and \(A \cap B\), and the analogous result for \(B\sc A\).
</p>
<p>
Note that the possibility that \(m(A \cap B) = \infty \) is allowed for within this proof, because we have not used subtraction and therefore the undefined quantity \(\infty -\infty \) does not arise.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> \(m(A \cup B) \leq m(A \cup B) + m(A \cap B) = m(A) + m(B)\) follows immediately from (a) as \(m(A \cap B) \geq 0\). The general case is proved by induction.
We’ve just established \(n=2\). Now suppose the result holds for some \(n\). Then
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                              n+1                 n                               n
                                                                                         !                          !                    !
                                                                              [                   [                               [
                                                                          m         Ai       =m         Ai ∪ An+1       ≤m              Ai + m(An+1 )
                                                                              i=1                 i=1                             i=1
                                                                                                                            n
                                                                                                                            X                           n+1
                                                                                                                                                        X
                                                                                                                        ≤         m(Ai ) + m(An+1 ) =         m(Ai ).
                                                                                                                            i=1                         i=1



-->


<p>


\begin{align*}
m\left (\bigcup _{i=1}^{n+1}A_{i}\right ) = m\left (\bigcup _{i=1}^{n}A_{i} \cup A_{n+1}\right ) &amp;\leq m\left (\bigcup _{i=1}^{n}A_{i}\right ) + m(A_{n+1}) \\ &amp;\leq \sum
_{i=1}^{n}m(A_{i}) +m(A_{n+1}) = \sum _{i=1}^{n+1}m(A_{i}).
\end{align*}
The first inequality is justified by the \(n=2\) case, and the second is justified by the inductive hypothesis.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measure_spaces.html#ps:conditional_measure"><b>1.5</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We have that \((km)(\emptyset )=km(\emptyset )=0\) because \(m(\emptyset )=0\).
</p>
<p>
If \((A_n)_{n\in \N }\) is a sequence of disjoint measurable sets then
</p>
<p>
\[\sum \limits _{n=1}^\infty (km)(A_n)=k\sum \limits _{n=1}^\infty m(A_n)=k m\l (\bigcup _{n=1}^\infty A_n\r )=(km)\l (\bigcup _{n=1}^\infty A_n\r ).\]
</p>
<p>
Note that in the second equality above we have used that \(m\) is \(\sigma \)-additive. Hence \(km\) is \(\sigma \)-additive.
</p>
<p>
Thus \(km\) is a measure.
</p>
<p>
If \(m\) is a finite measure, then by taking \(k=\frac {1}{m(S)}\) it follows immediately that \(\P (\cdot )=\frac {m(\cdot )}{m(S)}\) is a measure. Noting that \(\P (S)=\frac {m(S)}{m(S)}=1\), \(\P \)
is a probability measure.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> We have \(m_B(\emptyset )=m(\emptyset \cap B)=m(\emptyset )=0\).
</p>
<p>
If \((A_n)_{n\in \N }\) is a sequence of disjoint measurable sets then \((A_n\cap B)_{n\in \N }\) are also disjoint and measurable, hence
</p>
<p>
\[\sum \limits _{n=1}^\infty m_B(A_n)=\sum \limits _{n=1}^\infty m(A_n\cap B)=m\l (\bigcup _{n=1}^\infty A_n\cap B\r )=m\l (\l (\bigcup _{n=1}^\infty A_n\r )\cap B\r )=m_B\l
(\bigcup _{n=1}^\infty A_n\r ).\]
</p>
<p>
Here to deduce the second equality we use the \(\sigma \)-additivity of \(m\).
</p>
<p>
Thus \(m_B\) is a measure.
</p>
</li>
<li>


<p>
<span class="textnormal">(c)</span> Applying part (a) to \(m_B\), it is immediate that \(\P _B\) is a probability measure.
</p>
<p>
If \(m\) itself is a probability measure, say we write \(m=\P \), then \(\P _B(A)\) is the conditional probability of the event \(A\) given that the event \(B\) occurs.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measure_spaces.html#ps:borel_closed"><b>1.6</b></a> By definition \((a,b) \in {\cal B}(\R )\). We’ve shown in the notes that \(\{a\}, \{b\} \in {\cal
B}(\R )\) and so by S(ii), \([a,b] = \{a\} \cup (a,b) \cup \{b\} \in {\cal B}(\R )\).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measure_spaces.html#ps:meas_decreasing"><b>1.7</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(i)</span> We have that \(A\cap A^c=\emptyset \) and \(A\cup A^c=S\), so \(m(A)+m(A^c)=m(S)=M\). Because \(m(S)&lt;\infty \) we have also that \(m(A)&lt;\infty \),
hence we may subtract \(m(A)\) and obtain \(m(A^c)=M-m(A)\).
</p>
</li>
<li>


<p>
<span class="textnormal">(ii)</span> Let \((A_n)\) be a decreasing sequence of sets. Then \(B_n=S\sc A_n\) defines an increasing sequence of sets, so by the first part of Lemma <a
href="Measures-limits.html#l:monotone_meas">1.7.1</a> we have \(m(B_n)\to m(B)\) where \(B=\cup _j B_j\).
</p>
<p>
By part (a) we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--



                                                                                 m(Bn ) = m(S \ An ) = m(S) − m(An )

                                                                                    m(B) = m(∪j S \ An ) = m(S \ ∩j Aj ) = m(S) − m(∩j Aj )



-->


<p>


\begin{align*}
m(B_n)&amp;=m(S\sc A_n)=m(S)-m(A_n)\\ m(B)&amp;=m(\cup _j S\sc A_n)=m(S\sc \cap _j A_j)=m(S)-m(\cap _j A_j)
\end{align*}
Thus \(m(S)-m(A_n)\to m(S)-m(\cap _j A_j)\). Since \(m(S)&lt;\infty \) we may subtract it, and after multiplying by \(-1\) we obtain that \(m(A_n)\to m(\cap _j A_j)\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Let \(S=\R \), \(\Sigma =\mc {B}(\R )\) and \(m=\lambda \) be Lebesgue measure on \(\R \). Set \(A_n=(-\infty ,-n]\). Note that \(\cap _n A_n=\emptyset \)
so \(\lambda (\cap _n A_n)=0\). However, \(m(A_n)=\infty \) for all \(n\), so \(m(A_n)\nrightarrow m(\cap _n A_n)\) in this case.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measure_spaces.html#ps:full_measure_intersect"><b>1.8</b></a> We have \(m(S\sc E_n)=0\) for all \(n\in \N \). Hence by set algebra and the union bound
(Lemma <a href="Measures-limits.html#l:union_bound_inf">1.7.2</a>)
</p>
<p>
\[m\l (S\sc \l (\bigcap _{n=1}^\infty E_n\r )\r )=m\l (\bigcup _{n=1}^\infty S\sc E_n\r ) \leq \sum _{n=1}^\infty m(S\sc E_n)=0\]
</p>
<p>
as required.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measure_spaces.html#ps:full_measure_intersect"><b>1.8</b></a> The sets \(S\sc E_n\) are null sets, so by Lemma <a
href="Null-sets.html#l:null_union">1.8.1</a> we have that \(\bigcup _{n=1}^\infty S\sc E_n\) is null. By set algebra we have \(S\sc \bigcap _{n=1}^\infty E_n=\bigcup _{n=1}^\infty S\sc
E_n\), hence \(\bigcap _{n=1}^\infty E_n=\bigcup _{n=1}^\infty \) has full measure.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measure_spaces.html#ps:size_Pn"><b>1.9</b></a> There are \(n \choose r\) subsets of size \(r\) for \(0 \leq r \leq n\) and so the total number of subsets is
\(\sum _{r=0}^{n}{n \choose r} = (1 + 1)^{2} = 2^{n}\). Here we used the binomial theorem \((x + y)^{n} = \sum _{r=0}^{n}{n \choose r}x^{r}y^{n-r}\).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measure_spaces.html#ps:atomic_sigma_field"><b>1.10</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Note that each element of \(\Pi \) is a subset of \(S\). Hence \(\Pi \) itself is a subset of the power set \(\mc {P}(S)\) of \(S\). Since \(S\) is a finite set, \(\mc
{P}(S)\) is also a finite set, hence \(\Pi \) is also finite.
</p>
<p>
<i>Part (b) requires you to keep a very clear head. To solve a question like this you have to explore what you have deduced from what else, with lots of thinking ‘if I knew this then I would also know that’ and then
trying to fit a bigger picture together, connecting your start point to your desired end point. Analysis can often be like this. </i>
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(i)</span> Suppose \(\Pi _i\cap \Pi _j \neq \emptyset \). Note that \(\Pi _i \cap \Pi _j\) is a subset of both \(\Pi _i\) and \(\Pi _j\).
</p>
<p>
By definition of \(\Pi \), any subset of \(\Pi _i\) is either equal to \(\Pi _i\) or is equal to \(\emptyset \). Since we assume that \(\Pi _i\cap \Pi _j \neq \emptyset \), we therefore have \(\Pi _i=\Pi
_i\cap \Pi _j\). Similarly, \(\Pi _j=\Pi _i\cap \Pi _j\).
</p>
<p>
Hence \(\Pi _i=\Pi _j\), but this contradicts the fact that the \(\Pi _i\) are distinct from each other. Thus we have a contradiction and in fact we must have \(\Pi _i\cap \Pi _j = \emptyset \).
</p>
</li>
<li>


<p>
<span class="textnormal">(ii)</span> By definition of \(\Pi \) we have \(\cup _{i=1}^k \Pi _i \sw S\). Suppose \(\cup _{i=1}^k \Pi _i \neq S\). Then \(C=S\sc \cup _{i=1}^k \Pi _i\) is a
non-empty set in \(\Sigma \).
</p>
<p>
Since \(C\) is disjoint from all the \(\Pi _i\), we must have \(C\notin \Pi \). Noting that \(C\in \Sigma \), by definition of \(\Pi \) this implies that there is some<sup>1</sup> \(B_1\subset C\) such that
\(B_1\neq \emptyset \).
</p>
<p>
We have that \(B_1\) is disjoint from all the \(\Pi _i\), so we must have \(B_1\notin \Pi \). Thus by the same reasoning (as we gave for \(C\)) there exists \(B_2\subset B_1\) such that \(B_2\neq \emptyset
\). Iterating, we construct an infinite decreasing sequence of sets \(C\supset B_1\supset B_2 \supset B_3\ldots \) each strictly smaller than the previous one, none of which are empty. However, this is
impossible because \(C\sw S\) is a finite set.
</p>
</li>
<li>


<p>
<span class="textnormal">(iii)</span> Let \(i\in I\). So \(\Pi _i\cap A\neq \emptyset \). Noting that \(\Pi _i\cap A\sw \Pi _i\), by definition of \(\Pi \) we must have \(\Pi _i\cap A=\Pi _i\).
That is, \(\Pi _i\sw A\). Since we have this for all \(i\in I\), we have \(\cup _{i\in I} \Pi _i\sw A\).
</p>
<p>
Now suppose that \(A\sc \cup _{i\in I} \Pi _i\neq \emptyset \). Since by (ii) we have \(S=\cup _{i=1}^k \Pi _i\), and the union is disjoint by (i), this means that there is some \(\Pi _j\) with
\(j\notin I\) such that \(A\cap \Pi _j\neq \emptyset \). However \(A\cap \Pi _j\sw \Pi _j\) so by definition of \(\Pi \) we must have \(\Pi _j\cap A=\Pi _j\). That is \(\Pi _j\sw A\), but then we
would have \(j \in I\), which is a contraction.
</p>
<p>
Thus \(A\sc \cup _{i\in I} \Pi _i\) must be empty, and we conclude that \(A=\cup _{i\in I} \Pi _i\).
</p>
<p>


</p>
</li>
</ul>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measure_spaces.html#ps:cantor_false"><b>1.11</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Recall that \(C_n\) is the union of \(2^n\) disjoint closed intervals, each with length \(3^{-n}\), and that \(C=\cap _n C_n\), with notation as in Example <a
href="Measure-Spaces.html#ex:cantor">1.1.1</a>.
</p>
<p>
Suppose, for a contradiction, that \((a,b)\sw C\) with \(a&lt;b\). Then \((a,b)\sw C_n\) for all \(n\). Choose \(n\) such that \((\frac {2}{3})^{-n}&lt;\frac 12(b-a)\). Let us write the \(2^n\) disjoint
closed intervals making up \(C_n\) as \(I_1,\ldots ,I_{2^n}\). The point \(c=\frac {a+b}{2}\) must fall into precisely one of these intervals, say \(I_j\). Since \(I_j\) has length \((\frac {2}{3})^{-n}\),
which is less than \(\frac 12(b-a)\), we must have \(I_j\sw (a,b)\) (draw a picture!). However, \(C_{n+1}\) does not contain all of \(I_j\), because the middle part of \(I_j\) will be removed – so we cannot
have \((a,b)\sw C_{n+1}\). Thus we have reached a contradiction.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> For a counterexample, consider a variant of the construction of the Cantor set, where instead of removing the middle thirds at stage \(n\), we instead remove the middle
\(1-e^{-1/n^2}\) (from each component of \(C_n\)). Then, by the same argument as in the proof of Lemma <a href="Lebesgue-measure.html#l:cantor_zero_meas">1.5.4</a>, we would have
</p>
<p>
\[\lambda (C)=\lim _n \lambda (C_n)=\lim _{n\to \infty } e^{-1}e^{-1/4}e^{-1/9}\ldots e^{-1/n^2}=\lim _{n\to \infty } \exp \l (-\sum _1^n \frac {1}{i^2}\r ) =\exp \l (-\sum
_1^\infty \frac {1}{n^2}\r ).          \]
</p>
<p>
We have that \(\lambda (C)\) is positive because \(\sum _1^\infty \frac {1}{n^2}&lt;\infty \).
</p>
<p>
A similar argument as in part (a) applies here, and shows that \(C\) does not contain any open intervals. The length of each interval within \(C_{n+1}\) is less than half the length of the intervals in \(C_{n}\)
(because each interval of \(C_n\) has a middle part removed to become two intervals in \(C_{n+1}\)). Thus, by a trivial induction, each of the \(2^{n}\) disjoint closed intervals in \(C_n\) has length \(\leq
(\frac 12)^{n}\). You can check that we can apply the same argument as in (v), but replacing \((\frac 23)^n\) with \((\frac 12)^{n}\).
</p>
</li>
</ul>
</li>
</ul>
<div role="note" class="footnotes">

<a id="notes-autopage-296"></a>

<p>
<sup>1</sup>&nbsp;\(X\subset Y\) means that \(X\sw Y\) and \(X\neq Y\) i.e.&nbsp;\(X\) is <i>strictly</i> smaller than the set \(Y\)
</p>



</div>
<!--
......        subsection Chapter <a href=Real-Analysis.html#c:real_analysis>2</a> ......
-->
<h5 id="autosec-297">Chapter <a href="Real-Analysis.html#c:real_analysis">2</a></h5>
<a id="notes-autopage-297"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-c-real_analysis.html#ps:lils_bdd"><b>2.1</b></a> We will prove the forwards implication first. If \((a_n)\) is bounded the there exists \(M\in \R \) with
\(|a_n|\leq M\) for all \(n\in \N \). Hence \(|\sup _{k\geq n} a_k|\leq M\) and \(|\inf _{k\geq n} a_k|\leq M\) for all \(n\in \N \), which (using that limits preserve weak inequalities) implies that
\(|\limsup _n a_n|\leq M\) and \(|\liminf _n a_n|\leq M\). In particular, both are real valued.
</p>
<p>
For the reverse implication, suppose that both \(\liminf _n a_n\) and \(\limsup _n a_n\) are elements of \(\R \). Hence, the sequences \(b_n=\sup _{k\geq n}a_k\) and \(c_n=\inf _{k\geq n} a_k\) are
bounded (because sequences that converge in \(\R \) are necessarily bounded). In particular, \(b_1=\sup _{k\geq 1}a_k\) and \(c_1=\inf _{k\geq a}a_k\) are elements of \(\R \), which implies that the
sequence \((a_n)\) is bounded.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-real_analysis.html#ps:ae_conv_example"><b>2.2</b></a> If \(x&lt;0\) then \(f_n(x)=0\) for all \(n\). If \(x&gt;0\) then for \(n\) large enough that \(\frac
1n&lt;x\) we have \(f_n(x)=0\). Hence \(f_n(x)\to 0\) for all \(x\neq 0\), which means \(f_n\to 0\) almost everywhere.
</p>
<p>
(Note that \(f_n(0)=n\), which does not tend to zero.)
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-real_analysis.html#ps:lims_meas"><b>2.3</b></a> We have that \(f_n(x)\to f(x)\) for all \(x\in S\sc A\), and \(g_n(x)\to g(x)\) for all \(x\in S\sc B\),
where \(m(A)=m(B)=0\). Hence \(m(A\cup B)\leq m(A)+m(B)=0\), so \(m(A\cup B)=0\). For all \(x\in S\sc (A\cup B)\) we have \(f_n(x)\to f(x)\) and \(g_n(x)\to g(x)\), hence for such \(x\) we have
\((f_n+g_n)(x)\to (f+g)(x)\) and \((f_ng_n)(x)\to (fg)(x)\). Therefore \(f_n+g_n\to f+g\) and \(f_ng_n\to fg\), both almost everywhere.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-real_analysis.html#ps:meas_sum"><b>2.4</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> For \(N\in \N \) we have \(\sum _{n=1}^N a_n + \sum _{n=1}^N b_n = \sum _{n=1}^N (a_n + b_n)\). Since \(a_n,b_n\geq 0\), all of these terms are monotone
increasing in \(N\), and therefore have limits (in \(\ov {\R }\)) by Lemma <a href="Real-Analysis.html#l:ovR_monotone_seq">2.1.1</a> as \(N\to \infty \). The result follows by uniqueness of limits.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> We check (M1) and (M2). For (M1), we have that \((m_1+m_2)(\emptyset )=m_1(\emptyset )+m_2(\emptyset )=0+0=0\).
</p>
<p>
For (M2), if \((A_n)_{n\in \N }\) is a sequence of disjoint measurable sets then by part (a) we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                                  ∞
                                                                                  X                         ∞
                                                                                                            X
                                                                                        (m1 + m2 )(An ) =         m1 (An ) + m2 (An )
                                                                                  n=1                       n=1
                                                                                                            X∞                 ∞
                                                                                                                               X
                                                                                                       =          m1 (An ) +         m2 (An )
                                                                                                            n=1                j=1
                                                                                                                                                 
                                                                                                                   ∞                      ∞
                                                                                                                           !
                                                                                                                   [                      [
                                                                                                       = m1              An + m2               An 
                                                                                                                   n=1                    j=1
                                                                                                                            ∞
                                                                                                                                      !
                                                                                                                            [
                                                                                                       = (m1 + m2 )              An .
                                                                                                                           n=1



-->


<p>


\begin{align*}
\sum \limits _{n=1}^\infty (m_1+m_2)(A_n) &amp;=\sum \limits _{n=1}^\infty m_1(A_n)+m_2(A_n) \\ &amp;=\sum \limits _{n=1}^\infty m_1(A_n)+\sum \limits _{j=1}^\infty m_2(A_n) \\
&amp;= m_1\l (\bigcup _{n=1}^\infty A_n\r )+m_2\l (\bigcup _{j=1}^\infty A_n\r ) \\ &amp;=(m_1+m_2)\l (\bigcup _{n=1}^\infty A_n\r ).
\end{align*}
Thus \(m_1+m_2\) is a measure.
</p>
</li>
<li>


<p>
<span class="textnormal">(c)</span> Combining part (b) of this question with the result of Exercise <a
href="Exercises-on-Chapter-ref-c-measure_spaces.html#ps:conditional_measure"><b>1.5</b></a> part (a), we can show (by a trivial induction) that if \(m_{1},m_{2} \ldots , m_{n}\) are measures
and \(c_{1}, c_{2}, \ldots , c_{n}\) are non-negative numbers then \(c_{1}m_{1} + c_{2}m_{2} + \cdots + c_{n}m_{n}\) is a measure. Apply this with \(m_{j} = \de _{x_{j}} (1 \leq j \leq n)\)
to obtain the result.
</p>
<p>
To get a probability measure we need \(\sum _{j=1}^{n}c_{j} = 1\). Then, as \(\de _{x}\) is a probability measure for all \(x\), we have \(m(S) = \sum _{j=1}^{n}c_{j}\de _{x_{j}}(S) = \sum
_{j=1}^{n}c_{j} = 1.\)
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-real_analysis.html#ps:indicator_funcs"><b>2.5</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(i)</span> If \(x \in A\) and \(x \in B\), the equation becomes \(1 =1 + 1 -1 = 1\),
</p>
<p>
If \(x \in A\) and \(x \notin B\), the equation becomes \(1 =1 + 0 -0 = 1\),
</p>
<p>
If \(x \notin A\) and \(x \in B\), the equation becomes \(1 =0 + 1 -0 = 1\),
</p>
<p>
If \(x \notin A\) and \(x \notin B\), the equation becomes \(0 =0 + 0 - 0 = 0\), so we have equality in all cases.
</p>
</li>
<li>


<p>
<span class="textnormal">(ii)</span> Since \(A = B \cup (A\sc B)\) and \(B \cap (A\sc B) = \emptyset \), we can apply (i) to obtain that \({\1}_{A} = {\1}_{B} + {\1}_{A\sc B}\).
</p>
</li>
<li>


<p>
<span class="textnormal">(iii)</span> Note that both sides are non-zero if and only if both \(x\in A\) and \(x\in B\), in which case both sides are equal to \(1\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="textnormal">(b)</span> The function \(\sum _1^\infty \1_{A_n}\) is defined as a pointwise limit as \(N\to \infty \) of the partial sums \(\sum _1^N \1_{A_n}\), which are themselves
defined pointwise.
</p>
<p>
For the last part, if \(x \notin A\) then \(x \notin A_{n}\) for all \(\nN \) and so lhs \(=\) rhs \(= 0\). If \(x \in A\) then \(x \in A_{n}\) for one and only one \(\nN \) and so lhs \(=\) rhs \(= 1\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-real_analysis.html#ps:alg_limsup_liminf"><b>2.6</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(i)</span> For all \(\nN \) we have \(\sup _{k \geq n}(a_{k} + b_{k}) \leq \sup _{k \geq n}a_{k} + \sup _{k \geq n}b_{k}\). Taking limits on both sides gives
\(\limsup _n (a+n+b_n)\leq \limsup _n a_n + \limsup _n b_n\).
</p>


</li>
<li>


<p>
<span class="textnormal">(ii)</span> Note that \(\left (\sup _{k \geq n}a_{k}\right )\left (\sup _{k \geq n}b_{k}\right )\leq \sup _{k \geq n}(a_{k}b_{k})\) and take limits on both sides.
</p>


</li>
<li>


<p>
<span class="textnormal">(iii)</span> Note that \(\sup _{k \geq n} ca_{k} = c \sup _{k \geq n}a_{k}\) because \(c\geq 0\), and take limits on both sides.
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="textnormal">(b)</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(i)</span> Putting \((-a_n)\) and \((-b_n)\) in place of \((a_n)\) and \((b_n)\) in part (a)(i), Lemma <a href="Liminf-limsup.html#l:liminf=-limsup">2.2.3</a> gives that
\((-\liminf _n a_n) + (-\liminf _n b_n) \leq -\liminf _n (a+n+b_n)\), which gives \(\liminf _n (a_n+b_n)\leq \liminf _n a_n + \liminf _n b_n\).
</p>


</li>
<li>


<p>
<span class="textnormal">(ii)</span> Note that \(\left (\inf _{k \geq n}a_{k}\right )\left (\inf {k \geq n}b_{k}\right )\geq \sup _{k \geq n}(a_{k}b_{k})\) and take limits on both sides as
in (a)(ii).
</p>


</li>
<li>


<p>
<span class="textnormal">(iii)</span> Putting \((-a_n)\) in place of \((a_n)\) in part (a)(iii), Lemma <a href="Liminf-limsup.html#l:liminf=-limsup">2.2.3</a> gives that \(-\liminf _n ca_n =
c(-\liminf _n a_n)\), hence \(\liminf _n ca_n = c\liminf _n a_n\).
</p>
</li>
</ul>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-real_analysis.html#ps:unif_vs_pointwise"><b>2.7</b></a> It is clear that \(e^{-nx^2}\to 0\) as \(n\to \infty \) for all \(x\neq 0\), and that
\(e^{-n0^2}=e^0=1\) for all \(n\). Hence \(f_n\to \1_{\{0\}}\) pointwise.
</p>
<p>
If the convergence was uniform then, from real analysis, for any sequence \(a_n\to a\) we would have that \(f_n(a_n)\to f(a)\) as \(n\to \infty \). However, if we take \(a_n=\frac {1}{\sqrt {n}}\) then
\(f_n(a_n)=e^{-1}\), which does not converge to \(f(\lim _n a_n)=f(0)=1\).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-real_analysis.html#ps:ovR_ms"><b>2.8</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> It is clear that \(d(x,y)=d(y,x)\) and that \(d(x,x)=0\). If \(x\neq y\) then \(d(x,y)&gt;0\), because \(\arctan \) is strictly increasing on \([-\infty ,\infty ]\).
For the triangle law we use the triangle law for \(\R \) to deduce that
</p>
<p>
\[d(x,z)=|\arctan (x)-\arctan (z)|\leq |\arctan (x)-\arctan (y)|+|\arctan (y)-\arctan (z)|=d(x,y)+d(y,z).\]
</p>
<p>
<i>There is nothing special about \(\arctan \) here. Any function that is a strictly increasing map from \(\ov {\R }\) to a bounded subset of \(\R \) will do.</i>
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> Recall that a metric space is compact if and only if it sequentially compact. Let \((a_n)\) be a sequence in \(\ov {\R }\). If \((a_n)\) within \(\R \) and \((a_n)\) is
bounded, then \((a_n)\) has a subsequence convergent to some \(a\in \R \) (by the Heine-Borel theorem). Alternatively, if \((a_n)\) is unbounded then there it contains a subsequence \((a_{r_n})\) such that
\(|a_{r_n}|\to \infty \). In particular there must be a subsequence that converges to \(+\infty \) or to \(-\infty \). Thus \(\ov {\R }\) is sequentially compact, and thus compact.
</p>


</li>
<li>


<p>
<span class="textnormal">(c)</span> If \((a_n)\) converges then any subsequence of \((a_n)\) converges to the same limit. Hence (i) \(\ra \) (ii).
</p>
<p>
For the reverse implication, let \((a_n)\) be a sequence in \(\ov {\R }\) and assume (ii). Recall that a metric space is sequentially compact if and only if it is compact, hence \(\ov {\R }\) is sequentially compact.
We will argue by contradiction: suppose that \((a_n)\) does not converge to \(a\). Then there exists \(\eps &gt;0\) and an infinite subsequence \((a_{r_n})\) of \((a_n)\) such that \(|a_{r_n}-a|\geq \eps
\) for all \(n\in \N \). By sequential compactness \((a_{r_n})\) has a convergent subsequence. By (ii), this convergent subsequence converges to \(a\), which is a contradiction. Hence in fact \(a_n\to a\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-real_analysis.html#ps:liminf_limsup_subseqs"><b>2.9</b></a> It suffices to show that \(\liminf _n a_n=\inf \mathscr {L}\). The corresponding result for
\(\limsup \) follows by multiplying both sides by \(-1\) and using Lemma <a href="Liminf-limsup.html#l:liminf=-limsup">2.2.3</a>.
</p>
<p>
Let \((a_{r_n})\) be a convergent subsequence of \(a_n\). Note that this implies \(r_n\geq n\). It follows immediately that \(\inf _{k\geq n} a_k\leq \inf _{k\geq n} a_{r_k}\). Hence also \(\liminf
_n a_n \leq \liminf _n a_{r_n}\). By Lemma <a href="Liminf-limsup.html#l:lim_lininf_limsup">2.2.2</a> we have \(\liminf _n a_{r_n}=\lim _n a_n\). Since \((a_{r_n})\) was an arbitrary
convergent subsequence we thus have \(\liminf _n a_n\leq \inf \mathscr {L}\).
</p>
<p>
To complete the proof we need to show the reverse inequality. We will argue by contradiction. Suppose that \(\liminf _n a_n &lt; \inf \mathscr {L}\). Let us write \(a=\liminf _n a_n\). Let \(\eps
&gt;0\) be such that \(a+\eps &lt; \inf \mathscr {L}\). By definition of \(\liminf \), there exists \(N\in \N \) such that for all \(n\geq \N \) we have \(\inf _{k\geq n} a_k \leq a+\eps \). Hence we
can define a subsequence \((a_{r_n})\) of \((a_n)\) by setting
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--



                                                                                                r1 = inf{k ≥ N ; ak ≤ a + ϵ},

                                                                                              rn+1 = inf{k > rn ; an ≤ a + ϵ}.



-->


<p>


\begin{align*}
r_1 = \inf \{k\geq N\- a_k\leq a +\eps \},\\ r_{n+1} = \inf \{k&gt;r_n\- a_n\leq a +\eps \}.
\end{align*}
We have \(a_{r_n}\leq a+\eps \) for all \(n\), and by part (c) of Exercise <a href="Exercises-on-Chapter-ref-c-real_analysis.html#ps:ovR_ms"><b>2.8</b></a> the sequence \((a_{r_n})\) has a
convergent subsequence. The limit of this subsequence must be less that or equal to \(a+\eps \), which implies that \(\mathscr {L}\leq a+\eps &lt;\mathscr {L}\). This is a contradiction, which completes
the proof.
</p>
<p>


</p>
</li>
</ul>
<!--
......   subsection Chapter <a href=Measurable-Functions.html#c:measurable_funcs>3</a> ......
-->
<h5 id="autosec-298">Chapter <a href="Measurable-Functions.html#c:measurable_funcs">3</a></h5>
<a id="notes-autopage-298"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-c-measurable_funcs.html#ps:meas_borel_examples"><b>3.1</b></a> Note that there are very many different ways to solve the various parts of this question, using the
results in Sections <a href="Measurable-Functions.html#s:meas_funcs_overview">3.1</a> and <a href="Borel-measurable-functions.html#s:meas_funcs_borel">3.2</a>.
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We have \(f^{-1}([c,\infty ))=\emptyset \) if \(\alpha &lt;c\) and \(f^{-1}([c,\infty ))=\R \) if \(c\leq \alpha \). In both cases, \(f^{-1}([c,\infty
))\) is Borel.
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> We can write \(g(x)=\1_{[0,\infty )}(x)e^x\). Indicator functions of measurable sets are measurable, and \(x\mapsto e^x\) is continuous, hence measurable.
Products of measurable functions are measurable, hence \(g\) is measurable.
</p>


</li>
<li>


<p>
<span class="textnormal">(c)</span> \(x\mapsto \sin (\cos x)\) is a continuous function, because the composition of continuous functions is also continuous. Hence \(h\) is measurable.
</p>


</li>
<li>


<p>
<span class="textnormal">(d)</span> The function \(\sin \) is continuous, hence measurable. A similar method to (b) shows that \(x^2\1_{[0,\infty )}(x)\) is measurable. The composition of a Borel
measurable functions is measurable, hence \(i\) is measurable.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measurable_funcs.html#ps:open_int_meas"><b>3.2</b></a> If \(f\) is measurable then \(f^{-1}((a,b))\in \Sigma \), as \((a,b)\in \mc {B}(\R )\).
</p>
<p>
For the reverse implication, we have that \(f^{-1}((a,b))\in \Sigma \) for all \(-\infty \leq a&lt;b\leq \infty \). Take \(b=\infty \) and we have \(f^{-1}((a,\infty ))\in \Sigma \) for all \(a\in
\R \). From this, Lemma <a href="Measurable-Functions.html#l:meas_func_reductions">3.1.4</a> gives that \(f\) is measurable.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measurable_funcs.html#ps:abs_meas"><b>3.3</b></a> Note that \(|f|=\max (0,f)+\max (0,-f)\). Theorem <a
href="Measurable-Functions.html#t:alg_meas_funcs">3.1.5</a> implies that all parts of this formula are operations that preserve measurability, hence \(|f|\) is measurable.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measurable_funcs.html#ps:meas_borel"><b>3.4</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> For any \(a&gt;0\), we have
</p>
<p>
\[h^{-1}((a,\infty ))=\{x\in \R \- f(x+y)&gt;(a,\infty )\}=\{z-y\in \R \-f(z)&gt;a\}=(f^{-1}((a,\infty )))_{-y}.\]
</p>
<p>
Here we use the notation \(A_y=\{a+y\-a\in A\}\) from Section <a href="The-Borel-field.html#s:borel_field">1.4</a>. Using that \(A_y\in \mc {B}(\R )\) whenever \(A\in \mc {B}(\R )\), we have
that \(h^{-1}((a,\infty ))\in \mc {B}(\R )\), and hence \(h\) is measurable.
</p>
<p>
<i>Alternative:</i> Write \(h = f \circ \tau _{y}\) where \(\tau _{y}(x) = x + y\). The mapping \(\tau _{y}\) is continuous and hence measurable and so \(h\) is measurable by Lemma <a
href="Borel-measurable-functions.html#l:cts_is_meas">3.2.1</a>.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> If \(f\) is differentiable then it is continuous, hence also measurable by Lemma <a href="Borel-measurable-functions.html#l:cts_is_meas">3.2.1</a>.
</p>
<p>
For each \(x \in \R \) we have \(f^{\prime }(x) = \lim _{h \rightarrow 0}\frac {f(x + h) - f(x)}{h}\). Note that \(x \rightarrow f(x+h)\) is measurable by part (a), so \(x \rightarrow \frac
{f(x + h) - f(x)}{h}\) is measurable by Theorem <a href="Measurable-Functions.html#t:alg_meas_funcs">3.1.5</a>. Hence \(f^{\prime }\) is also measurable, again by Theorem <a
href="Measurable-Functions.html#t:alg_meas_funcs">3.1.5</a>.
</p>
</li>
<li>


<p>
<span class="textnormal">(c)</span> Recall that we have shown all intervals (i.e.&nbsp;sets of the form \((a,b)\), \([a,b)\) and so on) are Borel sets. Another way to describe intervals is that \(I\sw \R \) is
an interval if, whenever \(a,b\in \R \) and \(a&lt;c&lt;b\) we have \(c\in I\).
</p>
<p>
Suppose \(f\) is monotone increasing. Fix \(c\in \R \) and consider \(I=f^{-1}((c,\infty ))\). We want to show that \(I\) is an interval. Let \(a,b\in I\), so that we have \(f(a),f(b)&gt;c\), and let
\(a&lt;d&lt;b\). We have \(a\leq d\) so, by monotonicity of \(f\) we have \(f(a)\leq f(d)\). Thus \(f(d)&gt;c\), so \(d\in I\). Hence \(I\) is an interval, so \(I\in \mc {B}(\R )\). We thus have that
\(f^{-1}((c,\infty ))\in \Sigma \) for all \(c\in \R \), so \(f\) is measurable by Lemma <a href="Measurable-Functions.html#l:meas_func_reductions">3.1.4</a>.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measurable_funcs.html#ps:meas_without_alg"><b>3.5</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> For any \(a\in \R \), we have \((f+\alpha )^{-1}((a,\infty ))=\{x\in \R \-f(x)+\alpha &gt;a\}=\{x\in \R \-f(x)&gt;a-\alpha \}=f^{-1}((a-\alpha
,\infty ))\). Hence \((f+\alpha )^{-1}((a,\infty ))\in \Sigma \) by measurability of \(f\), which by Lemma <a href="Measurable-Functions.html#l:meas_func_reductions">3.1.4</a> implies that
\(f\) is measurable.
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> First note that if \(\alpha =0\) then \((\alpha f)(x)= 0\) for all \(x\), so in this case \(f\) is measure by <a
href="Exercises-on-Chapter-ref-c-measurable_funcs.html#ps:meas_borel_examples"><b>3.1</b></a> part (a).
</p>
<p>
Consider when \(\alpha &gt;0\). For any \(a\in \R \), we have \((\alpha f)^{-1}((a,\infty ))=\{x\in \R \-\alpha f(x)&gt;a\}=\{x\in \R \-f(x)&gt;a/\alpha \}=f^{-1}((a/\alpha ,\infty
))\). For \(k&lt;0\), similarly \((\alpha f)^{-1}((a,\infty ))=\) \((\alpha f)^{-1}((a,\infty ))=\{x\in \R \-\alpha f(x)&gt;a\}=\{x\in \R \-f(x)&lt;a/\alpha \}=f^{-1}((-\infty ,a/\alpha
))\). In both cases there are measurable sets, so Lemma <a href="Measurable-Functions.html#l:meas_func_reductions">3.1.4</a> gives that \(\alpha f\) is measurable.
</p>


</li>
<li>


<p>
<span class="textnormal">(c)</span> Note that \((G \circ f)^{-1}(A) = f^{-1}(G^{-1}(A))\). For \(A\in \mc {B}(\R )\), measurability of \(G\) implies that \(G^{-1}(A)\in \mc {B}(\R )\), and
measurabilty of \(f\) thus implies that \(f^{-1}(G^{-1}(A))\in \Sigma \). Hence \(G\circ f\) is measurable.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measurable_funcs.html#ps:simple_funcs_vector_space"><b>3.6</b></a> We need to show that if \(f\) and \(g\) are simple functions and \(\alpha ,\beta \in
\R \), then \(\alpha f+\beta g\) is also a simple function. This follows from the same calculation as leads to equation <span class="textup">(<a
href="Lebesgue-Integration.html#eq:simple_func_linearity">4.4</a>)</span>, which is the first step in the proof of Lemma <a
href="Lebesgue-Integration.html#l:leb_int_step_1_properties">4.1.2</a>.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measurable_funcs.html#ps:open_sets"><b>3.7</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Let \(x\in O_1\cup O_2\). Consider if \(x\in O_1\), then there is an open interval \(I_1\) containing \(x\). Thus \(I_1\) is an open interval within \(O_1\cup O_2\)
containing \(x\). We can do the same for \(x\in O_2\), then with \(x\in I_2\sw O_2\), hence \(O_1\cup O_2\) is open.
</p>
<p>
Now let \(x\in O_1\cap O_2\). Then for each \(i=1,2\) we have an open interval \(I_i\sw O_i\) containing x. Let us write \(I_1=(a_1,b_1), I_2=(a_2,b_2)\), and \(c_1=\max (a_1,b_1)\), \(c_2=\min
(a_2,b_2)\). Then \((c_1,c_2)=I_1\cap I_2\), and since \(x\in I_1\cap I_2\) we have \(x\in (c_1,c_2)\). In particular this means \(c_1&lt;c_2\), so \(I_1\cap I_2\) is an open interval. Also
\(I_1\cap I_2\sw O_1\cap O_2\), so \(O_1\cap O_2\) is open.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span>
</p>
<ul style="list-style-type:none">


<li>
<p>
1. This is true. We can use exactly the same method as in part (a): let \(x\in \cup _n O_n\), and the assume \(x\in O_1\) (or use \(O_i\) in place of \(O_1\)), then we have an open interval \(I_1\sw O_1\)
containing \(x\), then \(I_1\sw \cup _n O_n\), and we are done.
</p>


</li>
<li>


<p>
2. This is false. A counterexample is given by \(O_n=(\frac {-1}{n},1+\frac {1}{n})\), for which \(\cap _n O_n=[0,1]\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="textnormal">(c)</span> Let \((C_n)_{n\in \N }\) be a sequence of closed sets. Then \(\R \sc C_n\) is open, for each \(n\). Using set operations we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--



                                                                                        R \ (C1 ∪ C2 ) = (R \ C1 ) ∩ (R \ C2 )

                                                                                        R \ (C1 ∩ C2 ) = (R \ C1 ) ∪ (R \ C2 )
                                                                                                        !
                                                                                              [                 \
                                                                                         R\        Cn       =       (R \ Cn )
                                                                                               n                n
                                                                                                        !
                                                                                              \                 [
                                                                                         R\        Cn       =       (R \ Cn )
                                                                                               n                n



-->


<p>


\begin{align*}
\R \sc (C_1\cup C_2)&amp;=(\R \sc C_1)\cap (\R \sc C_2)\\ \R \sc (C_1\cap C_2)&amp;=(\R \sc C_1)\cup (\R \sc C_2)\\ \R \sc \l (\bigcup _n C_n\r )&amp;=\bigcap _n\l (\R \sc C_n\r
)\\ \R \sc \l (\bigcap _n C_n\r )&amp;=\bigcup _n\l (\R \sc C_n\r )
\end{align*}
The first two equations combined with part (a) tell us that both the results of part (a) carry over to closed sets: both \(C_1\cap C_2\) and \(C_1\cup C_2\) are closed.
</p>
<p>
From the fourth equation, since \(\R \sc C_n\) is open (for all \(n\)), using (b)(i) we see that \(\R \sc \l (\bigcup _n C_n\r )\) is also open, hence \(\bigcap _n C_n\) is closed.
</p>
<p>
However, we can’t do the same for the third equation, because (b)(ii) was false. Instead, we can take complements of our counterexample in (b)(ii) to find a counterexample here, giving \(C_n=\R \sc (\frac
{-1}{n},1+\frac {1}{n})=(-\infty ,\frac {-1}{n}]\cup [1+\frac {1}{n},\infty )\). Then \(\cup _nC_n=(-\infty ,0)\cup (1,\infty )\) which is not closed (because its complement \([0,1]\) is
not open).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-measurable_funcs.html#ps:upper_sc"><b>3.8</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> It is sufficient to consider the case \(x = a\). Then for any \(\eps &gt; 0\) and arbitrary \(\de , f(a - \de ) = 0 &lt; f(a) + \eps = 1 + \eps \) and \(f(a +
\de ) = f(a) &lt; f(a) + \eps = 1 + \eps \).
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> Its sufficient to consider the case \(x = n\) for some integer \(n\). Again for any \(\eps &gt; 0\) and arbitrary \(\de , f(n-\de ) = n-1 &lt; f(n) + \eps = n +
\eps \) and \(f(n+\de ) = n &lt; f(n) + \eps = n + \eps \).
</p>


</li>
<li>


<p>
<span class="textnormal">(c)</span> Let \(U = f^{-1}((-\infty , a))\). We will show that \(U\) is open. Then it is a Borel set and \(f\) is measurable. Fix \(x \in U\) and let \(\eps = a - f(x)\).
Then there exists \(\de &gt; 0\) so that \(|x - y| &lt; \de \Rightarrow f(y) &lt; f(x) + \eps = a\) and so \(y \in U\). We have shown that for each \(x \in U\) there exists an open interval (of radius
\(\de \)) so that if \(y\) is in this interval then \(y \in U\). Hence \(U\) is open.
</p>
</li>
</ul>
</li>
</ul>
<!--
......     subsection Chapter <a href=Lebesgue-Integration.html#c:lebesgue_integration>4</a> ......
-->
<h5 id="autosec-299">Chapter <a href="Lebesgue-Integration.html#c:lebesgue_integration">4</a></h5>
<a id="notes-autopage-299"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:simple_func_ints"><b>4.1</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> \(f=2\1_{[-2,-1]}-\1_{(-1,1)}+3\1_{[1,2)}-5\1_{[2,3)}\)
</p>
<p>
\(\int _{\R } f\,dm = 2((-1)-(-2))-1(1-(-1))+3(2-1)-5(3-2) = -2\)
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> \(f_+=2\1_{[-2,-1]}+3\1_{[1,2)}\) and \(f_-=\1_{(-1,1)}+5\1_{[2,3)}\)
</p>
<p>
\(\int _{\R } f_+\,dm = 2((-1)-(-2))+3(2-1) = 5\) and \(\int _{\R } f_-\,dm = 1(1-(-1))+5(3-2) = 7\)
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:simple_func_restriction"><b>4.2</b></a> Let us write \(f = \sum _{i=1}^{n}c_{i}{\1}_{A_{i}}\), where the \(A_i\) are
disjoint measurable sets. Then
</p>
<p>
\[{\1}_{A}f = \sum _{i=1}^{n}c_{i}{\1}_{A_{i}}{\1}_{A} = \sum _{i=1}^{n}c_{i}{\1}_{A_{i}\cap A},\]
</p>
<p>
where we have used \(\1_A\1_B=\1_{A\cap B}\) (which might be obvious, but it is also from Exercise <a href="Exercises-on-Chapter-ref-c-real_analysis.html#ps:indicator_funcs"><b>2.5</b></a>).
</p>
<p>
Note that if \(i\neq j\) then \((A_i\cap A)\cap (A_j\cap A)=A\cap (A_i\cap A_j)=A\cap \emptyset =\emptyset \), so the \(A\cap A_i\) are disjoint sets. Hence \(\1_A f\) is a simple function.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:chebyshev"><b>4.3</b></a> Note that \(f^2\) is a non-negative function. Applying Lemma <a
href="The-Lebesgue-integral-non-negative-measurable-functions.html#l:markov_ineq">4.2.3</a> to \(f^2\) gives
</p>
<p>
\[m(\{x\in S\- |f(x)|\geq c\})=m(\{x\in S\- |f(x)^2|\geq c^2\})\leq \frac {1}{c^2}\int _S f^2\,dm.\]
</p>
<p>
Similarly, applying it to \(|f|^p\) gives \(m(\{x\in S\- |f(x)|\geq c\})\leq \frac {1}{c^p}\int _S |f|^p\,dm.\) Note that in general we need to use \(|f|\) instead of \(f\) to ensure non-negativity.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:t_leb_int_properties"><b>4.4</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Noting \(f_+\) and \(f_-\) are both non-negative, with non-negative integrals, we have
</p>
<p>
\[\l |\int _S f\,dm\r | = \l |\int _S f_+\,dm - \int _S f_-\,dm\r | \leq \int _S f_+\,dm + \int _S f_-\,dm = \int _S |f|\,dm.\]
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> By the triangle inequality we have \(|f(x)+g(x)|\leq |f(x)|+|g(x)|\). Thus by monotonicity and linearity for integrals of non-negative functions (Lemmas <a
href="The-Lebesgue-integral-non-negative-measurable-functions.html#l:leb_int_step_2_monotonicity">4.2.2</a> and Lemma <a
href="The-monotone-convergence-theorem.html#l:leb_int_step_2_linearity">4.3.2</a>) we have
</p>
<p>
\[\int _S |f+g|\,dm\leq \int _S |f|+|g|\,dm = \int _S |f|\,dm + \int _S |g|\,dm.\]
</p>
</li>
<li>


<p>
<span class="textnormal">(c)</span> Using linearity for integrals of non-negative functions (Lemma <a href="The-monotone-convergence-theorem.html#l:leb_int_step_2_linearity">4.3.2</a>), if \(c
\geq 0\) then
</p>
<p>
\[\int _{S}cf\,dm = \int _{S}cf_{+}\,dm - \int _{S}cf_{-}\,dm = c\int _{S}f_{+}\,dm - c\int _{S}f_{-}\,dm = c\int _{S}f \,dm.\]
</p>
<p>
If \(c = -1\) then \((-f)_{+} = f_{-}\) and \((-f)_{-} = f_{+}\), so
</p>
<p>
\[ \int _{S}(-f)\,dm = \int _{S}f_{-}\,dm - \int _{S}f_{+}\,dm = -\left (\int _{S}f_{+}\,dm - \int _{S}f_{-}\,dm\right ) = -\int _{S}f\,dm.\]
</p>
<p>
Finally for general \(c &lt; 0\) write \(c =(-1)\times d\) where \(d &gt; 0\) and use the two cases we’ve just proved.
</p>
</li>
<li>


<p>
<span class="textnormal">(d)</span> If \(f \leq g\) then \(g - f \geq 0\) so by monotonicty for integrals of non-negative functions (Lemma <a
href="The-Lebesgue-integral-non-negative-measurable-functions.html#l:leb_int_step_2_monotonicity">4.2.2</a>), \(\int _{S}(g-f)dm \geq 0\). By linearity (c.f.&nbsp;the hint) this is equivalent
to \(\int _{S}gdm - \int _{S}f\,dm \geq 0\), which gives \(\int _{S}g\,dm \geq \int _{S}fdm\) as required.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:L1_finite_meas"><b>4.5</b></a> Suppose that \(|f|\leq C\) where \(C\in [0,\infty )\). By monotoncity and linearity (from
Lemmas <a href="The-Lebesgue-integral-non-negative-measurable-functions.html#l:leb_int_step_2_monotonicity">4.2.2</a> and <a
href="The-monotone-convergence-theorem.html#l:leb_int_step_2_linearity">4.3.2</a>) we have \(\int _S |f|\,dm \leq \int _S C\,dm=C\int _S 1\,dm = Cm(S)&lt;\infty .\) Hence \(f\in \mc
{L}^1\).
</p>
<p>
Note that we do not use Theorem <a href="The-Lebesgue-integral.html#t:leb_int_properties">4.5.3</a> here because, at the point where we need monotonicty and linearity, we do not yet know that
\(f\in \mc {L}^1\).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:L1_MCT"><b>4.6</b></a> By Riemann integration, we have
</p>
<p>
\[\int _{1/n}^1 \log x\,dx=\l [x\log x - x\r ]_{1/n}^1=(-1)-\l (\frac {1}{n}\log \frac {1}{n}-\frac {1}{n}\r )=\frac {1+\log n}{n} -1.\]
</p>
<p>
Noting that \(\log x\in (-\infty ,0)\) for \(x\in (0,1)\), multiplying the above by \(-1\) gives
</p>
<p>
\[\int _{1/n}^1|\log x|\,dx=1-\frac {1+\log n}{n}.\]
</p>
<p>
We have that \(g_n(x)=|\log x|\1_{x\in (1/n,1)}\) is a monotone increasing sequence of non-negative functions, with pointwise convergence to \(g(x)=|\log x|\) for \(x\in (0,1)\). Hence, by the
monotone convergence theorem,
</p>
<p>
\[\int _0^1|\log x|\,dx=\lim _{n\to \infty }\l (1-\frac {1+\log n}{n}\r )=1.\]
</p>
<p>
Thus \(\log x\) is in \(\mc {L}^1\) on \((0,1)\).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:int_nonconv_1"><b>4.7</b></a> Let \(x \in \R \) be arbitrary. Then we can find \(n_{0} \in \mathbb {N}\) so that \(\frac
{1}{n_{0}} &lt; |x|\) and then for all \(n \geq n_{0}, f_{n}(x) = n{\1}_{(0, 1/n)}(x) = 0\). So we have proved that \(\lim _{n \rightarrow }f_{n}(x) = 0\). But for all \(\nN \)
</p>
<p>
\[ \int _{\R }|f_{n}(x) - 0|dx = n\int _{\R }{\1}_{(0, 1/n)}(x)dx = n\frac {1}{n} = 1,\]
</p>
<p>
and so we cannot find any function in the sequence that gets arbitrarily close to \(0\) in the \({\cal L}_{1}\) sense.
</p>
<p>
The MCT does not apply here because \((f_n)\) is not monotone. The DCT does not apply because, if it did, then the DCT would give \(\int _\R f_n \to \int _\R f=0\) which is not true! We conclude that
there is no dominating integrable function for \((f_n)\), because all the other conditions for the DCT do hold.
</p>
<p>
\((\star )\) Extension: Fatou’s lemma does apply, and would give \(\int _\R \liminf _n f_n=\int _\R 0 \leq \liminf _n\int _\R f_n = \liminf _n 1=1\) which we already knew because we could
calculate the integrals explicitly in this case.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:cos_int"><b>4.8</b></a> Since \(|\cos (\cdot )| \leq 1\) we have \(|\cos (\alpha x)f(x)| \leq |f(x)|\) for all \(x \in \R
\). Lemma <a href="The-dominated-convergence-theorem.html#l:dom_int">4.6.1</a> thus gives that this function is in \(\mc {L}^1\).
</p>
<p>
Let \(f_n(x)=\cos (x/n)f(x)\). Note that \(\lim _{n \rightarrow \infty }\cos (x/n) = \cos (0) = 1\) for all \(x \in \R \). Hence \(f_n\to f\) pointwise. Moreover, \(f\in \mc {L}^1\) is a
dominating function for the sequence \((f_n)\), so we may use the dominated convergence theorem to deduce that
</p>
<p>
\[ \lim _{n \rightarrow \infty }\int _{\R }\cos (x/n)f(x)dx = \int _{\R } \lim _{n \rightarrow \infty }\cos (x/n)f(x)dx = \int _{\R }f(x)dx.\]
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:int_on_incr_set"><b>4.9</b></a> Let \(g=|f|\1_A\) and \(g_n=|f|\1_{\bigcup _{i=1}^n A_i}\). Thus \(0\leq g_n\leq
g_{n+1}\) and \(g_n\to g\) pointwise. From the monotone convergence theorem we have \(\int _S g_n\,dm \to \int _S g\,dm\). Note that \(\int _S g\,dm=\int _A |f|\,dm\). Since the \(A_i\) are
disjoint we have \(\1_{\bigcup _{i=1}^n A_i}=\sum _{i=1}^n\1_{A_i}\). Hence by linearity we have \(g_n=|f|\sum _{i=1}^n \1_{A_i}=\sum _{i=1}^n |f|\1_{A_i}\), so
</p>
<p>
\[\int _S g_n\,dm = \sum _{i=1}^n \int _S |f|\1_{A_i}\,dm = \sum _{i=1}^n \int _{A_i} f\,dm.\]
</p>
<p>
Fitting all this together, we have shown that
</p>
<p>
\[\lim _{n\to \infty }\sum _{i=1}^n\int _{A_i}|f|\,dm = \int _A |f|\,dm.\]
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:series_int"><b>4.10</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We can write
</p>
<p>
\[a^{(N)}_n=\sum \limits _{i=1}^N a_i \1_{\{i\}}(n).\]
</p>
<p>
Noting that \(\{i\}\) is measurable and \(a_i\in \R \), this is a simple function. Since \(\#(\{i\})=1\), using <span class="textup">(<a
href="Lebesgue-Integration.html#eq:leb_int_simple">4.3</a>)</span> we have
</p>
<p>
\[\int _\N a^{(N)}\,d\#=\sum \limits _{i=1}^N a_i.\]
</p>
<p>
We have \(a^{(N)}\to a\) pointwise as \(N\to \infty \), and \(0\leq a^{(N)}\leq a^{(N+1)}\), so by the monotone convergence theorem
</p>
<p>
\[\int _\N a\,d\#=\lim _{N\to \infty }\int _\N a^{(N)}\,d\#=\sum \limits _{i=1}^\infty a_i,\]
</p>
<p>
as required.
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> We have that \(a\in \mc {L}^1\) if and only if \(|a|\in \mc {L}^1\), which from part (a) occurs if and only if \(\sum _n |a_n|&lt;\infty \). That is, \(a\in \mc
{L}^1\) if and only if it is absolutely convergent.
</p>
<p>
Lastly, writing \(a=a_+-a_-\) we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                                                Z                   Z                       Z
                                                                                                         a d# =             a+ d# −                   a− d#
                                                                                                     N              N                         N
                                                                                                                    ∞
                                                                                                                    X                                   ∞
                                                                                                                                                        X
                                                                                                                =            max(an , 0) −                      max(−an , 0)
                                                                                                                    n=1                                n=1
                                                                                                                    X∞
                                                                                                                =            max(an , 0) − max(−an , 0)
                                                                                                                    n=1
                                                                                                                    X∞
                                                                                                                =            an .
                                                                                                                    n=1



-->


<p>


\begin{align*}
\int _\N a\,d\# &amp;=\int _\N a_+\,d\#-\int _N a_-\,d\# \\ &amp;=\sum \limits _{n=1}^\infty \max (a_n,0)-\sum \limits _{n=1}^\infty \max (-a_n,0) \\ &amp;=\sum \limits
_{n=1}^\infty \max (a_n,0)-\max (-a_n,0) \\ &amp;=\sum \limits _{n=1}^\infty a_n.
\end{align*}
Here, the third line follows from the second using absolute convergence, which allows us to rearrange infinite series.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:ae_equiv"><b>4.11</b></a> Reflexivity is obvious as \(f(x) = f(x)\) for all \(x \in S\). So is symmetry, because \(f(x) \eqae
g(x)\) if and only if \(g(x) \eqae f(x)\). For transitivity, let \(A = \{x \in S; f(x) \neq g(x)\}, B = \{x \in S; g(x) \neq h(x)\}\) and \(C = \{x \in S; f(x) \neq h(x)\}\). Then \(C
\subseteq A \cup B\) and so \(m(C) \leq m(A) + m(B) = 0\). Thus if \(f\eqae g\) and \(g\eqae h\) we have \(f\eqae h\).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:rev_fatou"><b>4.12</b></a> Note that \(f - f_{n} \geq 0\) for all \(\nN \), so by Fatou’s lemma
</p>
<p>
\[ \li \int _{S} (f - f_{n})\,dm \geq \int _{S} \li (f - f_{n})\,dm.\]
</p>
<p>
Rearranging both sides,
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                                 Z                              Z                             Z                      Z
                                                                                     f dm + lim inf                 (−fn ) dm ≥                       f dm +              lim inf (−fn ) dm,
                                                                                 S                    n→∞        S                                                    S n→∞
                                                                                                                Z                           ZS
                                                                                                lim inf −               fn dm ≥                       lim inf (−fn ) dm.
                                                                                                 n→∞                S                           S n→∞



-->


<p>


\begin{align*}
\int _{S}f \,dm + \li \int _{S}(-f_{n})\, dm &amp;\geq \int _{S}f \,dm + \int _{S}\li (- f_{n})\,dm, \\ \li -\left (\int _{S}f_{n}\,dm\right ) &amp;\geq \int _{S}\li (-
f_{n})\,dm.
\end{align*}
Multiplying both sides by \(-1\) reverses the inequality to yield
</p>
<p>
\[ -\li -\left (\int _{S}f_{n}\,dm\right ) \leq \int _{S}\left (-\li (- f_{n})\right )\,dm.\]
</p>
<p>
Hence by Lemma <a href="Liminf-limsup.html#l:liminf=-limsup">2.2.3</a> we have
</p>
<p>
\[ \ls \int _{S}f_{n}\,dm \leq \int _{S}\ls f_{n} \,dm.\]
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:dct_C"><b>4.13</b></a> <i><b>Dominated convergence theorem, complex version.</b> Let \(f_n,f\) be functions from
\(S\) to \(\C \). Suppose that \(f_n\) is measurable and:<br />
      1. There is a function \(g\in \Lone _\C \) such that \(|f_{n}| \leq |g|\) almost everywhere. <br />
      2. \(f_n\to f\) almost everywhere.<br />
Then \(f\in \Lone _\C \) and<br />
\[ \int _{S} f_{n}dm\to \int _{S} f dm \]<br />
as \(n\to \infty \).</i>
</p>
<p>
To prove this, note that \(f_n\to f\) implies that \(\Re (f_n)\to \Re (f)\) and \(\Im (f_n)\to \Im (f)\), all almost everywhere (in \(\C \) or \(\R \) as appropriate). The function \(|g|:S\to \R \)
satisfies \(\int _S |g|\,dm&lt;\infty \) because \(g\in \Lone _\C \). We have \(|\Re (f_n)|\leq |f|\leq |g|\) and \(|\Im (f_n)|\leq |f|\leq |g|\), hence \(|g|\) serves as a dominating function to
apply the DCT to real and imaginary parts of \(f\). The result then follows by linearity.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:int_C_calc"><b>4.14</b></a> We have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                                      Z     x                       Z    x                             Z    x
                                                                                                     iay
                                                                                                ie         dy = i            cos(ay) dy −                       sin(ay) dy
                                                                                        0                            0                                  0
                                                                                                                  1               1
                                                                                                               = i (sin(ax) − 0) − (− cos(ax) − (−1))
                                                                                                                  a               a
                                                                                                                 1  iax    
                                                                                                               =    e −1 .
                                                                                                                 a


-->


<p>


\begin{align*}
\int _0^x ie^{iay}\,dy &amp; = i\int _0^x \cos (ay)\,dy - \int _0^x \sin (ay)\,dy \\ &amp;= i\frac {1}{a}(\sin (ax)-0)-\frac {1}{a}(-\cos (ax)-(-1)) \\ &amp;= \frac {1}{a}\l
(e^{iax}-1\r ).
\end{align*}


</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:R_to_C"><b>4.15</b></a> All of them do, where we treat \(|\cdot |\) as the complex modulus and replace functions with \(\C \)
valued equivalents. Details are left for you.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:ints_cts"><b>4.16</b></a> Define \(f_{n}(x) = f(t_{n}, x)\) for each \(\nN , x \in S\). Then \(|f_{n}(x)| \leq g(x)\) for
all \(x \in S\). Since \(g\) is integrable, by dominated convergence
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                                                           Z                                  Z
                                                                                                lim            f (tn , x)dm(x) =                       lim fn (x)dm(x)
                                                                                                n→∞ S                                             S n→∞
                                                                                                                                              Z
                                                                                                                                          =            lim f (tn , x)dm(x)
                                                                                                                                                      n→∞
                                                                                                                                              ZS
                                                                                                                                          =           f (t, x)dm(x),
                                                                                                                                                  S



-->


<p>


\begin{align*}
\lim _{n \rightarrow \infty }\int _{S}f(t_{n}, x)dm(x) &amp; = \int _{S}\lim _{n \rightarrow \infty }f_{n}(x)dm(x)\\ &amp; = \int _{S}\lim _{n \rightarrow \infty }f(t_{n}, x)
dm(x)\\ &amp; = \int _{S}f(t, x)dm(x),
\end{align*}
where we used the continuity assumption (ii) in the last step.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:diff_under_int"><b>4.17</b></a> Let \((h_n)\) be an arbitrary sequence such that \(h_n\to 0\) and define \(a_{n,t}(x)=\frac
{f(t_n+h,x)-f(t,x)}{h_n}\).
</p>
<p>
Since \(\frac {\p f}{\p t}\) exists we have \(a_{n,t}(x)\to \frac {\p f}{\p t}(x,t)\) as \(n\to \infty \) for all \(x\). By the mean value theorem there exists \(\theta _n\in [0,1]\) such that
\(a_{n,t}(x)=\frac {\p f}{\p t}(t+\theta _n h,x)\), hence \(|f_n(x)|\leq h(x)\). Thus by dominated convergence \(\int _S a_{n,t}(x)\,dm(x)\to \int _S \frac {\p f}{\p t}(t,x)\,dm(x)\).
</p>
<p>
By linearity of the integral we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--


                                                                        ∂                                           1
                                                                             Z                                                   Z                                            Z                  
                                                                                 f (t, x) dm(x) = lim                                    f (t + hn , x) dm(x) −                      f (t, x) dm(x)
                                                                        ∂t   S                                 n→∞ hn                S                                           S
                                                                                                                        Z
                                                                                                           = lim                an,t (x) dm(x)
                                                                                                               n→∞ S




-->


<p>


\begin{align*}
\frac {\p }{\p t}\int _S f(t,x)\,dm(x) &amp;=\lim _{n\to \infty }\frac {1}{h_n}\l (\int _S f(t+h_n,x)\,dm(x)-\int _Sf(t,x)\,dm(x)\r )\\ &amp;=\lim _{n\to \infty }\int _S
a_{n,t}(x)\,dm(x)
\end{align*}
and the result follows.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:nonconv_int"><b>4.18</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> For each \(x \in \R , \nN \), the expression for \(f_{n}(x)\) is a telescopic sum. If you begin to write it out, you see that terms cancel in pairs and you obtain
</p>
<p>
\[ f_{n}(x) = -2xe^{-x^{2}} + 2(n+1)^{2}xe^{-(n+1)^{2}x^{2}}.\]
</p>
<p>
Using the fact that \(\lim _{N \rightarrow \infty }N^{2}e^{-yN^{2}} = 0\), for all \(y \in \R \) we find that
</p>
<p>
\[ \lim _{n \rightarrow \infty }f_{n}(x) = f(x) = -2xe^{-x^{2}}.\]
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> The functions \(f\) and \(f_{n}\) are continuous and so Riemann integrable over the closed interval \([0,a]\). We can calculate (which is left for you) that \(\int
_{0}^{a}f(x)dx = -2\int _{0}^{a}xe^{-x^{2}}dx = e^{-a^{2}} - 1\). But on the other hand
</p>
<p>
\[\begin {aligned} \int _{0}^{a}f_{n}(x)\,dx &amp; = \sum _{r=1}^{n}\int _{0}^{a}\l (-2r^{2}xe^{-r^{2}x^{2}} + 2(r+1)^{2}xe^{-(r+1)^{2}x^{2}}\r )\,dx\\ &amp; = \sum _{r=1}^{n}\l
(e^{-r^{2}a} - e^{-(r+1)^{2}a}\r )\\ &amp; = e^{-a^{2}} - e^{-(n+1)^{2}a} \rightarrow e^{-a^{2}}~\mbox {as}~n \rightarrow \infty .                                                                    \end {aligned}\]
</p>
<p>
So we conclude that \(\int _{0}^{a}f(x)\,dx \neq \lim _{n \rightarrow \infty }\int _{0}^{a}f_{n}(x)\,dx\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:fourier_linear"><b>4.19</b></a> Using the fact that \(|e^{-ixy}| \leq 1\), by Lemma <a
href="The-Lebesgue-integral-non-negative-measurable-functions.html#l:leb_int_step_2_monotonicity">4.2.2</a> extended to the complex case we have that
</p>
<p>
\[ |\widehat {f}(y)| \leq \int _{\R }|e^{-ixy}|\;|f(x)|\,dx \leq \int _{\R }|f(x)|\,dx &lt; \infty .\]
</p>
<p>
For the linearity, we have
</p>
<p>
\[\begin {aligned} \widehat {af + bg}(y) &amp; = \int _{\R }e^{-ixy}(a f(x) + b g(x))\,dx \\ &amp; = a\int _{\R }e^{-ixy}f(x)\,dx + b\int _{\R }e^{-ixy}g(x)\,dx \\ &amp; = a
\widehat {f}(y) + b \widehat {g}(y).               \end {aligned}\]
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:fourier_jump"><b>4.20</b></a> \(x \rightarrow {\1}_{\mathbb {Q}}(x)\cos (nx)\) is \(\mc {L}^1\) as \(|{\1}_{\mathbb
{Q}}(x)\cos (nx)| \leq |\cos (nx)|\) for all \(x \in \R \) and \(x \rightarrow \cos (nx)\) is \(\mc {L}^1\) . Similarly \(x \rightarrow {\1}_{\mathbb {Q}}(x)\sin (nx)\) is \(\mc {L}^1\). So the
Fourier coefficients \(a_{n}\) and \(b_{n}\) are well-defined as Lebesgue integrals. As \(|\cos (nx)| \leq 1\), we have \(a_{n} = 0\) for all \(n \in \mathbb {Z}_{+}\) since,
</p>
<p>
\[\begin {aligned} |a_{n}| &amp; \leq \frac {1}{\pi }\int _{-\pi }^{\pi }{\1}_{\mathbb {Q}}(x)|\cos (nx)|\,dx \\ &amp; \leq \frac {1}{\pi }\int _{-\pi }^{\pi }{\1}_{\mathbb
{Q}}(x)\,dx = 0.       \end {aligned}\]
</p>
<p>
By a similar argument, \(b_{n} = 0\) for all \(\nN \). So it is possible to associate a Fourier series to \({\1}_{\mathbb {Q}}\), but this Fourier series will be identically zero.
</p>
<p>
<i>This illustrates that pointwise convergence is not the right tool for examining convergence of Fourier series!</i>
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:fourier_translate"><b>4.21</b></a> Note that \(f_{a}\) is measurable by Problem <a
href="Exercises-on-Chapter-ref-c-measurable_funcs.html#ps:meas_borel"><b>3.4</b></a>(a). To show that \(f_a\in \mc \Lone \), we use
</p>
<p>
\[ \int _{\R }|f_{a}(x)|\,dx = \int _{\R }|f(x - a)|\,dx = \int _{\R }|f(x)|\,dx &lt; \infty .\]
</p>
<p>
Then
</p>
<p>
\[ \widehat {f_{a}}(y) = \int _{\R }e^{-ixy}f(x-a) dx,\]
</p>
<p>
and the result follows on making a change of variable \(u = x-a\).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:fourier_cts"><b>4.22</b></a> Let \(y \in \R \) and \((y_{n})\) be an arbitrary sequence converging to \(y\) as \(n
\rightarrow \infty \). We need to show that the sequence \((f(y_{n}))\) converges to \(f(y)\). We have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                                                                            Z                                    Z
                                                                                                                                    −ixyn
                                                                                       |fb(yn ) − fb(y)| =                       e          f (x)dx −                    e−ixy f (x) dx
                                                                                                                             R                                    R
                                                                                                                         Z
                                                                                                                    ≤            |e−ixyn − e−ixy |.|f (x)| dx.
                                                                                                                             R



-->


<p>


\begin{align*}
|\widehat {f}(y_{n}) - \widehat {f}(y)| &amp; = \left |\int _{\R }e^{-ixy_{n}}f(x)dx - \int _{\R }e^{-ixy}f(x)\,dx\right |\\ &amp; \leq \int _{\R }|e^{-ixy_{n}}-
e^{-ixy}|.|f(x)|\,dx.
\end{align*}
Now \(|e^{-ixy_{n}}- e^{-ixy}| \leq |e^{-ixy_{n}}| + |e^{-ixy}| = 2\) and the function \(x \rightarrow 2f(x)\) is \(\mc {L}^1\). Also the mapping \(y \rightarrow e^{-ixy}\) is continuous, and
so \(\lim _{n \rightarrow \infty }|e^{-ixy_{n}}- e^{-ixy}| = 0\). The result follows from these two facts, and the use of Lebesgue’s dominated convergence theorem.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:fourier_xfx"><b>4.23</b></a> To prove that \(y \rightarrow \widehat {f}(y)\) is differentiable, we need to show that
</p>
<p>
\(\lim _{h \rightarrow 0}(\widehat {f}(y + h) - \widehat {f}(y))/h\) exists for each \(y \in \R \). We have
</p>
<p>
\[\begin {aligned} \frac {\widehat {f}(y + h) - \widehat {f}(y)}{h} &amp; = \frac {1}{h}\int _{\R }(e^{-ix(y + h)} - e^{-ixy})f(x)\,dx \\ &amp; = \int _{\R }e^{-ixy}\left (\frac
{e^{-ihx} - 1}{h}\right )f(x)\,dx.              \end {aligned}\]
</p>
<p>
Since \(|e^{-ixy}| \leq 1\), and using the hint with \(b = hx\), we get
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--
                                                                   fb(y + h) − fb(y)               e−ihx − 1
                                                                                          Z                                                   Z
                                                                                     ≤                       |f (x)| dx ≤                         |x||f (x)| dx < ∞.
                                                                           h               R           h                                      R



-->


<p>


\begin{align*}
\left |\frac {\widehat {f}(y + h) - \widehat {f}(y)}{h}\right | \;\leq \; \int _{\R }\left |\frac {e^{-ihx} - 1}{h}\right |\;|f(x)|\,dx \;\leq \; \int _{\R }|x||f(x)|\,dx &lt;
\infty .
\end{align*}
Then we can use Lebesgue’s dominated convergence theorem to get
</p>
<p>
\[\begin {aligned} \lim _{h \rightarrow 0}\frac {\widehat {f}(y + h) - \widehat {f}(y)}{h} &amp; = \int _{\R }e^{-ixy}\lim _{h \rightarrow 0}\left (\frac {e^{-ihx} - 1}{h}\right
)f(x)\,dx \\ &amp; = -i\int _{\R }e^{-ixy}xf(x)\,dx = -i\widehat {g}(y), \end {aligned}\]
</p>
<p>
and the result is proved. In the last step we used
</p>
<p>
\[ \lim _{h \rightarrow 0}\frac {e^{-ihx} - 1}{h} = \left .\frac {d}{dy}e^{-ixy}\right |_{y=0} = -ix.\]
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:fourier_conv"><b>4.24</b></a> First observe that by the hint and Problem <a
href="Exercises-on-Chapter-ref-c-measurable_funcs.html#ps:meas_borel"><b>3.4</b></a> part (a), the mapping \((x, y) \rightarrow f(x-y)g(y)\) is measurable. Let \(K = \sup _{x \in \R
}|g(x)| &lt; \infty \), since \(g\) is bounded. Then since \(f\in \Lone \) we have
</p>
<p>
\[ |(f*g)(x)| \leq \int _{\R }|f(x - y)|\,|g(y)|dy \leq K \int _{\R }|f(x - y)|dy = K \int _{\R }|f(y)\,|dy &lt; \infty .\]
</p>
<p>
We also have by Fubini’s theorem
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                       Z Z                                       Z Z                                    
                                                                                |(f (x − y)g(y)|dydx ≤                         |f (x − y)| |g(y)| dy dx
                                                                        R   R                                     R   R
                                                                                                                 Z Z                             
                                                                                                            =                  |f (x − y)|dx |g(y)| dy
                                                                                                                 ZR        R
                                                                                                                                      Z
                                                                                                            =           |f (x)| dx         |g(y)| dy < ∞,
                                                                                                                    R                  R



-->


<p>


\begin{align*}
\int _{\R }\int _{\R }|(f(x-y)g(y)|dydx &amp;\leq \int _{\R }\left (\int _{\R }|f(x - y)|\,|g(y)|\,dy \right )dx \\ &amp;= \int _{\R }\left (\int _{\R }|f(x-y)|dx \right
)|g(y)|\,dy \\ &amp;= \int _{\R }|f(x)|\,dx \int _{\R }|g(y)|\,dy &lt; \infty ,
\end{align*}
from which it follows that \(f*g\in \Lone \).
</p>
<p>
By a similar argument using Fubini’s theorem, we have that
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                                               Z             Z
                                                                                                     −ixy
                                                                                   ∗ g(y) =
                                                                                  f[                e               f (x − z)g(z) dz dx
                                                                                               ZR Z            R
                                                                                                                                          
                                                                                          =                 e−iy(u+z) f (u)du g(z) dz
                                                                                               ZR       R
                                                                                                                           Z
                                                                                                     −iyu
                                                                                          =         e       f (u)du.               e−iyz g(z) dz
                                                                                                R                              R

                                                                                          = fb(y)gb(y),



-->


<p>


\begin{align*}
\widehat {f * g}(y) &amp; = \int _{\R }e^{-ixy}\int _{\R }f(x -z)g(z)\,dz\, dx \\ &amp; = \int _{\R }\left (\int _{\R }e^{-iy(u + z)}f(u)du \right )g(z)\,dz \\ &amp; = \int _{\R
}e^{-iyu}f(u)du.     \int _{\R }e^{-iyz}g(z)\,dz\\ &amp; = \widehat {f}(y)\widehat {g}(y),
\end{align*}
where we used that change of variable \(x = u + z\).
</p>
<p>


</p>
</li>
</ul>
<!--
......   subsection Chapter <a href=Probability-with-Measure.html#c:prob_with_meas>5</a> ......
-->
<h5 id="autosec-301">Chapter <a href="Probability-with-Measure.html#c:prob_with_meas">5</a></h5>
<a id="notes-autopage-301"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:prob_notation"><b>5.1</b></a> <i>Monotone Convergence Theorem.</i> Let \((X_{n})\) be an increasing sequence of non-negative
random variables that converges almost surely to a random variable \(X\). Then \(\E [X_n]\to \E [X]\).
</p>
<p>
<i>Dominated Convergence Theorem.</i> Let \((X_{n})\) be a sequence of random variables that converges pointwise to a random variable \(X\). Suppose that there exists a random variable \(Y\in \Lone \)
such that \(|X_{n}| \leq Y\) almost surely, for all \(n\in \N \). Then \(X\in \Lone \) and \(\E [X_n]\to \E [X]\).
</p>
<p>
<i>Markov’s Inequality.</i> Let \(X\) be a non-negative random variable and let \(c&gt;0\). Then \(\P [X\geq c]\leq \frac {1}{c}\E [X]\).
</p>
<p>
<i>Chebyshev’s Inequality.</i> Let \(X\) be a non-negative random variable and let \(c&gt;0\). Then \(\P [X\geq c]\leq \frac {1}{c^2}\E [X^2]\).
</p>
<p>
<i>Theorem <a href="Integration-as-measure.html#t:int_as_meas">4.4.1</a>.</i> Let \(X\) be a non-negative random variable on the probability space \((\Omega ,\mc {F},\P )\). Then \(\nu :\mc
{F}\to [0,\infty ]\) by \(\nu (A)=\E [\1_A X]\) is a measure.
</p>
<p>
<i>Fatou’s Lemma.</i> Let \((X_{n})\) be a sequence of non-negative random variables. Then \(\E [\liminf _n X_n]\leq \liminf _n \E (X_{n})\).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:chebyshev_prob"><b>5.2</b></a> Put \(|X-\E [X]|\) in place of \(X\) in the version of Chebyshev’s inequality in Exercise <a
href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:prob_notation"><b>5.1</b></a>, to obtain \(\P [|X-\E [X]|\geq c]\leq \frac {1}{c}\E \l [(X-\E [X])^2\r ]=\frac {\var
(X)}{c}\).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:uniform_dist"><b>5.3</b></a> \(\P [U\in A]=\int _A \frac {1}{b-a}\,dx=\frac {\lambda (A)}{b-a}\), where \(\lambda \) denotes
Lebesgue measure.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:cdf_bits"><b>5.4</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> \(\P [X&gt;x]=1-\P [X\leq x]=F(x)\) and \(\P [x&lt;X\leq y]=\P [X\leq y]-\P [X\leq x]=F(y)-F(x)\).
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Let \((a_{n})\) be an increasing sequence that tends to \(\infty \). Define \(A_{n} = \{\omega \in \Omega ; X(\omega ) \leq a_{n}\}\). Then \((A_{n})\)
increases to \(\Omega \) and by Lemma <a href="Probability-with-Measure.html#l:monotone_events_P">5.1.1</a>,
</p>
<p>
\[\lim _{x \rightarrow \infty }F(x) = \lim _{n \rightarrow \infty }\P [A_{n}] = \P [\Omega ] = 1.\]
</p>
<p>
Similarly, let \(B_{n} = \{\omega \in \Omega ; X(\omega ) \leq - a_{n}\}\). Then \((B_{n})\) decreases to \(\emptyset \) and by Lemma <a
href="Probability-with-Measure.html#l:monotone_events_P">5.1.1</a>,
</p>
<p>
\[\lim _{x \rightarrow -\infty }F(x) = \lim _{n \rightarrow \infty }\P [B_{n}] = \P [\emptyset ] = 0.\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:countable_atoms"><b>5.5</b></a> From Lemma <a href="The-cumulative-distribution-function.html#l:cdf">5.2.1</a> we have
that \(x\mapsto F_X(x)=\P [X\leq x]\) is right-continuous and monotone increasing. At each \(x\) such that \(\P [X=x]&gt;0\) the function \(x\mapsto F_X(x)\) has an upwards jump. The region it jumps
through is \(Q_x=(F_X(x-),F_X(x))\), which is non-empty open interval, and in particular contains a rational number \(q_x\) (in fact, infinitely many rationals, but one will do).
</p>
<p>
Hence, for each \(x\) with \(\P [X=x]\) we have a rational number \(q_x\), and because \(F\) is increasing we have \(q_x\neq q_y\) whenever \(x\neq y\). Therefore \(x\mapsto q_x\) is an injective map from
\(\{x\in \R \-\P [X=x]&gt;0\}\) to \(\Q \). Since \(\Q \) is countable, so is \(\{x\in \R \-\P [X=x]&gt;0\}\).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:indep_inf"><b>5.6</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Define \(B_n=\cap _{i=1}^n A_i\). Then \((B_n)\) is a decreasing sequence of sets and, since \(\P \) is a finite measure, by Lemm a<a
href="Probability-with-Measure.html#l:monotone_events_P">5.1.1</a> we have \(\P [B_n]\to \P [\cap _{i=1}^\infty B_i]\) as \(n\to \infty \). Since \(\cap _{i=1}^\infty A_i=\cap
_{i=1}^\infty B_i\) we thus have \(\P [\cap _{i=1}^\infty A_i]=\lim _{n\to \infty }\P [\cap _{i=1}^n A_i]\). Using independence on the right hand side, we obtain
</p>
<p>
\[\P [\cap _{i=1}^\infty A_i]=\lim _{n\to \infty }\P [A_1]\P [A_2]\ldots \P [A_n]=\prod _{i=1}^\infty \P [A_i]\]
</p>
<p>
as required. Note that the limit on the right hand side exists because \(\P [A_1]\P [A_2]\ldots \P [A_n]\) is decreasing as \(n\) increases.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> There are many ways to answer this question, but they all focus around the possibility that \(\prod _{n=1}^\infty \P [A_n]\) might be zero, in which case <span
class="textup">(<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#eq:inf_indep">5.3</a>)</span> might not give us any information.
</p>
<p>
For example: if \(0&lt;\P [A_{n}] &lt; 1-\kappa \) for infinitely many \(n\), where \(\kappa &gt;0\) does not depend on \(n\), then \(\prod _{n=1}^{\infty }\P [A_{n}] = 0\) so \(\P \l [\bigcap
_{n=1}^{\infty }A_{n}\r ] = \prod _{n=1}^{\infty }\P [A_{n}]\) would hold in, for example, the case where all the \((A_n)\) were disjoint. Disjoints events with non-zero probability are always
<i>dependent</i> (note that if one occurs then all the others do not!) so clearly this ‘alternative’ definition is not what we want.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:indep_exts"><b>5.7</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We have \(\P [A\cap B]=\P [A]\cap \P [B]\). Noting that \(A^c\cap B^c=(A\cup B)^c\), we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--



                                                                                          P[Ac ∩ B c ] = P[(A ∪ B)c ] = 1 − P[A ∪ B]

                                                                                                                    = 1 − P[A] − P[B] + P[A ∩ B]

                                                                                                                    = 1 − P[A] − P[B] − P[A]P[B]

                                                                                                                    = (1 − P[A])(1 − P[B])

                                                                                                                    = P[Ac ]P[B c ].



-->


<p>


\begin{align*}
\P [A^c\cap B^c]=\P [(A\cup B)^c] &amp;=1-\P [A\cup B]\\ &amp;=1-\P [A]-\P [B]+\P [A\cap B]\\ &amp;=1-\P [A]-\P [B]-\P [A]\P [B]\\ &amp;=(1-\P [A])(1-\P [B])\\ &amp;=\P [A^c]\P
[B^c].
\end{align*}
Hence \(A^c\) and \(B^c\) are independent.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> If \(A,B \in {\cal B}(\R )\) and \(f,g\) are Borel measurable, then \(f^{-1}(A), g^{-1}(B) \in {\cal B}(\R )\) and so
</p>
<p>
\[\begin {aligned} \P [f(X) \in A, g(Y) \in B] &amp; = \P [X \in f^{-1}(A), Y \in g^{-1}(B)]\\ &amp; = \P [X \in f^{-1}(A))\P (Y \in g^{-1}(B)]\\ &amp; = \P [f(X) \in A)\P (g(Y)
\in B). \end {aligned}\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:indep_counterexs"><b>5.8</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We have \(XY=U^2V(1-V)=0\) because \(V(1-V)=0\), hence \(\E [XY]=0\). Note that \(\E [U]=0\). By independence of \(U\) and \(V\), \(\E [X]=\E [U]\E [V]=0\)
and \(\E [Y]=\E [U]\E [1-V]=0\). Hence \(\E [XY]=\E [X]\E [Y]\).
</p>
<p>
To see that \(X\) and \(Y\) are not independent, note that \(\{X=0\}=\{V=0\}\) and \(\{Y=0\}=\{V=1\}\). Thus \(\P [X=Y=0]=0\) but that \(\P [X=0]=\P [Y=0]\frac 12\), so \(\P [X=Y=0]\neq \P
[X=0]\P [Y=0]\).
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> It is clear that \(X\) and \(Y\) are independent. Considering \(X\) and \(Z\), for any \(a,b\in \{-1,1\}\) we have \(\P [X=a,Z=b]=\P [X=a,XY=b]=\frac {1}{2}\).
The same calculation applies to \(X\) and \(Y\). We thus have pairwise independence.
</p>
<p>
However, \(\P [X=Y=Z=1]=\P [X=Y=1]=\P [X=Y=1]=\P [X=1]\P [Y=1]=\frac 12\frac 12=\frac 14\) and \(\P [X=1]\P [Y=1]\P [Z=1]=\frac 12\frac 12\frac 12=\frac 18\). Hence \(\{X,Y,Z\}\) is
not a set of independent random variables.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:bct"><b>5.9</b></a> The constant \(M\) provides a dominating function for \((X_n)\) and we have \(\E [M]=M&lt;\infty \), so (the
constant function) \(M\) is in \(\Lone \). By the dominated convergence theorem we have \(\E [X_n]\to \E [X]\).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:E_P_tail_bound"><b>5.10</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> If \(X=k\) and \(k\in \N \) then
</p>
<p>
\[\sum \limits _{n=1}^\infty \1_{\{X\geq n\}}=\sum \limits _{n=1}^\infty \1_{\{k\geq n\}}=k=X\]
</p>
<p>
because the first \(k\) terms of the sum are \(1\) and the rest are \(0\). Since \(X\) only takes values in \(\N \), we have
</p>
<p>
\[X=\sum _{n=1}^\infty \1_{\{X\geq n\}}.\]
</p>
<p>
By the monotone convergence theorem
</p>
<p>
\[ \E [X] = \E \l [\sum _{n=1}^{\infty }{\1}_{\{X \geq n\}}\r ] = \sum _{n=1}^{\infty }\E [{\1}_{\{X \geq n\}}] = \sum _{n=1}^{\infty }\P [X \geq n].\]
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Let \(X_1=\lfloor Y\rfloor \) and \(X_2=\lceil Y\rceil \), that is \(Y\) rounded up and down (respectively) to the nearest integer. We can apply part (a) to both
\(X_1\) and \(X_2\), since they take values in \(\N \cup \{0\}\).
</p>
<p>
Note that for \(n\in \N \) we have \(X_1\geq n\) if and only if \(Y\geq n\). Hence
</p>
<p>
\[\sum \limits _{n=1}^\infty \P [Y\geq n] =\sum \limits _{n=1}^\infty \P [X_1\geq n] =\E [X_1] \leq \E [Y].\]
</p>
<p>
Here, the last line follows by monotonicity, since \(X_1\leq Y\).
</p>
<p>
For \(X_2\) we need to be slightly more careful. We have \(Y\leq X_2\leq Y+1\), hence \(\P [X_2\geq k]\leq \P [Y+1\geq k]\). Hence
</p>
<p>
\[ \E [Y]\leq \E [X_2] =\sum \limits _{n=1}^\infty \P [X_2\geq n] \leq \sum \limits _{n=1}^\infty \P [Y+1\geq n] = \1_{\{Y\geq 0\}}+\sum \limits _{n=1}^\infty \P [Y\geq n] =
1+\sum \limits _{n=1}^\infty \P [Y\geq n].           \]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:cauchy_schwarz"><b>5.11</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> By linearity, the quadratic function \(g(t) = \E [X^{2}] + 2t\E [XY] + t^{2}\E [Y^{2}] \geq 0\) for all \(t \in \R \). A non-negative quadratic function has at
most one real root, and hence has a non-positive discriminant (i.e.&nbsp;\(b^2-4ac\leq 0\)). Hence \(4(\E [XY])^{2} - 4\E [X^{2}]\E [Y^{2}]\leq 0\) and the result follows.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Put \(Y=1\) in the Cauchy-Schwarz inequality from (a) to get \(\E [|X|] \leq \E [X^{2}]^{\frac {1}{2}} &lt; \infty \). Thus \(X\) is integrable. By Lemma <a
href="The-Lebesgue-integral-non-negative-measurable-functions.html#l:leb_int_step_2_monotonicity">4.2.2</a> we have \(|\E [X]| \leq \E [|X|]\). Combining our two inequalities gives \(|\E
[X]|^{2} \leq \E [X^{2}]\).
</p>
</li>
<li>


<p>
<span class="textnormal">(c)</span> If \(\E [X^2]&lt;\infty \) then by part (b) \(X\) is also integrable, so by linearity we have
</p>
<p>
\[ \var (X) = \E [(X - \mu )^{2}] = \E [X^{2}] - 2\mu \E [X] + \mu ^{2} = \E [X^{2}] - \mu ^{2}.\]
</p>
<p>
Hence \(\var (X)&lt;\infty \).
</p>
<p>
Conversely, suppose that \(\var (X)&lt;\infty \), and note that by assumption we also have \(\E [X]&lt;\infty \). We can write \(X^2=(X-\E [X])^2+2X\E [X]-\E [X]^2\) and note that all terms here are
integrable by our assumptions, thus
</p>
<p>
\[\E [X^2]=\var (X)+2\E [X]\E [X]-\E [X]^2=\var (X)-\E [X]^2.\]
</p>
<p>
Hence \(\E [X^2]\) is finite.
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:exp_moments"><b>5.12</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Since \(e^{-ax} \leq 1\) for all \(x \geq 0\) we have
</p>
<p>
\[ \E \l [e^{-aX}\r ] = \int _{0}^{\infty }e^{-ax}dp_{X}(x) \leq \int _{0}^{\infty }dp_{X}(x) = 1.\]
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Using the fact that \(e^{a|x|} = \sum _{n=0}^{\infty }\frac {a^{n}|x|^{n}}{n!}\) for all \(x \in \R \) we see that for each \(\nN , |x|^{n} \leq \frac
{n!}{a^{n}}e^{a|x|}\) and so by monotonicity
</p>
<p>
\[ \E \l [|X|^{n}\r ] \leq \frac {n!}{a^{n}}\E \l [e^{a|X|}\r ] &lt; \infty .\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:E_int"><b>5.13</b></a> If \(f\) is an indicator function: \(f = {\1}_{A}\) for some \(A \in {\cal B}(\R )\):
</p>
<p>
\[ \int _{\Omega }{\1}_{A}(X(\omega ))d\P (\omega ) = \P (X \in A) = p_{X}(A) = \int _{\R }{\1}_{A}(x)p_{X}(dx),\]
</p>
<p>
and so the result holds in this case. It extends to simple functions by linearity. If \(f\) is non-negative and bounded
</p>
<p>
\[\begin {aligned} \int _{\Omega }f(X(\omega ))d\P (\omega ) &amp; = \sup \left \{\int _{\Omega }g(\omega )d\P (\omega ); g~\mbox {simple on}~\Omega , 0 \leq g \leq f \circ
X\right \}\\ &amp; = \sup \left \{\int _{\Omega }h(X(\omega ))d\P (\omega ); h~\mbox {simple on}~\R , 0 \leq h \circ X \leq f \circ X\right \}\\ &amp; = \sup \left \{\int _{\R
}h(x)p_{X}(dx); h~\mbox {simple},~0 \leq h \leq f\right \}\\ &amp; = \int _{\R }f(x)dp_{X}(x).\end {aligned}\]
</p>
<p>
In the general case write \(f = f_{+} - f_{-}\) and similarly for \(X\) (details left for you).
</p>
<p>
If \(f\) is non-negative but not necessarily bounded, the result still holds but both \(\int _\Omega f(X(\omega )\,d\P (\omega )\) and \(\int _\R f(x)\,dp_X(x)\) may be (simultaneously) infinite.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:BC2_baby_version"><b>5.14</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Since \(\P [E_n]\geq \eps \) we have \(\P [\Omega \sc E_n]\leq 1-\eps \). Hence for all \(N\in \N \) we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                              N                        N                      N                  N
                                                                          "           #           "            #          "            #
                                                                              [                        [                      \                  Y
                                                          P[∪n En ] ≥ P             En = 1 − P Ω \           En = 1 − P           Ω \ En = 1 −         P[Ω \ En ] ≥ 1 − (1 − ϵ)N .
                                                                              n=1                      n=1                n=1                    n=1



-->


<p>


\begin{align*}
\P [\cup _n E_n] \geq \P \l [\bigcup _{n=1}^N E_n\r ] = 1-\P \l [\Omega \sc \bigcup _{n=1}^N E_n\r ] = 1-\P \l [\bigcap _{n=1}^N \Omega \sc E_n\r ] = 1-\prod _{n=1}^N\P [\Omega
\sc E_n] \geq 1-(1-\eps )^N.
\end{align*}
In the above the third step is obtained using independence of the \((E_n)\). As the above equation holds for all \(N\in \N \) we obtain that \(\P \l [\cup _{n} E_n\r ]=1.\)
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Suppose that there exists \(\omega \in \Omega \) such that \(\P [\omega ]&gt;0\). Define a sequence of events \((E&apos;_n)\) by setting \(E&apos;_n=E_n\) if
\(\omega \notin E_n\) and \(E&apos;_n=\Omega \sc E_n\) if \(\omega \in E_n\). Clearly \(\omega \notin \cup _{n\in \N } E&apos;_n\). By part (a) of exercise <a
href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:indep_exts"><b>5.7</b></a> the events \((E&apos;_n)_{n\in \N }\) are independent of one another. We have \(\P [E&apos;_n]\in (\eps
,1-\eps )\) for all \(n\in \N \) so from exercise <a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:BC2_baby_version"><b>5.14</b></a> we have that \(\P [\cup _{n\in \N }
E&apos;_n]=1\). However \(\omega \notin \cup _{n\in \N } E&apos;_n\) and \(\P [\omega ]&gt;0\), so this is a contradiction to \(\P [\Omega ]=1\). Hence \(\P [\omega ]=0\) for all \(\omega \in
\Omega \).
</p>
<p>
For the last part, if \(\Omega \) was countable then we could write \(\Omega =\{\omega _1,\omega _2,\ldots \}\) and by definition of a measure we would have \(1=\P [\Omega ]=\sum _{i\in \N }\P
[\omega _i]\). Hence at least one \(\omega _i\) must satisfy \(\P [\omega _i]&gt;0\), but we have already shown that this may not happen.
</p>
</li>
</ul>
</li>
</ul>
<!--
......   subsection Chapter <a href=Inequalities-Random-Variables.html#c:ineqs>6</a> ......
-->
<h5 id="autosec-302">Chapter <a href="Inequalities-Random-Variables.html#c:ineqs">6</a></h5>
<a id="notes-autopage-302"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-c-ineqs.html#ps:chernoff"><b>6.1</b></a> Let us first calculate the moment generating function of the Poisson distribution (or you could look it up). If \(X\) has the
Poisson\((\lambda )\) distribution the \(\P [X_n=k]=\frac {\lambda ^ke^{-\lambda }}{k!}\). Hence we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                                                   ∞                       ∞
                                                                                                     λk e−λ                  (et λ)k                      t           t
                                                                                                              etk = e−λ                           = e−λ ee λ = eλ(e −1) .
                                                                                                   X                       X
                                                                                 E[etX ] =
                                                                                                   k=0
                                                                                                         k!                k=0
                                                                                                                                      k!


-->


<p>


\begin{align*}
\E [e^{tX}] =\sum _{k=0}^\infty \frac {\lambda ^ke^{-\lambda }}{k!} e^{tk} =e^{-\lambda }\sum _{k=0}^\infty \frac {(e^t\lambda )^k}{k!} =e^{-\lambda }e^{e^t\lambda } =e^{\lambda
(e^t-1)}.
\end{align*}
Putting \(n\lambda \) as the parameter, we obtain \(\E [e^{tX_n}]=e^{n\lambda (e^t-1)}\) as required.
</p>
<p>
To derive the Chernoff bound, note that by Markov’s inequality we have
</p>
<p>
\[ \P [X_n\geq n\lambda ^2] =\P [e^{tX_n}\leq e^{tn\lambda ^2}] \leq e^{-tn\lambda ^2}\E [e^{tX_n}] = \exp \l (n\lambda (e^t-1-\lambda t)\r ).                                                              \]
</p>
<p>
Differentiating the above with respect to \(t\) obtains \(n\lambda (e^t-\lambda )\exp \l (n\lambda (e^t-1-\lambda t)\r )\), which is minimized when \(\lambda =e^t\). We thus obtain the Chernoff
bound
</p>
<p>
\[ \P [X_n\geq n\lambda ^2] \leq \exp \l (n\lambda (\lambda -1-\lambda \log \lambda )\r ).                                       \]
</p>
<p>
<em>Note that \(\lambda -1-\lambda \log \lambda \leq 0\) for \(\lambda &gt;0\), with equality only when \(\lambda =1\), so this is a useful bound provided \(\lambda \geq 1\).</em>
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-ineqs.html#ps:pz_appl"><b>6.2</b></a> By linearity we have \(\E [X_n]=\sum _{i=1}^n \P [E_i]\) and
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                                                                                                      
                                                                   n                    n
                                                                                      n X                         n                 n
                                                                                                                                  n X                          n                  n
                                                                                                                                                                                n X
                                                                             12Ei +                1Ei 1Ej               1Ei +                   1Ei ∩Ej 
                                                                   X                  X                           X               X                            X                X
                                                       E[Xn2 ] = E 
                                                                                                                                                      
                                                                                                           = E
                                                                                                                
                                                                                                                                                          =         P[Ei ] +                P[Ei ∩ Ej ].
                                                                       i=1            i=1   j=1                     i=1           i=1      j=1                 i=1              i=1   j=1
                                                                                            j̸=i                                           j̸=i                                       j̸=i


-->


<p>


\begin{align*}
\E [X_n^2] = \E \l [\sum _{i=1}^n \1_{E_i}^2 + \sum _{i=1}^n\sum _{\stackrel {j=1}{j\neq i}}^n\1_{E_i}\1_{E_j}\r ] = \E \l [\sum _{i=1}^n \1_{E_i} + \sum _{i=1}^n\sum _{\stackrel
{j=1}{j\neq i}}^n\1_{E_i\cap E_j}\r ] = \sum _{i=1}^n \P [E_i] + \sum _{i=1}^n\sum _{\stackrel {j=1}{j\neq i}}^n \P [E_i\cap E_j].
\end{align*}
Hence,
</p>
<p>
\[\frac {\E [X_n]^2}{\E [X_n^2]} =\frac {\l (\sum _{i=1}^n \P [E_i]\r )^2} {\sum _{i=1}^n \P [E_i] + \sum _{i=1}^n\sum _{\stackrel {j=1}{j\neq i}}^n\P [E_i\cap E_j]} =\frac {1}
{\l (\sum _{i=1}^n \P [E_i]\r )^{-1}+\frac {\sum _{i=1}^n\sum _{\stackrel {j=1}{j\neq i}}^n\P [E_i\cap E_j]}{\l (\sum _{i=1}^n \P [E_i]\r )^2}}.                                                                 \]
</p>
<p>
Using that \(\P [E_i\cap E_j]\leq \P [E_i]\P [E_j]\) we have
</p>
<p>
\[ \frac {\sum _{i=1}^n\sum _{\stackrel {j=1}{j\neq i}}^n\P [E_i\cap E_j]}{\l (\sum _{i=1}^n \P [E_i]\r )^2} \leq \frac {\sum _{i=1}^n \P [E_i]^2+\sum _{i=1}^n\sum _{\stackrel
{j=1}{j\neq i}}^n\P [E_i]\P [E_j]}{\l (\sum _{i=1}^n \P [E_i]\r )^2}=1, \]
</p>
<p>
hence
</p>
<p>
\[\frac {\E [X_n]^2}{\E [X_n^2]} \geq \frac {1}{\l (\sum _{i=1}^n \P [E_i]\r )^{-1}+1}.\]
</p>
<p>
The right hand side of the above tends to \(1\) as \(n\to \infty \) because \(\sum _{i=1}^\infty \P [E_i]=\infty \). From the Paley-Zygmund inequality we have \(1\geq \P [X_n\geq 1] \geq \frac
{\E [X]^2}{\E [X^2]}\), so by the sandwich rule \(\P [X_n\geq 1]\to 1\) as \(n\to \infty \).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-ineqs.html#ps:jenson_compare"><b>6.3</b></a> Here we use Lemma <a href="Jensen-inequality.html#l:convex_tangents">6.3.2</a> (and Remark <a
href="Jensen-inequality.html#r:convex_on_interval">6.3.4</a>) to check for convexity, by calculating the second derivative. This part is left for you.
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> The function \(x\mapsto x^{4}\) is convex, so Jensen’s inequality gives \(\E [X]^{4} \leq \E [X^{4}]\).
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> The function \(x\mapsto x^{1/4}\) is not convex, but \(x\mapsto -x^{1/4}\) is convex for \(x\geq 0\), so Jensen’s inequality gives \(-\E [X]^{1/4} \leq \E
[-X^{1/4}]\), that is \(\E [X^{1/4}]\leq \E [X]^{1/4}\).
</p>


</li>
<li>


<p>
<span class="textnormal">(c)</span> The function \(x\mapsto e^x\) is convex, so Jensen’s inequality gives \(e^{\E [X]}\leq \E [e^X]\).
</p>


</li>
<li>


<p>
<span class="textnormal">(d)</span> If we take the case \(\P [X=0]=\P [X=\frac {\pi }{2}]=\frac 12\) then we have \(\E [\cos (X)]=\frac 12(1)+\frac 12(0)=\frac 14\) and \(\cos (\E
[X])=\cos (\frac {\pi }{4})=\frac {1}{\sqrt {2}}\). Hence \(\E [\cos (X)] &lt; \cos (\E [X])\).
</p>
<p>
If we take the case \(\P [X=\pi ]=\P [X=\frac {\pi }{2}]=\frac 12\) then we have \(\E [\cos (X)]=\frac 12(-1)+\frac 12(0)=-\frac 14\) and \(\cos (\E [X])=\cos (\frac {3\pi }{4})=-\frac
{1}{\sqrt {2}}\). Hence \(\E [\cos (X)] &gt; \cos (\E [X])\).
</p>
<p>
Therefore no inequality holds in general between \(\E [\cos (X)]\) and \(\cos (\E [X])\).
</p>
<p>
<em>The point here is that \(x\mapsto \cos (x)\) is convex on \([\frac {\pi }{2},\pi ]\) and \(x\mapsto -\cos (x)\) is convex on \([0,\frac {\pi }{2}]\), which allows us to produce examples or
random variables where the inequality goes both ways.</em>
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-ineqs.html#ps:jensen_am_gm"><b>6.4</b></a> The function \(g(x)=-\log (x)\) is convex for \(x&gt;0\) by Lemma <a
href="Jensen-inequality.html#l:convex_tangents">6.3.2</a>, because \(g&apos;&apos;(x)=\frac {1}{x^2}\geq 0\). Applying Jensen’s inequality to \(X\), where \(X\) has the uniform distribution on
\(\{x_1,\ldots ,x_n\}\), we obtain
</p>
<p>
\[-\log \l (\frac {x_1+\ldots +x_n}{n}\r ) \leq -\frac {\log (x_1)+\ldots +\log (x_n)}{n}.\]
</p>
<p>
Rearranging, we obtain \(\frac {1}{n}\log (x_1x_2\ldots x_n)=\log (\sqrt [n]{x_1x_2\ldots x_n})\leq \log (\frac {x_1+\ldots +x_n}{n})\). The required result follows since \(x\mapsto \log x\)
is monotone increasing.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-ineqs.html#ps:jensen_LpLq"><b>6.5</b></a> Since \(p\leq q\) the function \(g(x)=x^{q/p}\) is convex for \(x\geq 0\), by Lemma <a
href="Jensen-inequality.html#l:convex_tangents">6.3.2</a> (you should check this). We apply Jensen’s inequality to \(|X|^p\) and \(g(x)\), which gives that
</p>
<p>
\[(\E [|X|^p])^{q/p} \leq \E [(|X|^p)^{q/p}]=\E [|X|^q]&lt;\infty .\]
</p>
<p>
Hence \(\E [|X|^p]&lt;\infty \).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-ineqs.html#ps:pz_plus_chebyshev"><b>6.6</b></a> We have \(X\geq 0\). By monotonicity this implies \(\E [X]\geq 0\). If \(\E [X]=0\) then it would follow
from Lemma <a href="The-Lebesgue-integral-non-negative-measurable-functions.html#l:int0_=0_nonneg">4.2.5</a> that \(X\eqas 0\), which would imply \(\E [X^2]=0\). This is a contradiction,
hence in fact \(\E [X]&gt;0\).
</p>
<p>
Rearranging the Paley-Zygmund inequality from <span class="textup">(<a href="The-Paley-Zygmund-inequality.html#eq:paley_zygmund_0">6.3</a>)</span> gives that \(\P [X=0]\leq 1-\frac {\E
[X]^2}{\E [X^2]}\).
</p>
<p>
To obtain the other case of the minimum, note that \(X=0\) implies that \(|X-\E [X]|\leq \E [X]\), hence \(\P [X=0]\leq \P [|X-\E [X]|\geq \E [X]]\). Using Chebyshev’s inequality from Exercise <a
href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:chebyshev_prob"><b>5.2</b></a>, with \(c=\E [X]&gt;0\), we therefore have \(\P [X=0]\leq \frac {\var (X)}{\E [X]^2}=\frac {\E
[X^2]-\E [X]^2}{\E [X]^2}=\frac {\E [X]^2}{\E [X]^2}-1\).
</p>
<p>


</p>
</li>
</ul>
<!--
......     subsection Chapter <a href=Sequences-Random-Variables.html#c:rv_sequences>7</a> ......
-->
<h5 id="autosec-303">Chapter <a href="Sequences-Random-Variables.html#c:rv_sequences">7</a></h5>
<a id="notes-autopage-303"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-c-rv_sequences.html#ps:indep_coin_runs"><b>7.1</b></a> Let \(E_{m}\) be the event that starting at the \(m\)th toss, \(k\) consecutive heads appear. Then \(\P
[E_{m}] = 1/2^{k}\). Set \(A_n=E_{m+kn}\) and then the \((A_n)\) are independent. Moreover, \(\sum _{r=1}^{\infty }\P [A_n] = \infty \), so by the second Borel-Cantelli lemma \(\P [A_n\text {
i.o.}]=1\).
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-rv_sequences.html#ps:ev_io"><b>7.2</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> You might reasonably think that this is obvious - if \((A_n)\) occurs eventually then it occurs for all \(n\) after some \(N\), and of course there are infinitely many such
\(n\) so then \((A_n)\) occurs infinitely often. Let’s give a proof anyway.
</p>
<p>
Suppose \(\omega \in \{A_n\text { e.v.}\}=\bigcup _m\bigcap _{n\geq m} A_n\). Then, for at least one value of \(m\), we have \(\omega \in A_n\) for all \(n\geq m\). Take any \(k\in \N \) and pick
some \(n\geq \max (m,k)\). Then \(\omega \in \bigcup _{i\geq k}A_i\), but this holds for all \(k\), which implies \(\omega \in \bigcap _k\bigcup _{i\geq k} A_i=\{A_i\text { i.o.}\}\).
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> By the laws of set algebra we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>


<!--

                                                                                                       
                                                                                    \ [       [       [         [ \
                                                               Ω \ {An i.o.} = Ω \     An  = Ω \    An  =     Ω \ An = {Ω \ An e.v.}.
                                                                                       m n≥m           m          n≥m           m n≥m



-->


<p>


\begin{align*}
\Omega \sc \{A_n\text { i.o.}\} =\Omega \sc \l (\bigcap _m\bigcup _{n\geq m}A_n\r ) =\bigcup _m\l (\Omega \sc \l (\bigcup _{n\geq m}A_n\r )\r ) =\bigcup _m\bigcap _{n\geq
m}\Omega \sc A_n =\{\Omega \sc A_n\text { e.v.}\}.
\end{align*}
It follows immediately that \(1-\P [A_n\text { i.o.}]=\P [A_n^c\text { e.v.}].\)
</p>
</li>
<li>


<p>
<span class="textnormal">(c)</span> Define \(B_m=\cap _{n\geq m} A_n\). Note that \(B_m\) is increasing. Note that \(\P [B_m]\leq \P [A_m]\) because \(B_m\sw A_m\). Thus by Lemma <a
href="Probability-with-Measure.html#l:monotone_events_P">5.1.1</a> we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{0}\)</span>
<!--


        P[An e.v.] = P[∪m Bm ] = lim P[Bm ] = lim inf P[Bm ] ≤ lim inf P[Am ].        (B.1)                                                                                        --><a id="eq:evbound"></a><!--
                                 m→∞            m→∞              m→∞

-->
<p>


\begin{equation}
\label {eq:evbound} \P [A_n\text { e.v.}]=\P [\cup _m B_m]=\lim _{m\to \infty }\P [B_m]=\liminf _{m\to \infty }\P [B_m]\leq \liminf _{m\to \infty }\P [A_m].
\end{equation}


</p>
<p>
In the above, we must switch from \(\lim \) to \(\liminf \) before using \(\P [B_m]\leq \P [A_m]\), because we cannot be sure if \(\lim _n\P [A_n]\) exists (and in general it will not).
</p>
<p>
Using (b), we then have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{1}\)</span>
<!--


                                                       P[An i.o.] = 1 − P[Acn e.v.] ≥ 1 − lim inf P[Acm ] = 1 − lim inf (1 − P[Am ]) = − lim inf −P[Am ] = lim sup P[Am ].
                                                                                              m→∞               m→∞                      m→∞                m→∞
                                                                                      (B.2)                                                                                       --><a id="eq:iobound"></a><!--
-->
<p>


\begin{equation}
\label {eq:iobound} \P [A_n\text { i.o.}]=1-\P [A_n^c\text { e.v.}]\geq 1-\liminf _{m\to \infty }\P [A^c_m]=1-\liminf _{m\to \infty }(1-\P [A_m])=-\liminf _{m\to \infty }-\P
[A_m]=\limsup _{m\to \infty }\P [A_m].
\end{equation}


</p>
<p>
Note that \(\liminf _{m\to \infty }\P [A_m]\leq \limsup _{m\to \infty }\P [A_m]\) holds automatically. Putting <span class="textup">(<a
href="Solutions-exercises.html#eq:iobound">B.2</a>)</span> and <span class="textup">(<a href="Solutions-exercises.html#eq:iobound">B.2</a>)</span> together completes the argument.
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-rv_sequences.html#ps:D_not_P"><b>7.3</b></a> The sequence is assumed independent and identically distributed, which means that \(\P [X_n\leq x]=\P [X_1\leq
x]\) for all \(x\), and in particular \(\P [X_n\leq x]\to \P [X_1\leq x]\). Thus \(X_n\stackrel {d}{\to } X\) in distribution.
</p>
<p>
Let \(a\in (0,1]\). Since \(X_n\) only takes the value \(0\) and \(1\), \(|X_n-X_1|\) only takes the values \(0\) and \(1\). Thus \(\{|X_n-X|&gt;a\}=\{|X_n-X|=1\}=\{X_n=1,X_1=0\}\cup
\{X_n=0,X_1=1\}\). For \(n&gt;1\), since \(X_n\) and \(X_1\) are independent we thus have
</p>
<p>
\[\P [|X_n-X|&gt;a]=\P [X_n=1,X_1=0]+\P [X_n=0,X_1=1]=\frac 12\frac 12+\frac 12\frac 12=\frac 12\]
</p>
<p>
which does not tend to zero as \(n\to \infty \). Thus \(X_n\) does not converge to \(X\) in probability.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-rv_sequences.html#ps:as_borel_cantelli"><b>7.4</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We have
</p>
<p>
\[\E [|X_n-0|]=\E [X_n]=n\frac {1}{n^2}+0\l (1-\frac 1{n^2}\r )=\frac {1}{n^2}\to 0\]
</p>
<p>
so \(X_n\stackrel {L^1}{\to }0\). Since \(\sum \frac {1}{n^2}&lt;\infty \), by the second Borel-Cantelli lemma we have \(\P [X_n=n\text { i.o.}]=0\). Since \(X_n\) is either equal to \(n^2\) or
\(0\), this means that \(\P [X_n=0\text { e.v.}]=1\). Thus \(X_n\stackrel {a.s.}{\to } 0\).
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> We have
</p>
<p>
\[\E [|X_n-0|]=\E [X_n]=n\frac {1}{n}+0\l (1-\frac 1{n}\r )=1\]
</p>
<p>
which does not tend to zero, so \(X_n\) does not converge to \(0\) in \(L^1\). Since \(\sum \frac {1}{n}=\infty \) and the \(X_n\) are independent, by the second Borel-Cantelli lemma we have \(\P
[X_n=n\text { i.o.}]=1\). Thus \(X_n\) does not convergence almost surely to \(0\).
</p>


</li>
<li>


<p>
<span class="textnormal">(c)</span> We have
</p>
<p>
\[\E [|X_n-0|]=\E [X_n]=n^2\frac {1}{n^2}+0\l (1-\frac 1{n^2}\r )=1\]
</p>
<p>
which does not tend to zero, so \(X_n\) does not converge to \(0\) in \(L^1\). Since \(\sum \frac {1}{n^2}&lt;\infty \), by the second Borel-Cantelli lemma we have \(\P [X_n=n^2\text { i.o.}]=0\).
Since \(X_n\) is either equal to \(n^2\) or \(0\), this means that \(\P [X_n=0\text { e.v.}]=1\). Thus \(X_n\stackrel {a.s.}{\to } 0\).
</p>


</li>
<li>


<p>
<span class="textnormal">(d)</span> We have
</p>
<p>
\[\E [|X_n-0|]=\E [X_n]=\sqrt {n}\frac {1}{n}+0\l (1-\frac 1{n}\r )=\frac {1}{\sqrt {n}}\to 0\]
</p>
<p>
so \(X_n\stackrel {L^1}{\to }0\). Since \(\sum \frac {1}{n}=\infty \) and the \(X_n\) are independent, by the second Borel-Cantelli lemma we have \(\P [X_n=\sqrt {n}\text { i.o.}]=1\). Thus
\(X_n\) does not convergence almost surely to \(0\).
</p>


</li>
<li>


<p>
<span class="textnormal">(e)</span> In cases (a), (c) and (d) this follows from Lemma <a href="Convergence-random-variables.html#l:conv_modes">7.2.1</a>. For case (b), since \(X_n\) only takes the
values \(0\) and \(n\) we have that \(\{|X_n-0|&gt;a\}=\{X_n=n\}\) whenever \(a&lt;n\), in which case \(\P [|X_n-0|&gt;a]=\P [X_n=n]=\frac {1}{n}\to 0\) as \(n\to \infty \). Thus \(X_n\stackrel
{\P }{\to } 0\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-rv_sequences.html#ps:P_not_as"><b>7.5</b></a> Let us first show that \(X_n\stackrel {\P }{\to }0\). Given any \(\eps &gt; 0\) and \(c &gt; 0\) we can find
\(m\in \N \) such that \(\frac {1}{2^{m}c} &lt; \eps \). The key point is that for \(n&gt;2^m\) the length of the interval \(A_n\) is less than or equal to \(\frac {1}{2^m}\), and since our probability
measure is Lebesgue measure this gives \(\E [1_{A_n}]\leq \frac {1}{2^m}\). Hence, for all \(n &gt; 2^m\), by Markov’s inequality
</p>
<p>
\[ \P [|X_{n} - 0| &gt; c] = \P [{\1}_{A_{n}} &gt; c] \leq \frac {\E [{\1}_{A_{n}}]}{c} &lt; \frac {1}{2^{m}c} &lt; \eps .\]
</p>
<p>
On the other hand \((X_{n})\) cannot converge to \(0\) almost surely since given any \(\nN \), we can find \(m &gt; n\) so that \(A_{m}\) and \(A_{n}\) are disjoint, in which case for any \(\omega \in
\Omega \) we have \(X_n(\omega )-X_m(\omega )={\1}_{A_{n}}(\omega ) - {\1}_{A_{m}}(\omega )\) is equal to either \(1-0\) or \(0-1\). In either case, \(|X_n(\omega )-X_m(\omega )|=1\). Since \(n\)
was arbitrary and \(m\geq n\), this means \(X_n(\omega )\) cannot converge (to anything) as \(n\to \infty \). In particular, there is no almost sure convergence to zero.
</p>
<p>
<i>The best way to think about this question is to rewrite it in terms of probability. Lebesgue measure on \([0,1]\) is the distribution of a uniform random variable \(U\). Then \(X_n=\1_{A_n}\) is equal to \(1\)
if that uniform random variable falls into \(A_n\), and zero otherwise. Fix some sampled value for \(U\), and then think about how the sequence \(X_n\) will behave. </i>
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-rv_sequences.html#ps:unique_limit"><b>7.6</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We need to show that \(X\) and \(Y\) have the same distribution (i.e.&nbsp;they have the same distribution functions \(F_X(x)=F_Y(x)\)).
</p>
<p>
If \(x\in \R \) is such that \(\P [X=x]=0\) then we have both \(\P [X_n\leq x]\to \P [X\leq x]\) and \(\P [X_n\leq x]\to \P [Y\leq x]\), so by uniqueness of limits for real sequences we have \(\P
[X\leq x]=\P [Y\leq x]\).
</p>
<p>
By exercise <a href="Exercises-on-Chapter-ref-c-prob_with_meas.html#ps:countable_atoms"><b>5.5</b></a> there are at most countably many \(x\in \R \) such that \(\P [X=x]&gt;0\). Therefore, for
all but a countable set of \(x\in \R \), we have \(F_X(x)=F_Y(x)\). From Lemma <a href="The-cumulative-distribution-function.html#l:cdf">5.2.1</a> we have that both \(F_X\) and \(F_Y\) are right
continuous. Hence, in fact, \(F_X(x)=F_Y(y)\) at all \(x\in \R \).
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> By definition of convergence in probability, for any \(a&gt;0\), for any \(\eps &gt;0\) there exists \(N\in \N \) such that, for all \(n\geq N\),
</p>
<p>
\[\P [|X_n-X|&gt;a]&lt;\eps \hspace {1pc}\text { and }\hspace {1pc}\P [|X_n-Y|&gt;a]&lt;\eps .\]
</p>
<p>
By the triangle inequality we have
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{2}\)</span>
<!--


                                                                     P[|X − Y | > 2a] = P[|X − Xn + Xn − Y | > 2a] ≤ P[|X − Xn | + |Xn − Y | > 2a].
                                                                              (B.3)                                                                                          --><a id="eq:ps_uniq_limit"></a><!--
-->
<p>


\begin{equation}
\label {eq:ps_uniq_limit} \P [|X-Y|&gt;2a]=\P [|X-X_n+X_n-Y|&gt;2a]\leq \P [|X-X_n|+|X_n-Y|&gt;2a].
\end{equation}


</p>
<p>
If \(|X-X_n|+|X_n-Y|&gt;2a\) then \(|X-X_n|&gt;a\) or \(|X_n-Y|&gt;a\) (or possibly both). Hence, continuing <span class="textup">(<a
href="Solutions-exercises.html#eq:ps_uniq_limit">B.3</a>)</span>,
</p>
<p>
\[\P [|X-Y|&gt;2a]\leq \P [|X_n-X|&gt;a]+\P [|X_n-Y|&gt;a]\leq 2\eps .\]
</p>
<p>
Since this is true for any \(\eps &gt;0\) and any \(a&gt;0\), we have \(\P [X=Y]=1\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-rv_sequences.html#ps:wlln_uncor"><b>7.7</b></a> Without loss of generality (as in the argument given for the general case) we may assume that \(\E (X_{n}) =
0\) for all \(\nN \). If this is not the case, we consider \(X_{n} - \mu \) in place of \(X_{n}\).
</p>
<p>
The proof proceeds in exactly the same way as when the random variables are independent, but needs the following extra calculation:
</p>
<p>
\[\begin {aligned} \var (\overline {X_n}) &amp; = \frac {1}{n^{2}}\E \l [\left (\sum _{i=1}^{n}X_{i}\right )^{2}\r ]\\ &amp; = \frac {1}{n^{2}}\sum _{i=1}^{n}\sum _{j=1}^{n}\E
[X_{i}X_{j}]\\ &amp; = \frac {1}{n^{2}}\sum _{i=1}^{n}\E [X_{i}^{2}]\\ &amp; = \frac {\sigma ^{2}}{n}.                             \end {aligned}\]
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-rv_sequences.html#ps:conv_prob_vs_L1"><b>7.8</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Write
</p>
<p>
\[\min (1,X)=\min (1,X)\1_{\{X&lt;a\}} + \min (1,X)\1_{\{X\geq a\}}\]
</p>
<p>
and take expectations, giving
</p>
<span class="hidden"> \(\seteqnumber{0}{B.}{3}\)</span>


<!--



                                                                                 E[min(1, X)] = E[min(1, X)1{X<a} ] + E[min(1, X)1{X≥a} ]

                                                                                                ≤ E[a] + E[1{X≥a} ]

                                                                                                = a + P[X ≥ a].



-->


<p>


\begin{align*}
\E [\min (1,X)] &amp;= \E [\min (1,X)\1_{\{X&lt;a\}}] + \E [\min (1,X)\1_{\{X\geq a\}}] \\ &amp;\leq \E [a] + \E [\1_{\{X\geq a\}}] \\ &amp;=a + \P [X\geq a].
\end{align*}
To deduce the second line of the above we use monotonicity of \(\E \).
</p>


</li>
<li>


<p>
<span class="textnormal">(b)</span> \((\Leftarrow ):\) Suppose that \(\E [\min (1,X)]\to 0\). For \(a\in (0,1]\) we have
</p>
<p>
\[\P [X_n\geq a]=\P [\min (1,X_n)\geq a]\leq \frac {1}{a}\E [\min (1,X_n)]\]
</p>
<p>
which tends to zero as \(n\to \infty \). Here we use Markov’s inequality.
</p>
<p>
For \(a\geq 1\) we have \(\P [X\geq a]\leq \P [X\geq 1]\to 0\).
</p>
<p>
\((\Rightarrow ):\) Suppose that \(X_n\stackrel {\P }{\to }0\). Let \(a\in (0,1]\). Then \(\P [X_n\geq a]\to 0\).
</p>
<p>
From part (a) we have
</p>
<p>
\[0\leq \E [\min (1,X_n)]\leq a+\P [X_n\geq a].\]
</p>
<p>
Let \(\eps &gt;0\). Choose \(a=\frac {\eps }{2}\) and let \(N\in \N \) be large enough that \(\P [X_n\geq a]\leq \frac {\eps }{2}\) for all \(n\geq \N \). Then \(0\leq \E [\min (1,X_n)]\leq
\frac {\eps }{2}+\frac {\eps }{2}=\eps \) for all \(n\geq N\). Hence \(\E [\min (1,X_n)]\to 0\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-rv_sequences.html#ps:N_char_func"><b>7.9</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> Write
</p>
<p>
\[\begin {aligned} \phi (u) &amp; = \frac {1}{\sqrt {2 \pi }}\int _{\R }e^{iuy}e^{-\frac {1}{2}y^{2}}dy \\ &amp; = \frac {1}{\sqrt {2 \pi }}\int _{\R }\cos (uy)e^{-\frac
{1}{2}y^{2}}dy + i \frac {1}{\sqrt {2 \pi }}\int _{\R }\sin (uy)e^{-\frac {1}{2}y^{2}}dy.                         \end {aligned}\]
</p>
<p>
As \(|\cos (uy)ye^{-\frac {1}{2}u^{2}}| \leq |y|e^{-\frac {1}{2}y^{2}}\) and \(|\sin (uy)ye^{-\frac {1}{2}u^{2}}| \leq |y|e^{-\frac {1}{2}y^{2}}\) and \(y \rightarrow |y|e^{-\frac
{1}{2}y^{2}}\) is integrable on \(\R \), we may apply Problem <a href="Exercises-on-Chapter-ref-c-lebesgue_integration.html#ps:diff_under_int"><b>4.17</b></a> to real and imaginary parts, to
deduce that \(u \rightarrow \phi (u)\) is differentiable and its derivative at \(u \in \R \) is
</p>
<p>
\[ \phi &apos;(u) = \frac {i}{\sqrt {2 \pi }}\int _{\R }e^{iuy}ye^{-\frac {1}{2}y^{2}}dy.\]
</p>
<p>
Now integrate by parts to find that
</p>
<p>
\[\begin {aligned} \phi &apos;(u) &amp; = \frac {i}{\sqrt {2 \pi }}\left [-e^{iuy}e^{-\frac {1}{2}y^{2}}\right ]_{-\infty }^{\infty } - \frac {1}{\sqrt {2 \pi }}\int _{-\infty
}^{\infty }ue^{iuy}e^{-\frac {1}{2}y^{2}}dy\\ &amp; = -u \phi (u).                    \end {aligned}\]
</p>
<p>
So we have the ordinary differential equation \(\ds \frac {d \phi (u)}{du} = -u \phi (u)\) with initial condition, \(\Phi _{Y}(0) = 1\) and the result follows by using the standard separation of variables
technique.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> First suppose that we have established the case for \(Y\sim N(0,1)\) i.e.&nbsp;we know that \(\phi _{Y}(u) = e^{-\frac {1}{2}u^{2}}\) for all \(u \in \R \). Then
since \(X = \mu + \sigma Y\), we have
</p>
<p>
\[\begin {aligned} \phi _{X}(u) &amp; = \E (e^{iu (\mu + \sigma Y)})\\ &amp; = e^{iu\mu }\E (e^{i(u\sigma )Y}) = e^{i\mu u - \frac {1}{2}\sigma ^{2}u^{2}},\end {aligned}.\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-rv_sequences.html#ps:clt_bernoulli"><b>7.10</b></a> In this case \(\mu = p\) and \(\sigma = \sqrt {p(1-p)}\) and so we can write
</p>
<p>
\[ \frac {S_{n} - np}{\sqrt {np(1-p)}} \tod N(0,1)\]
</p>
<p>
The random variable \(S_{n}\) is the sum of \(n\) i.i.d.&nbsp;Bernoulli random variables and so is binomial with mean \(np\) and variance \(np(1-p)\). Hence for large \(n\) it is approximately equal to
\(N(np,np(1-p))\) in distribution. If we take \(p=\frac {1}{n}\) this allows us to approximate normal random variables with binomial random variables.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-rv_sequences.html#ps:e_lim_with_error"><b>7.11</b></a> We will apply the AM-GM inequality from Exercise <a
href="Exercises-on-Chapter-ref-c-ineqs.html#ps:jensen_am_gm"><b>6.4</b></a> to \(x_1=x_2=\ldots =x_{n}=1+\frac {x}{n}\) and \(x_{n+1}=1\), where \(x\geq -n\) (so that \(x_i\geq 0\)). This
gives
</p>
<p>
\[\l (1+\frac {x}{n}\r )^{\frac {n}{n+1}}\leq \frac {n(1+\frac {x}{n})+1}{n+1}=1+\frac {x}{n+1}.\]
</p>
<p>
Raising both sides to the power of \(n+1\) we obtain that \(f_n(x)\leq f_{n+1}(x)\), for \(x\geq -n\) and \(n\in \N \).
</p>
<p>
The sequence \(f_n(x)=(1+\frac {x}{n})^n\) is thus a sequence of continuous functions that satisfies \(f_n(x)\leq f_{n+1}(x)\) for all \(x\geq -n\). The pointwise limit is \(f(x)=e^x\). Hence Dini’s
theorem (applied to the sequence \((f_n)){n\geq M}\)) gives that the convergence is uniform on all intervals \([-M,M]\) where \(M\in (0,\infty )\).
</p>
<p>
<em>Note: we don’t have uniform convergence on \(\R \). We have to work around this difficulty by restricting to an interval \([-M,M]\) instead.</em>
</p>
<p>
Uniform convergence implies that \(f_n(x_n)\to f(x)\) whenever \(x_n\to n\). In the notation of Lemma <a href="The-central-limit-theorem.html#l:e_lim_with_error">7.5.2</a>, if we set
\(x_n=y+\alpha _n\) then we have \(x_n\to y\). Choosing \(M=\sup _n |x_n|\), which is finite because the convergent sequence \((x_n)\) is bounded, we obtain that \(f_n(y+\alpha _n)\to f(y)\), that is
\((1+\frac {y+\alpha _n}{n})^n\to e^y\), as required.
</p>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-rv_sequences.html#ps:conv_const"><b>7.12</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> For the deterministic random variable \(X=c\), the only discontinuity of its distribution function is at the value \(c\), where it jumps from \(0\) to \(1\). Therefore, from
\(X_n\stackrel {d}{\to } c\) we have that for all \(\eps &gt;0\), \(\P [X_n\leq c-\eps ]\to \P [c&lt;c-\eps ]=0\) and \(\P [X_n\leq c+\eps ]\to \P [c\leq c+\eps ]=1\), as \(n\to \infty \).
From the second statement we may deduce that \(\P [X_n\geq c+\eps ]\to 0\) for all \(\eps &gt;0\). We thus have
</p>
<p>
\[\P [|X_n-c|\geq \eps ]] = \P [X_n\leq c-\eps ]+\P [X_n\geq c+\eps ]\to 0 \]
</p>
<p>
as \(n\to \infty \), which is to say that \(X_n\stackrel {\P }{\to } c\).
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Let \((X_n)\) be a sequence of independent random variables such that \(X_n\stackrel {\P }{\to } X\). We will argue by contradiction: suppose that \(X\) is not
almost equal to a constant. There there exists \(c\in \R \) and \(\eps ,\de &gt;0\) such that \(\P [X\leq c-\eps ]\geq \de \) and \(\P [X\geq c+\eps ]\geq \de \).
</p>
<p>
By Lemma <a href="Convergence-random-variables.html#l:P_as_sub">7.2.4</a> there is a subsequence \((Y_n)\) of \((X_n)\) such that \(Y_n\stackrel {a.s.}{\to } X\). By Lemma <a
href="Convergence-random-variables.html#l:conv_modes">7.2.1</a> we have that \(Y_n\stackrel {\P }{\to } X\).
</p>
<p>
Since \(Y_n\stackrel {\P }{\to } X\), there exists \(N\in \N \) such that for all \(n\geq N\) we have \(\P [|Y_n-X|\geq \eps /2]\leq \de /2\). For \(n\geq N\) we thus have \(\P [Y_n\leq c-\eps
/2]\geq \de /2\) and \(\P [Y_n\geq c+\eps /2]\geq \de /2\). Hence also
</p>
<p>
\[ \sum _n\P [Y_n\leq c-\eps /2]=\infty \quad \quad \text { and }\quad \quad \sum _n\P [Y_n\geq c+\eps /2]=\infty .                                \]
</p>
<p>
The \((X_n)\) are independent, hence so are the elements of the subsequence \((Y_n)\). From the second Borel-Cantelli lemma we obtain that
</p>
<p>
\[\P [Y_n\leq c-\eps /2\text { infinitely often, and }Y_n\geq c+\eps /2\text { infinitely often}]=1.\]
</p>
<p>
However, this contradicts the fact that \(Y_n\stackrel {a.s}{\to }X\).
</p>
<p>
We have therefore reached a contradiction, so in fact there exists some \(c\in \R \) such that \(\P [X=c]=1\).
</p>
</li>
</ul>
</li>
<li>


<p>
<a href="Exercises-on-Chapter-ref-c-rv_sequences.html#ps:complete_conv"><b>7.13</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textnormal">(a)</span> We first show that complete convergence implies almost sure convergence. This part does not require independence. Let \(A_\eps =\{|X_n-X|\leq \eps \;\text {
e.v.}\}\) and note that \(A_{1/m}\) is a decreasing sequence of sets (as \(m\in \N \) increases), and that
</p>
<p>
\[\bigcap _{m\in \N }A_{1/m}=\bigcap _{\eps &gt;0}A_\eps =\{X_n\to X\}.\]
</p>
<p>
If \(X_n\) converges completely to \(X\) then, by the first Borel-Cantelli lemma, \(\P [|X_n-X|\geq \eps \;\text { i.o.}]=0\) which implies that \(\P [A_{1/m}]=1\) for all \(m\in \N \). Since
\((A_{1/m})\) is decreasing we obtain that \(\P [\cap _{m\in \N }A_{1/m}]=1\), and hence \(\P [X_n\to X]=1\), so we have almost sure convergence.
</p>
<p>
Let us now show that if the \((X_n)\) are independent then almost sure convergence implies complete convergence. By part (b) of exercise <a
href="Exercises-on-Chapter-ref-c-rv_sequences.html#ps:conv_const"><b>7.12</b></a> \(X\) we have that \(X_n\stackrel {a.s.}{\to } X=c\) where \(c\in \R \) is deterministic. For any \(\eps
&gt;0\), the sequence of events \(E_n(\eps )=\{|X_n-c|\geq \eps \}\) are independent. The fact that \(X_n\stackrel {a.s.}{\to } c\) means that \(\P [E_n(\eps )\text { i.o.}]=0\). Hence by the
second Borel-Cantelli lemma (here we use independence) we must have \(\sum _n \P [E_n(\eps )]&lt;\infty \), as required.
</p>
</li>
<li>


<p>
<span class="textnormal">(b)</span> Let \(U\) be a uniform random variable on \([0,1]\) and define
</p>
<p>
\[X_n= \begin {cases} 1 &amp; \text { if }U\leq \frac 1n \\ 0 &amp; \text { otherwise.} \end {cases} \]
</p>
<p>
Then \(\P [X_n\to 0]=\P [U&gt;0]=1\), so \(X_n\stackrel {a.s.}{\to } 0\).
</p>
<p>
For \(\eps \in (0,1]\) we have \(\P [|X_n-0|\geq \eps ]=\P [X_n=1]=\frac 1n\), so \(\sum _n\P [|X_n-0|\geq \eps ]=\infty \), hence \(X_n\) does not converge completely to \(0\).
</p>
</li>
</ul>
</li>
</ul>
<a id="notes-autofile-last"></a>
</section>

</main>

</div>

<footer>

<p>
Copyright Nic Freeman, Sheffield University, last updated September 19, 2023
</p>

</footer>



<nav class="botnavigation"><a href="notes.html" class="linkhome" >
Home</a></nav>

</body>
</html>
