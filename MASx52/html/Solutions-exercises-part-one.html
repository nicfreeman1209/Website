<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="UTF-8" />
<meta name="author" content="Nic Freeman" />
<meta name="generator" content="LaTeX Lwarp package" />
<meta name="description" content="MAS352/452/6052 Stochastic Processes and Financial Mathematics, Sheffield University, January 24, 2022." />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<!--[if lt IE 9]>
<script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
<![endif]-->
<title>MASx52 — Solutions to exercises (part one)</title>
<link rel="stylesheet" type="text/css" href="sans-serif-lwarp-sagebrush.css" />


<script>
// Lwarp MathJax emulation code
//
// Based on code by Davide P. Cervone.
// Equation numbering: https://github.com/mathjax/MathJax/issues/2427
// Starred and ifnextchar macros: https://github.com/mathjax/MathJax/issues/2428
//
// Modified by Brian Dunn to adjust equation numbering and add subequations.
//
// LaTeX can use \seteqnumber{subequations?}{section}{number} before each equation.
// subequations? is 0 usually, 1 if inside subequations.
// section is a string printed as-is, or empty.
// number is auto-incremented by MathJax between equations.
//
MathJax = {
     subequations: "0",
     section: "",
     loader: {
         load: ['[tex]/tagFormat'],
     },
     startup: {
         ready() {
             //       These would be replaced by import commands if you wanted to make
             //       a proper extension.
             const Configuration = MathJax._.input.tex.Configuration.Configuration;
             const CommandMap = MathJax._.input.tex.SymbolMap.CommandMap;
             const Macro = MathJax._.input.tex.Symbol.Macro;
             const TexError = MathJax._.input.tex.TexError.default;
             const ParseUtil = MathJax._.input.tex.ParseUtil.default;
             const expandable = MathJax._.util.Options.expandable;


             //       Insert the replacement string into the TeX string, and check
             //       that there haven't been too many maxro substitutions (prevents
             //       infinite loops).
             const useArgument = (parser, text) => {
                  parser.string = ParseUtil.addArgs(parser, text, parser.string.slice(parser.i));
                  parser.i = 0;
                  if (++parser.macroCount > parser.configuration.options.maxMacros) {
                      throw new TexError('MaxMacroSub1',
                      'MathJax maximum macro substitution count exceeded; ' +
                      'is there a recursive macro call?');
                  }
             }


             //       Create the command map for \ifstar, \ifnextchar, \seteqnumber
             new CommandMap('ifstar-ifnextchar-setequnumber', {
                  ifstar: 'IfstarFunction',
                  ifnextchar: 'IfnextcharFunction',
                  seteqnumber: 'SeteqnumberFunction'
             }, {
                  //      This function implements an ifstar macro.
                  IfstarFunction(parser, name) {
                      const resultstar = parser.GetArgument(name);
                      const resultnostar = parser.GetArgument(name);
                      const star = parser.GetStar();                        // true if there is a *
                      useArgument(parser, star ? resultstar : resultnostar);
                  },


                  //      This function implements an ifnextchar macro.
                  IfnextcharFunction(parser, name) {
                      let whichchar = parser.GetArgument(name);
                      if (whichchar.match(/^(?:0x[0-9A-F]+|[0-9]+)$/i)) {
                          // $ syntax highlighting
                          whichchar = String.fromCodePoint(parseInt(whichchar));
                      }
                      const resultnextchar = parser.GetArgument(name);
                      const resultnotnextchar = parser.GetArgument(name);
                      const gotchar = (parser.GetNext() === whichchar);
                      useArgument(parser, gotchar ? resultnextchar : resultnotnextchar);
                  },


                  //      This function modifies the equation numbers.
                  SeteqnumberFunction(parser, name) {
                          //   Get the macro parameters
                          const star = parser.GetStar();                       // true if there is a *
                          const optBrackets = parser.GetBrackets(name);        // contents of optional brackets
                          const newsubequations = parser.GetArgument(name);       // the subequations argument
                          const neweqsection = parser.GetArgument(name);       // the eq section argument
                          const neweqnumber = parser.GetArgument(name);        // the eq number argument
                          MathJax.config.subequations=newsubequations ;        // a string with boolean meaning
                          MathJax.config.section=neweqsection ;                // a string with numeric meaning
                          parser.tags.counter = parser.tags.allCounter = neweqnumber ;
                  }
             });


             //       Create the ifstar-ifnextchar-setequnumber package
             Configuration.create('ifstar-ifnextchar-setequnumber', {
                  handler: {macro: ['ifstar-ifnextchar-setequnumber']}
             });


             MathJax.startup.defaultReady();


             // For forward references:
             MathJax.startup.input[0].preFilters.add(({math}) => {
                  if (math.inputData.recompile){
                          MathJax.config.subequations = math.inputData.recompile.subequations;
                          MathJax.config.section = math.inputData.recompile.section;
                  }
             });
             MathJax.startup.input[0].postFilters.add(({math}) => {
                  if (math.inputData.recompile){
                          math.inputData.recompile.subequations = MathJax.config.subequations;
                          math.inputData.recompile.section = MathJax.config.section;
                  }
             });
         }       // ready
     },           // startup


     tex: {
         packages: {'[+]': ['tagFormat', 'ifstar-ifnextchar-setequnumber']},
         tags: "ams",
                  tagFormat: {
                          number: function (n) {
                               if(MathJax.config.subequations==0)
                                      return(MathJax.config.section + n);
                               else
                                      return(MathJax.config.section + String.fromCharCode(96+n));
                          },
                  },
     }
}
</script>


<script
         id="MathJax-script"
         src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"
></script>


</head>
<body>



<a id="notes_1-autopage-264"></a>
<nav class="topnavigation" ><a href="notes_1.html" class="linkhome" >
Home</a></nav>

<header>

<p>
last updated: January 24, 2022
</p>

</header>



<div class="bodyandsidetoc" >
<div class="sidetoccontainer" >



<nav class="sidetoc" >



<div class="sidetoctitle" >

<p>
<span class="sidetocthetitle" >Stochastic Processes and Financial Mathematics<br />
(part one)</span>
</p>

<p>
Contents
</p>
</div>



<div class="sidetoccontents" >

<p>
<a href="notes_1.html" class="linkhome" >
Home</a>
</p>

<p>
<a href="Introduction.html#autosec-4" class="tocchapter" >
<span class="sectionnumber" >0</span>&#x2003;Introduction</a>
</p>



<p>
<a href="Introduction.html#autosec-5" class="tocsection" >
<span class="sectionnumber" >0.1</span>&#x2003;Organization</a>
</p>



<p>
<a href="Expectation-Arbitrage.html#autosec-12" class="tocchapter" >
<span class="sectionnumber" >1</span>&#x2003;Expectation and Arbitrage</a>
</p>



<p>
<a href="Expectation-Arbitrage.html#autosec-13" class="tocsection" >
<span class="sectionnumber" >1.1</span>&#x2003;Betting on coin tosses</a>
</p>



<p>
<a href="The-one-period-market.html#autosec-16" class="tocsection" >
<span class="sectionnumber" >1.2</span>&#x2003;The one-period market</a>
</p>



<p>
<a href="Arbitrage.html#autosec-21" class="tocsection" >
<span class="sectionnumber" >1.3</span>&#x2003;Arbitrage</a>
</p>



<p>
<a href="Modelling-discussion.html#autosec-31" class="tocsection" >
<span class="sectionnumber" >1.4</span>&#x2003;Modelling discussion</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-chap-pricing.html#autosec-33" class="tocsection" >
<span class="sectionnumber" >1.5</span>&#x2003;Exercises on Chapter&nbsp;<a href="Expectation-Arbitrage.html#chap:pricing">1</a></a>
</p>



<p>
<a href="Probability-spaces-random-variables.html#autosec-37" class="tocchapter" >
<span class="sectionnumber" >2</span>&#x2003;Probability spaces and random variables</a>
</p>



<p>
<a href="Probability-spaces-random-variables.html#autosec-38" class="tocsection" >
<span class="sectionnumber" >2.1</span>&#x2003;Probability measures and \(\sigma \)-fields</a>
</p>



<p>
<a href="Random-variables.html#autosec-48" class="tocsection" >
<span class="sectionnumber" >2.2</span>&#x2003;Random variables</a>
</p>



<p>
<a href="Infinite.html#autosec-62" class="tocsection" >
<span class="sectionnumber" >2.3</span>&#x2003;Infinite \(\Omega \)</a>
</p>



<p>
<a href="Expectation.html#autosec-68" class="tocsection" >
<span class="sectionnumber" >2.4</span>&#x2003;Expectation</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-chap-prob_meas.html#autosec-76" class="tocsection" >
<span class="sectionnumber" >2.5</span>&#x2003;Exercises on Chapter <a href="Probability-spaces-random-variables.html#chap:prob_meas">2</a></a>
</p>



<p>
<a href="Conditional-expectation-martingales.html#autosec-81" class="tocchapter" >
<span class="sectionnumber" >3</span>&#x2003;Conditional expectation and martingales</a>
</p>



<p>
<a href="Conditional-expectation-martingales.html#autosec-82" class="tocsection" >
<span class="sectionnumber" >3.1</span>&#x2003;Conditional expectation</a>
</p>



<p>
<a href="Properties-conditional-expectation.html#autosec-88" class="tocsection" >
<span class="sectionnumber" >3.2</span>&#x2003;Properties of conditional expectation</a>
</p>



<p>
<a href="Martingales.html#autosec-94" class="tocsection" >
<span class="sectionnumber" >3.3</span>&#x2003;Martingales</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-chap-cond_exp.html#autosec-105" class="tocsection" >
<span class="sectionnumber" >3.4</span>&#x2003;Exercises on Chapter <a href="Conditional-expectation-martingales.html#chap:cond_exp">3</a></a>
</p>



<p>
<a href="Stochastic-processes.html#autosec-109" class="tocchapter" >
<span class="sectionnumber" >4</span>&#x2003;Stochastic processes</a>
</p>



<p>
<a href="Stochastic-processes.html#autosec-111" class="tocsection" >
<span class="sectionnumber" >4.1</span>&#x2003;Random walks</a>
</p>



<p>
<a href="Urn-processes.html#autosec-117" class="tocsection" >
<span class="sectionnumber" >4.2</span>&#x2003;Urn processes</a>
</p>



<p>
<a href="A-branching-process.html#autosec-122" class="tocsection" >
<span class="sectionnumber" >4.3</span>&#x2003;A branching process</a>
</p>



<p>
<a href="Other-stochastic-processes.html#autosec-126" class="tocsection" >
<span class="sectionnumber" >4.4</span>&#x2003;Other stochastic processes</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs.html#autosec-129" class="tocsection" >
<span class="sectionnumber" >4.5</span>&#x2003;Exercises on Chapter <a href="Stochastic-processes.html#chap:stoch_procs">4</a></a>
</p>



<p>
<a href="The-binomial-model.html#autosec-133" class="tocchapter" >
<span class="sectionnumber" >5</span>&#x2003;The binomial model</a>
</p>



<p>
<a href="The-binomial-model.html#autosec-134" class="tocsection" >
<span class="sectionnumber" >5.1</span>&#x2003;Arbitrage in the one-period model</a>
</p>



<p>
<a href="Hedging-in-one-period-model.html#autosec-142" class="tocsection" >
<span class="sectionnumber" >5.2</span>&#x2003;Hedging in the one-period model</a>
</p>



<p>
<a href="Types-financial-derivative.html#autosec-153" class="tocsection" >
<span class="sectionnumber" >5.3</span>&#x2003;Types of financial derivative</a>
</p>



<p>
<a href="The-binomial-model-definition.html#autosec-155" class="tocsection" >
<span class="sectionnumber" >5.4</span>&#x2003;The binomial model (definition)</a>
</p>



<p>
<a href="Portfolios-arbitrage-martingales.html#autosec-159" class="tocsection" >
<span class="sectionnumber" >5.5</span>&#x2003;Portfolios, arbitrage and martingales</a>
</p>



<p>
<a href="Hedging.html#autosec-168" class="tocsection" >
<span class="sectionnumber" >5.6</span>&#x2003;Hedging</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-chap-bin_model.html#autosec-177" class="tocsection" >
<span class="sectionnumber" >5.7</span>&#x2003;Exercises on Chapter <a href="The-binomial-model.html#chap:bin_model">5</a></a>
</p>



<p>
<a href="Convergence-random-variables.html#autosec-182" class="tocchapter" >
<span class="sectionnumber" >6</span>&#x2003;Convergence of random variables</a>
</p>



<p>
<a href="Convergence-random-variables.html#autosec-183" class="tocsection" >
<span class="sectionnumber" >6.1</span>&#x2003;Modes of convergence</a>
</p>



<p>
<a href="The-monotone-convergence-theorem.html#autosec-189" class="tocsection" >
<span class="sectionnumber" >6.2</span>&#x2003;The monotone convergence theorem</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-chap-rv_conv.html#autosec-194" class="tocsection" >
<span class="sectionnumber" >6.3</span>&#x2003;Exercises on Chapter <a href="Convergence-random-variables.html#chap:rv_conv">6</a></a>
</p>



<p>
<a href="Stochastic-processes-martingale-theory.html#autosec-199" class="tocchapter" >
<span class="sectionnumber" >7</span>&#x2003;Stochastic processes and martingale theory</a>
</p>



<p>
<a href="Stochastic-processes-martingale-theory.html#autosec-200" class="tocsection" >
<span class="sectionnumber" >7.1</span>&#x2003;The martingale transform</a>
</p>



<p>
<a href="Roulette.html#autosec-203" class="tocsection" >
<span class="sectionnumber" >7.2</span>&#x2003;Roulette</a>
</p>



<p>
<a href="The-martingale-convergence-theorem.html#autosec-209" class="tocsection" >
<span class="sectionnumber" >7.3</span>&#x2003;The martingale convergence theorem</a>
</p>



<p>
<a href="Long-term-behaviour-stochastic-processes.html#autosec-217" class="tocsection" >
<span class="sectionnumber" >7.4</span>&#x2003;Long term behaviour of stochastic processes</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#autosec-236" class="tocsection" >
<span class="sectionnumber" >7.5</span>&#x2003;Exercises on Chapter <a href="Stochastic-processes-martingale-theory.html#chap:stoch_procs_1">7</a></a>
</p>



<p>
<a href="Further-theory-stochastic-processes.html#autosec-241" class="tocchapter" >
<span class="sectionnumber" >8</span>&#x2003;Further theory of stochastic processes \((\Delta )\)</a>
</p>



<p>
<a href="Further-theory-stochastic-processes.html#autosec-242" class="tocsection" >
<span class="sectionnumber" >8.1</span>&#x2003;The dominated convergence theorem \((\Delta )\)</a>
</p>



<p>
<a href="The-optional-stopping-theorem.html#autosec-247" class="tocsection" >
<span class="sectionnumber" >8.2</span>&#x2003;The optional stopping theorem \((\Delta )\)</a>
</p>



<p>
<a href="Hitting-probabilities-random-walks.html#autosec-254" class="tocsection" >
<span class="sectionnumber" >8.3</span>&#x2003;Hitting probabilities of random walks \((\Delta )\)</a>
</p>



<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_2.html#autosec-261" class="tocsection" >
<span class="sectionnumber" >8.4</span>&#x2003;Exercises on Chapter <a href="Further-theory-stochastic-processes.html#chap:stoch_procs_2">8</a> \((\Delta )\)</a>
</p>



<p>
<a href="Solutions-exercises-part-one.html#autosec-265" class="tocchapter" >
<span class="sectionnumber" >A</span>&#x2003;Solutions to exercises (part one)</a>
</p>



<p>
<a href="Formula-Sheet-part-one.html#autosec-282" class="tocchapter" >
<span class="sectionnumber" >B</span>&#x2003;Formula Sheet (part one)</a>
</p>



</div>

</nav>

</div>



<div class="bodycontainer" >



<section class="textbody" >

<h1>Stochastic Processes and Financial Mathematics<br />
(part one)</h1>

<!--MathJax customizations:-->



<div class="hidden" >

\(\newcommand{\footnotename}{footnote}\)

\(\def \LWRfootnote {1}\)

\(\newcommand {\footnote }[2][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand {\footnotemark }[1][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand \ensuremath [1]{#1}\)

\(\newcommand {\LWRframebox }[2][]{\fbox {#2}} \newcommand {\framebox }[1][]{\LWRframebox } \)

\(\newcommand {\setlength }[2]{}\)

\(\newcommand {\addtolength }[2]{}\)

\(\newcommand {\setcounter }[2]{}\)

\(\newcommand {\addtocounter }[2]{}\)

\(\newcommand {\cline }[1]{}\)

\(\newcommand {\directlua }[1]{\text {(directlua)}}\)

\(\newcommand {\luatexdirectlua }[1]{\text {(directlua)}}\)

\(\newcommand {\protect }{}\)

\(\def \LWRabsorbnumber #1 {}\)

\(\def \LWRabsorbquotenumber &quot;#1 {}\)

\(\def \mathchar {\ifnextchar &quot;\LWRabsorbquotenumber \LWRabsorbnumber }\)

\(\def \mathcode #1={\mathchar }\)

\(\let \delcode \mathcode \)

\(\let \delimiter \mathchar \)

\(\let \LWRref \ref \)

\(\renewcommand {\ref }{\ifstar \LWRref \LWRref }\)

\(\newcommand {\intertext }[1]{\text {#1}\notag \\}\)

\(\DeclareMathOperator {\var }{var}\)

\(\DeclareMathOperator {\cov }{cov}\)

\(\newcommand {\nN }{n \in \mathbb {N}}\)

\(\newcommand {\Br }{{\cal B}(\R )}\)

\(\newcommand {\F }{{\cal F}}\)

\(\newcommand {\ds }{\displaystyle }\)

\(\newcommand {\st }{\stackrel {d}{=}}\)

\(\newcommand {\uc }{\stackrel {uc}{\rightarrow }}\)

\(\newcommand {\la }{\langle }\)

\(\newcommand {\ra }{\rangle }\)

\(\newcommand {\li }{\liminf _{n \rightarrow \infty }}\)

\(\newcommand {\ls }{\limsup _{n \rightarrow \infty }}\)

\(\newcommand {\limn }{\lim _{n \rightarrow \infty }}\)

\(\def \ra {\Rightarrow }\)

\(\def \to {\rightarrow }\)

\(\def \iff {\Leftrightarrow }\)

\(\def \sw {\subseteq }\)

\(\def \wt {\widetilde }\)

\(\def \mc {\mathcal }\)

\(\def \mb {\mathbb }\)

\(\def \sc {\setminus }\)

\(\def \v {\textbf }\)

\(\def \p {\partial }\)

\(\def \E {\mb {E}}\)

\(\def \P {\mb {P}}\)

\(\def \R {\mb {R}}\)

\(\def \C {\mb {C}}\)

\(\def \N {\mb {N}}\)

\(\def \Q {\mb {Q}}\)

\(\def \Z {\mb {Z}}\)

\(\def \B {\mb {B}}\)

\(\def \~{\sim }\)

\(\def \-{\,;\,}\)

\(\def \|{\,|\,}\)

\(\def \qed {$\blacksquare $}\)

\(\def \1{\unicode {x1D7D9}}\)

\(\def \cadlag {c\‘{a}dl\‘{a}g}\)

\(\def \p {\partial }\)

\(\def \l {\left }\)

\(\def \r {\right }\)

\(\def \F {\mc {F}}\)

\(\def \G {\mc {G}}\)

\(\def \H {\mc {H}}\)

\(\def \Om {\Omega }\)

\(\def \om {\omega }\)

</div>

<p>
<h3 id="autosec-265">Chapter&nbsp;<span class="sectionnumber" >A&#x2003;</span>Solutions to exercises (part one)</h3>
<a id="notes_1-autopage-265"></a>
<a id="notes_1-autofile-40"></a>
<h5 id="autosec-266">Chapter <a href="Expectation-Arbitrage.html#chap:pricing">1</a></h5>
<a id="notes_1-autopage-266"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-pricing.html#ps:portfolio"><b>1.1</b></a> At time \(1\) we would have \(10(1+r)\) in cash and \(5\) units of stock. This gives the value of our assets at time
\(1\) as \(10(1+r)+5S_1\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-pricing.html#ps:portfolio_2"><b>1.2</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> Assuming we don’t buy or sell anything at time \(0\), the value of our portfolio at time \(1\) will be
\(x(1+r)+yS_1\), where \(S_1\) is as in the solution to <a href="Exercises-on-Chapter-ref-chap-pricing.html#ps:portfolio"><b>1.1</b></a>. Since we need to be certain of paying off our debt, we should
assume a worst case scenario for \(S_1\), that is \(S_1=d\). So, we are certain to pay off our debt if and only if
</p>
<p>
\[x(1+r)+ysd&gt;K.\]
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> Since certainty requires that we cover the worst case scenario of our stock performance, and
\(d&lt;1+r\), our best strategy is to exchange all our assets for cash at time \(t=0\). This gives us
</p>
<p>
\[x+ys\]
</p>
<p>
in cash at time \(0\). At time \(1\) we would then have \((1+r)(x+ys)\) in cash at time \(0\) and could pay our debt providing that
</p>
<p>
\[(1+r)(x+ys)\geq K.\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-pricing.html#ps:simple_arbitrage"><b>1.3</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> We borrow \(s\) cash at time \(t=0\) and use it to buy one stock. Then wait until time \(t=1\), at
which point we will a stock worth \(S_1\), where \(S_1\) is as in <a href="Exercises-on-Chapter-ref-chap-pricing.html#ps:portfolio"><b>1.1</b></a>. We sell our stock, which gives us \(S_1\) in cash.
Since \(S_1\geq sd&gt;s(1+r)\), we repay or debt (plus interest) which costs us \(s(1+r)\). We then have
</p>
<p>
\[S_1-s(1+r)&gt;0\]
</p>
<p>
in cash. This is an arbitrage.
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> We perform the same strategy as in (a), but with the roles of cash an stock swapped: We borrow \(1\)
stock at time \(t=0\) and then sell it for \(s\) in cash. Then wait until time \(t=1\), at which point we will have \(s(1+r)\) in cash. Since the price of a stock is now \(S_1\), where \(S_1\) is as in <a
href="Exercises-on-Chapter-ref-chap-pricing.html#ps:portfolio"><b>1.1</b></a>, and \(S_1\leq us&lt;(1+r)s\), we now buy one stock costing \(S_1\) and repay the stockbroker, leaving us with
</p>
<p>
\[s(1+r)-S_1&gt;0\]
</p>
<p>
in cash. This is an arbitrage.
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-pricing.html#ps:exp"><b>1.4</b></a> We can calculate, using integration by parts, that
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{0}\)</span>


<!--

                                                                                              Z   ∞                        Z    ∞                       1
                                                                                     E[X] =           xfX (x) dx =                  xλe−λx dx =           .
                                                                                               −∞                           0                           λ


-->


<p>


\begin{align*}
\E [X]=\int _{-\infty }^\infty x f_X(x)\,dx=\int _0^\infty x\lambda e^{-\lambda x}\,dx=\frac {1}{\lambda }.
\end{align*}
Similarly, we can calculate that
</p>
<p>
\[ \E [X^2]=\int _{-\infty }^\infty x^2 f_X(x)\,dx=\int _0^\infty x^2\lambda e^{-\lambda x}\,dx=\frac {2}{\lambda ^2} \]
</p>
<p>
This gives that
</p>
<p>
\[\var (X)=\E [X^2]-\E [X]^2=\frac {1}{\lambda ^2}.\]
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-pricing.html#ps:conv_P_intro"><b>1.5</b></a> We can calculate
</p>
<p>
\[\E [X_n]=0\cdot \P [X_n=0]+n^2\P [X_n=n^2]=\frac {n^2}{n}=n\to \infty \]
</p>
<p>
as \(n\to \infty \). Also,
</p>
<p>
\[\P [|X_n|&gt;0]=\P [X_n=n^2]=\frac {1}{n}\to 0\]
</p>
<p>
as \(n\to \infty \).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-pricing.html#ps:normal_standardization"><b>1.6</b></a> We have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{0}\)</span>


<!--


                                                                                                      X −µ
                                                                                                                                   
                                                                                         P[Y ≤ y] = P      ≤y
                                                                                                        σ
                                                                                                      = P[X ≤ µ + yσ]
                                                                                                          Z   µ+yσ         1             (x−µ)2
                                                                                                      =                √            e−     2σ 2   dx.
                                                                                                          −∞               2πσ 2


-->


<p>


\begin{align*}
\P [Y\leq y] &amp;=\P \l [\frac {X-\mu }{\sigma }\leq y\r ]\\ &amp;=\P [X\leq \mu +y\sigma ]\\ &amp;=\int _{-\infty }^{\mu +y\sigma }\frac {1}{\sqrt {2\pi \sigma ^2}}e^{-\frac
{(x-\mu )^2}{2\sigma ^2}}\,dx.
\end{align*}
We want to turn the above integral into the distribution function of the standard normal. To do so, substitute \(z=\frac {x-\mu }{\sigma }\), and we obtain
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{0}\)</span>


<!--

                                                                                                              Z   y       1      z2
                                                                                            P[Y ≤ y] =                 √      e− 2 σ dz
                                                                                                                  −∞    2πσ 2
                                                                                                              Z    y    1     z2
                                                                                                          =            √ e− 2 dz.
                                                                                                                  −∞    2π


-->


<p>


\begin{align*}
\P [Y\leq y]&amp;=\int _{-\infty }^y\frac {1}{\sqrt {2\pi \sigma ^2}}e^{-\frac {z^2}{2}}\sigma \,dz\\ &amp;=\int _{-\infty }^y \frac {1}{\sqrt {2\pi }}e^{-\frac {z^2}{2}}\,dz.
\end{align*}
Hence, \(Y\) has the distribution function of \(N(0,1)\), which means \(Y\sim N(0,1)\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-pricing.html#ps:tail_p"><b>1.7</b></a> For \(p\neq 1\),
</p>
<p>
\[\int _1^\infty x^{-p}\,dx = \l [\frac {x^{-p+1}}{-p+1}\r ]_{x=1}^\infty \]
</p>
<p>
and we obtain a finite answer only if \(-p+1&lt;0\). When \(p=-1\), we have
</p>
<p>
\[\int _1^\infty x^{-1}\,dx = \l [\log x\r ]_{x=1}^\infty =\infty ,\]
</p>
<p>
so we conclude that \(\int _1^\infty x^{-p}\,dx\) is finite if and only if \(p&gt;1\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-pricing.html#ps:seqs"><b>1.8</b></a> As \(n\to \infty \),
</p>
<ul style="list-style-type:none">


<li>
<p>
• \(e^{-n}\to 0\) because \(e&gt;1\).
</p>


</li>
<li>
<p>
• \(\sin \l (\frac {n\pi }{2}\r )\) oscillates through \(0,1,0,-1\) and does not converge.
</p>


</li>
<li>
<p>
• \(\frac {\cos (n\pi )}{n}\) converges to zero by the sandwich rule since \(|\frac {\cos (n\pi )}{n}|\leq \frac {1}{n}\to 0\).
</p>


</li>
<li>
<p>
• \(\sum \limits _{i=1}^n 2^{-i}=1-2^{-n}\) is a geometric series, and \(1-2^{-n}\to 1-0=1\).
</p>


</li>
<li>
<p>
• \(\sum \limits _{i=1}^n\frac {1}{i}\) tends to infinity, because
</p>
<p>
\[\overbrace {\frac 12}+\overbrace {\frac 13+\frac 14}+\overbrace {\frac 15+\frac 16+\frac 17+\frac 18}+\overbrace {\frac 19+\frac {1}{10}++\frac {1}{11}+\frac {1}{12}+\frac
{1}{13}+\frac {1}{14}+\frac {1}{15}+\frac {1}{16}}+\ldots .\]
</p>
<p>
each term contained in a \(\overbrace {\cdot }\) is greater than or equal to \(\frac 12\).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-pricing.html#ps:subseq"><b>1.9</b></a> From the previous question we have \(\sum _{n=1}^\infty n^{-2}&lt;\infty \). Define \(n_1=1\) and then
</p>
<p>
\[n_{r+1}=\inf \{k\-n&gt;n_r\text { and }|x_k|&lt;(r+1)^{-2}\}.\]
</p>
<p>
Then for all \(r\) we have \(|x_{n_r}|\leq r^{-2}\). Hence,
</p>
<p>
\[\sum \limits _{r=1}^\infty \l |x_{n_r}\r |\leq \sum \limits _{r=1}^\infty r^{-2}&lt;\infty .\]
</p>
<p>


</p>
</li>
</ul>
<h5 id="autosec-267">Chapter <a href="Probability-spaces-random-variables.html#chap:prob_meas">2</a></h5>
<a id="notes_1-autopage-267"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-prob_meas.html#ps:two_dice_sample"><b>2.1</b></a> The result of rolling a pair of dice can be written \(\omega =(\omega _1,\omega _2)\) where \(\omega
_1,\omega _2\in \{1,2,3,4,5,6\}\). So a suitable \(\Omega \) would be
</p>
<p>
\[\Omega =\Big \{(\omega _1,\omega _2)\-\omega _1,\omega _2\in \{1,2,3,4,5,6\}\Big \}.\]
</p>
<p>
Of course other choices are possible too. Since our choice of \(\Omega \) is finite, a suitable \(\sigma \)-field is \(\mc {F}=\mc {P}(\Omega )\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-prob_meas.html#ps:sigma_meas"><b>2.2</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> Consider the case of \(\mc {F}\). We need to check the three properties in the definition of a \(\sigma
\)-field.
</p>
<ul style="list-style-type:none">


<li>
<p>
1. We have \(\emptyset \in \mc {F}\) so the first property holds automatically.
</p>


</li>
<li>
<p>
2. For the second we check compliments: \(\Omega \sc \Omega =\emptyset \), \(\Omega \sc \{1\}=\{2,3\}\), \(\Omega \sc \{2,3\}=\{1\}\) and \(\Omega \sc \Omega =\emptyset \); in all cases we
obtain an element of \(\mc {F}\).
</p>


</li>
<li>
<p>
3. For the third, we check unions. We have \(\{1\}\cup \{2,3\}=\Omega \). Including \(\emptyset \) into a union doesn’t change anything; including \(\Omega \) into a union results in \(\Omega \). This
covers all possible cases, in each case resulting in an element of \(\mc {F}\).
</p>
</li>
</ul>
<p>
So, \(\mc {F}\) is a \(\sigma \)-field. Since \(\mc {F}’\) is just \(\mc {F}\) with the roles of \(1\) and \(2\) swapped, by symmetry \(\mc {F}’\) is also a \(\sigma \)-field.
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> We have \(\{1\}\in \mc {F}\cup \mc {F}’\) and \(\{2\}\in \mc {F}\cup \mc {F}’\), but
\(\{1\}\cup \{2\}=\{1,2\}\) and \(\{1,2\}\notin \mc {F}\cup \mc {F}’\). Hence \(\mc {F}\cup \mc {F}’\) is not closed under unions; it fails property 3 of the definition of a \(\sigma \)-field.
</p>
<p>
However, \(\mc {F}\cap \mc {F}’\) is the intersection of two \(\sigma \)-fields, so is automatically itself a \(\sigma \)-field. (Alternatively, note that \(\mc {F}\cap \mc {F}’=\{\emptyset ,\Omega \}\),
and check the definition.)
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-prob_meas.html#ps:inf_Om"><b>2.3</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> Let \(A_m\) be the event that the sequence \((X_n)\) contains precisely \(m\) heads. Let \(A_{m,k}\) be
the event that we see precisely \(m\) heads during the first \(k\) tosses and, from then on, only tails. Then,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{0}\)</span>


<!--



                                                                           P[Am,k ] = P[m heads in X1 , . . . , Xk , no heads in Xk+1 , Xk+2 , . . .]

                                                                                    = P[m heads in X1 , . . . , Xk , ] P[ no heads in Xk+1 , Xk+2 , . . .]

                                                                                    = P[m heads in X1 , . . . , Xk , ] × 0

                                                                                    = 0.



-->


<p>


\begin{align*}
\P [A_{m,k}] &amp;=\P [m\text { heads in }X_1,\ldots ,X_k,\text { no heads in }X_{k+1},X_{k+2},\ldots ]\\ &amp;=\P [m\text { heads in }X_1,\ldots ,X_k,]\,\P [\text { no heads in
}X_{k+1},X_{k+2},\ldots ]\\ &amp;=\P [m\text { heads in }X_1,\ldots ,X_k,]\times 0\\ &amp;=0.
\end{align*}
If \(A_m\) occurs then precisely one of \(A_{m,1},A_{m,2},\ldots \) occurs. Hence,
</p>
<p>
\[\P [A_m]=\sum \limits _{k=0}^\infty \P [A_{m,k}]=0\]
</p>
<p>
.
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> Let \(A\) be the event that the sequence \((X_n)\) contains finitely many heads. Then, if \(A\) occurs,
precisely one of \(A_1,A_2,\ldots \) occurs. Hence,
</p>
<p>
\[\P [A]=\sum \limits _{m=0}^\infty \P [A_m]=0.\]
</p>
<p>
That is, the probability of having only finite many heads is zero. Hence, almost surely, the sequence \((X_n)\) contains infinitely many heads.
</p>
<p>
By symmetry (exchange the roles of heads and tails) almost surely the sequence \((X_n)\) also contains infinitely many tails.
</p>
</li>
</ul>
</li>
</ul>

<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-prob_meas.html#ps:rv_meas_1"><b>2.4</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> \(\F _1\) contains the information of whether a \(6\) was rolled.
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> \(\F _2\) gives the information of which value was rolled if a \(1\), \(2\) or \(3\) was rolled, but if the
roll was in \(\{4,5,6\}\) then \(\F _1\) cannot tell the difference between these values.
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(c)</span></span></span> \(\F _3\) contains the information of which of the sets \(\{1,2\}\), \(\{3,4\}\) and \(\{5,6\}\)
contains the value rolled, but gives no more information than that.
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-prob_meas.html#ps:rv_meas_2"><b>2.5</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> The values taken by \(X_1\) are
</p>
<p>
\[X_1(\omega )= \begin {cases} 0 &amp; \text { if }\omega =3,\\ 1 &amp; \text { if }\omega =2,4,\\ 4 &amp; \text { if }\omega =1,5.\\ \end {cases} \]
</p>
<p>
Hence, the pre-images are \(X_1^{-1}(0)=\{3\}\), \(X_1^{-1}(1)=\{2,4\}\) and \(X_1^{-1}(4)=\{1,5\}\). These are all elements of \(\G _1\), so \(X_1\in m\G _1\). However, they are not all elements of
\(\G _2\) (in fact, none of them are!) so \(X_1\notin m\G _2\).
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> Define
</p>
<p>
\[X_2(\omega )= \begin {cases} 0 &amp; \text { if }\omega =1,2,\\ 1 &amp; \text { otherwise}.                               \end {cases} \]
</p>
<p>
Then the pre-images are \(X_2^{-1}(0)=\{1,2\}\) and \(X_2^{-1}(1)=\{3,4,5\}\). Hence \(X_2\in m\G _2\).
</p>
<p>
It isn’t possible to use unions/intersections/complements to make the set \(\{3,4,5\}\) out of \(\{1,5\},\{2,4\},\{3\}\) (one way to see this is note that as soon as we tried to include \(5\) we would also have
to include \(1\), meaning that we can’t make \(\{3,4,5\}\)). Hence \(X_2\notin m\G _1\).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-prob_meas.html#ps:two_coin_toss_sigma"><b>2.6</b></a> We have \(X(TT)=0\), \(X(HT)=X(TH)=1\) and \(X(HH)=2\). This gives
</p>
<p>
\[\sigma (X)=\Big \{\emptyset , \{TT\}, \{TH, HT\}, \{HH\}, \{TT, TH, HT\}, \{TT, HH\}, \{TH, HT, HH\}, \Omega \Big \}.\]
</p>
<p>
To work this out, you could note that \(X^{-1}(i)\) must be in \(\sigma (X)\) for \(i=0,1,2\), then also include \(\emptyset \) and \(\Omega \), then keep adding unions and complements of the events until
find you have added them all. (Of course, this only works because \(\sigma (X)\) is finite!)
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-prob_meas.html#ps:rv_gen"><b>2.7</b></a> Since sums, divisions and products of random variables are random variables, \(X^2+1\) is a random variable. Since
\(X^2+1\) is non-zero, we have that \(\frac {1}{X^2+1}\) is a random variable, and using products again, \(\frac {X}{X^2+1}\) is a random variable.
</p>
<p>
For \(\sin (X)\), recall that for any \(x\in \R \) we have
</p>
<p>
\[\sin (x)=\sum _{n=0}^\infty \frac {(-1)^n}{(2n+1)!}x^{2n+1}.\]
</p>
<p>
Since \(X\) is a random variable so is \(\frac {(-1)^n}{(2n+1)!}X^{2n+1}\). Since limits of random variables are also random variables, so is
</p>
<p>
\[\sin X=\lim \limits _{N\to \infty }\sum _{n=0}^N\frac {(-1)^n}{(2n+1)!}X^{2n+1}.\]
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-prob_meas.html#ps:not_int_rv"><b>2.8</b></a> Since \(X\geq 0\) we have \(\E [X^p]=\E [|X|^p]\) for all \(p\).
</p>
<p>
\[\E [X]=\int _1^\infty x^12x^{-3}\,dx=2\int _1^\infty x^{-2}\,dx=2\l [\frac {x^{-1}}{-1}\r ]_1^\infty =2&lt;\infty \]
</p>
<p>
so \(X\in L^1\), but
</p>
<p>
\[\E [X^2]=\int _1^\infty x^22x^{-3}\,dx=2\int _1^\infty x^{-1}\,dx=2\l [\log x\r ]_1^\infty =\infty \]
</p>
<p>
so \(X\notin L^2\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-prob_meas.html#ps:Lpq"><b>2.9</b></a> Let \(1\leq p\leq q&lt;\infty \) and \(X\in L^q\). Our plan is to generalize the argument that we used to prove
<span class="textup" >(<a href="Expectation.html#eq:L1L2">2.7</a>)</span>.
</p>
<p>
First, we condition to see that
</p>
<p>
\[|X|^p=|X|^p\1\{|X|\leq 1\}+|X|^p\1\{|X|&gt;1\}.\]
</p>
<p>
Hence,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{0}\)</span>


<!--



                                                                                      E[|X|p ] = E [|X|p 1{|X| ≤ 1} + |X|p 1{|X| > 1}]

                                                                                               ≤ E [1 + |X|q ]

                                                                                               = 1 + E|X|q ] < ∞



-->


<p>


\begin{align*}
\E [|X|^p] &amp;=\E \l [|X|^p\1\{|X|\leq 1\}+|X|^p\1\{|X|&gt;1\}\r ]\\ &amp;\leq \E \l [1+|X|^q\r ]\\ &amp;=1+\E |X|^q]&lt;\infty
\end{align*}
Here we use that, if \(|X|\leq 1\) then \(|X|^p\leq 1\), and if \(|X|&gt;1\) then since \(p\leq q\) we have \(|X|^p&lt;|X|^q\). So \(X\in L^p\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-prob_meas.html#ps:markovs_ineq"><b>2.10</b></a> First, we condition to see that
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{0}\)</span>
<!--


                                                X = X 1{X<a} + X 1{X≥a} .                                                                (A.1)                            --><a id="eq:markovs_pre"></a><!--


-->
<p>


\begin{equation}
\label {eq:markovs_pre} X=X\1_{\{X&lt; a\}}+X\1_{\{X\geq a\}}.
\end{equation}


</p>
<p>
From <span class="textup" >(<a href="Solutions-exercises-part-one.html#eq:markovs_pre">A.1</a>)</span> since \(X\geq 0\) we have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                                        X ≥ X 1{X≥a}

                                                                                                           ≥ a1{X ≥ a}.



-->


<p>


\begin{align*}
X &amp;\geq X\1_{\{X\geq a\}}\\ &amp;\geq a\1\{X\geq a\}.
\end{align*}
This second inequality follows by looking at two cases: if \(X&lt;a\) then both sides are zero, but if \(X\geq a\) then we can use that \(X\geq a\). Using monotonicity of \(\E \), we then have
</p>
<p>
\[\E [X]\geq \E [a\1_{\{X\geq a\}}]=a\E [1_{\{X\geq a\}}]=a\P [X\geq a].\]
</p>
<p>
Dividing through by \(a\) finishes the proof.
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-prob_meas.html#ps:exp_zero"><b>2.11</b></a> Since \(X\geq 0\), Markov’s inequality gives us that
</p>
<p>
\[\P [X\geq a]\leq \frac {1}{a}\E [X]=0\]
</p>
<p>
for any \(a&gt;0\). Hence,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                      P[X > 0] = P[X > 1] + P[1 ≥ X > 0]
                                                                                                                        ∞ n
                                                                                                                    "                                        #
                                                                                                                        [                                   o
                                                                                                                                    1               1
                                                                                                 = P[X > 1] + P                     n   ≥X>        n+1
                                                                                                                     n=1
                                                                                                                  ∞
                                                                                                                  X         h                           i
                                                                                                                                1                  1
                                                                                                 = P[X > 1] +           P       n   ≥X>           n+1
                                                                                                                 n=1
                                                                                                                 X∞         h                 i
                                                                                                                                         1
                                                                                                 ≤ P[X ≥ 1] +           P X>            n+1
                                                                                                                  n=1

                                                                                                 = 0.



-->


<p>


\begin{align*}
\P [X&gt;0]&amp;=\P [X&gt;1]+\P [1\geq X&gt;0]\\ &amp;=\P [X&gt;1]+\P \l [\bigcup \limits _{n=1}^\infty \l \{\tfrac {1}{n}\geq X&gt;\tfrac {1}{n+1}\r \}\r ]\\ &amp;=\P
[X&gt;1]+\sum \limits _{n=1}^\infty \P \l [\tfrac {1}{n}\geq X&gt;\tfrac {1}{n+1}\r ]\\ &amp;\leq \P [X\geq 1]+\sum \limits _{n=1}^\infty \P \l [X&gt;\tfrac {1}{n+1}\r ]\\
&amp;=0.
\end{align*}


</p>
</li>
</ul>
<h5 id="autosec-268">Chapter <a href="Conditional-expectation-martingales.html#chap:cond_exp">3</a></h5>
<a id="notes_1-autopage-268"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-cond_exp.html#ps:cond_exp_calc"><b>3.1</b></a> We have \(S_2=X_1+X_2\) so
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                             E[S2 | σ(X1 )] = E[X1 | σ(X1 )] + E[X2 | σ(X1 )]

                                                                                                             = X1 + E[X2 ]

                                                                                                             = X1 .



-->


<p>


\begin{align*}
\E [S_2\|\sigma (X_1)] &amp;=\E [X_1\|\sigma (X_1)]+\E [X_2\|\sigma (X_1)]\\ &amp;=X_1+\E [X_2]\\ &amp;=X_1.
\end{align*}
Here, we use the linearity of conditional expectation, followed by the fact that \(X_1\) is \(\sigma (X_1)\) measurable and \(X_2\) is independent of \(\sigma (X_1)\) with \(\E [X_2]=0\).
</p>
<p>
Secondly, \(S_2^2=X_1^2+2X_1X_2+X_2^2\) so
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                  E[S22 ] = E[X12 | σ(X1 )] + 2E[X1 X2 | σ(X1 )] + E[X22 | σ(X1 )]

                                                                                             = X12 + 2X1 E[X2 | σ(X1 )] + E[X22 ]

                                                                                             = X12 + 1



-->


<p>


\begin{align*}
\E [S_2^2] &amp;=\E [X_1^2\|\sigma (X_1)]+2\E [X_1X_2\|\sigma (X_1)]+\E [X_2^2\|\sigma (X_1)]\\ &amp;=X_1^2+2X_1\E [X_2\|\sigma (X_1)]+\E [X_2^2]\\ &amp;=X_1^2+1
\end{align*}
Here, we again use linearity of conditional expectation to deduce the first line. To deduce the second line, we use that \(X_1^2\) and \(X_1\) are \(\sigma (X_1)\) measurable (using the taking out what is known
rule for the middle term), whereas \(X_2\) is independent of \(\sigma (X_1)\). The final line comes from \(\E [X_2\|\sigma (X_1)]=0\) (which we already knew from above) and that \(\E [X_2^2]=1\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-cond_exp.html#ps:biased_walk_mart"><b>3.2</b></a> For \(i\leq n\), we have \(X_i\in m\mc {F}_n\), so \(S_n=X_1+X_2+\ldots +X_n\) is also \(\mc
{F}_n\) measurable. For each \(i\) we have \(|X_i|\leq 2\) so \(|S_n|\leq 2n\), hence \(S_n\in L^1\). Lastly,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                       E[Sn+1 | Fn ] = E[X1 + . . . + Xn | Fn ] + E[Xn+1 | Fn ]

                                                                                                         = (X1 + . . . + Xn ) + E[Xn+1 ]

                                                                                                         = Sn .



-->


<p>


\begin{align*}
\E [S_{n+1}\|\mc {F}_n] &amp;=\E [X_1+\ldots +X_n\|\mc {F}_n]+\E [X_{n+1}\|\mc {F}_n]\\ &amp;=(X_1+\ldots +X_n) + \E [X_{n+1}]\\ &amp;=S_n.
\end{align*}
Here, we use linearity of conditional expectation in the first line. To deduce the second, we use that \(X_i\in m\mc {F}_n\) for \(i\leq n\) and that \(X_{n+1}\) is independent of \(\mc {F_n}\) in the second.
For the final line we note that \(\E [X_{n+1}]=\frac 132+\frac 23(-1)=0\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-cond_exp.html#ps:mart_exs"><b>3.3</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> Since \(X_i\in m\mc {F}_n\) for all \(i\leq n\), we have \(M_n=X_1X_2\ldots X_n\in m\mc {F}_n\).
Since \(|X_i|\leq c\) for all \(i\), we have \(|M_n|\leq c^n&lt;\infty \), so \(M_n\) is bounded and hence \(M_n\in L^1\) for all \(n\). Lastly,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                               E[Mn+1 | Fn ] = E[X1 X2 . . . Xn Xn+1 | Fn ]

                                                                                                                  = X1 . . . Xn E[Xn+1 | Fn ]

                                                                                                                  = X1 . . . Xn E[Xn+1 ]

                                                                                                                  = X1 . . . Xn

                                                                                                                  = Mn .



-->


<p>


\begin{align*}
\E [M_{n+1}\|\mc {F}_n] &amp;=\E [X_1X_2\ldots X_nX_{n+1}\|\mc {F}_n]\\ &amp;=X_1\ldots X_n\E [X_{n+1}\|\mc {F}_n]\\ &amp;=X_1\ldots X_n\E [X_{n+1}]\\ &amp;=X_1\ldots X_n\\
&amp;=M_n.
\end{align*}
Here, to deduce the second line we use that \(X_i\in m\mc {F}_n\) for \(i\leq n\). To deduce the third line we use that \(X_{n+1}\) is independent of \(\mc {F}_n\) and then to deduce the forth line we use
that \(\E [X_{n+1}]=1\).
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> By definition of conditional expectation (Theorem <a
href="Conditional-expectation-martingales.html#thm:cond_exp">3.1.1</a>), we have \(M_n\in L^1\) and \(M_n\in m\mc {G}_n\) for all \(n\). It remains only to check that
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                                   E[Mn+1 | Gn ] = E[E[Z | Gn+1 ] | Gn ]

                                                                                                                     = E[Z | Gn ]

                                                                                                                     = Mn .



-->


<p>


\begin{align*}
\E [M_{n+1}\|\mc {G}_n] &amp;=\E [\E [Z\|\mc {G}_{n+1}]\|\mc {G}_n]\\ &amp;=\E [Z\|\mc {G}_n]\\ &amp;=M_n.
\end{align*}
Here, to get from the first to the second line we use the tower property.
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-cond_exp.html#ps:subsupmartmart"><b>3.4</b></a> Note that the first and second conditions in Definition <a href="Martingales.html#martdef">3.3.3</a> are
the same for super/sub-martingales as for martingales. Hence, \(M\) satisfies these conditions. For the third condition, we have that \(\E [M_{n+1}\|\mc {F}_n]\leq M_n\) (because \(M\) is a supermartingale)
and \(\E [M_{n+1}\|\mc {F}_n]\geq M_n\) (because \(M\) is a submartingale. Hence \(\E [M_{n+1}\|\mc {F}_n]=M_n\), so \(M\) is a martingale.
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-cond_exp.html#ps:mart_alt_char"><b>3.5</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> If \(m=n\) then \(\E [M_n\|\mc {F}_n]=M_n\) because \(M_n\) is \(\mc {F}_n\) measurable. For
\(m&gt;n\), we have
</p>
<p>
\[\E [M_m|\mc {F}_n]=\E [\E [M_m\|\mc {F}_{m-1}]\|\mc {F}_n]=\E [M_{m-1}\|\mc {F}_n].\]
</p>
<p>
Here we use the tower property to deduce the first equality (note that \(m-1\geq n\) so \(\mc {F}_{m-1}\supseteq \mc {F}_n\)) and the fact that \((M_n)\) is a martingale to deduce the second inequality
(i.e.&nbsp;\(\E [M_{m}\|\mc {F}_{m-1}]=M_{m-1}\)). Iterating, from \(m,m-1\ldots ,n+1\) we obtain that
</p>
<p>
\[\E [M_m\|\mc {F}_n]=\E [M_{n+1}\|\mc {F}_n]\]
</p>
<p>
and the martingale property then gives that this is equal to \(M_n\), as required.
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> If \((M_n)\) is a submartingale then \(\E [M_m\|\mc {F}_n]\geq M_n\), whereas if \((M_n)\) is a
supermartingale then \(\E [M_m\|\mc {F}_n]\leq M_n\).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-cond_exp.html#ps:doob_pre"><b>3.6</b></a> We have \((M_{n+1}-M_n)^2=M_{n+1}^2-2M_{n+1}M_n+M_n^2\) so
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--


                                                                            h                         i
                                                                        E (Mn+1 − Mn )2 | Fn = E[Mn+1
                                                                                                  2
                                                                                                      | Fn ] − 2E[Mn+1 Mn | Fn ] + E[Mn2 | Fn ]
                                                                                                               2
                                                                                                          = E[Mn+1 | Fn ] − 2Mn E[Mn+1 | Fn ] + Mn2
                                                                                                               2
                                                                                                          = E[Mn+1 | Fn ] − 2Mn2 + Mn2
                                                                                                               2
                                                                                                          = E[Mn+1 | Fn ] − Mn2




-->


<p>


\begin{align*}
\E \l [(M_{n+1}-M_n)^2\|\mc {F}_n\r ] &amp;=\E [M_{n+1}^2\|\mc {F}_n]-2\E [M_{n+1}M_n\|\mc {F}_n]+\E [M_n^2\|\mc {F}_n]\\ &amp;=\E [M_{n+1}^2\|\mc {F}_n]-2M_n\E [M_{n+1}\|\mc
{F}_n]+M_n^2\\ &amp;=\E [M_{n+1}^2\|\mc {F}_n]-2M_n^2+M_n^2\\ &amp;=\E [M_{n+1}^2\|\mc {F}_n]-M_n^2\\
\end{align*}
as required. Here, we use the taking out what is known run (since \(M_n\in m\mc {F}_n\)) and the martingale property of \((M_n)\).
</p>
<p>
It follows that \(\E [M_{n+1}^2\|\mc {F}_n]-M_n^2=\E [(M_{n+1}-M_n)^2\|\mc {F}_n]\geq 0\). We have \(M_n^2\in m\mc {F}_n\) and since \(M_n\in L^2\) we have \(M_n^2\in L^1\). Hence
\((M_n^2)\) is a submartingale.
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-cond_exp.html#ps:mart_linear"><b>3.7</b></a> Using that \(\E [X_{n+1}\|\mc {F}_n]=aX_n+bX_{n-1}\) we can calculate
</p>
<p>
\[\E [S_{n+1}\|\mc {F}_n]=\E [\alpha X_{n+1}+X_{n}\|\mc {F}_n]=\alpha (aX_n+bX_{n-1})+X_n=(\alpha a+1)X_n+\alpha b X_{n-1}.\]
</p>
<p>
We want this to be equal to \(S_n\), and \(S_n=\alpha X_n+X_{n-1}\). So we need
</p>
<p>
\[\alpha a+1=\alpha \hspace {1pc}\text { and }\hspace {1pc}\alpha b = 1.\]
</p>
<p>
Hence, \(\alpha =\frac {1}{b}\) is our choice. It is then easy to check that
</p>
<p>
\[\E [S_{n+1}\|\mc {F}_n]=\tfrac {1}{b}(aX_n+bX_{n-1})+X_n=(\tfrac {a}{b}+1)X_n+X_{n-1}=\tfrac {1}{b}X_n+X_{n-1}=S_n\]
</p>
<p>
and thus \(S_n\) is a martingale, as required.
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-cond_exp.html#ps:cond_exp_2"><b>3.8</b></a> Note that
</p>
<p>
\[\E [X_1\|\sigma (S_n)]=\E [X_2\|\sigma (S_n)]=\ldots =\E [X_n\|\sigma (S_n)]\]
</p>
<p>
is constant, by symmetry (i.e.&nbsp;permuting the \(X_1,X_2,\ldots ,X_n\) does not change the distribution of \(S_n\)). Hence, if we set \(\E [X_i\|\sigma (S_n)]=\alpha \) then
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--

                                                                                       n
                                                                                                                   " n              #
                                                                                       X                            X
                                                                                nα =         E[Xi | σ(Sn )] = E            Xi σ(Sn ) = E[Sn | σ(Sn )] = Sn .
                                                                                       i=1                           i=1



-->


<p>


\begin{align*}
n\alpha =\sum \limits _{i=1}^n\E [X_i\|\sigma (S_n)]=\E \l [\sum \limits _{i=1}^n X_i\,\Big |\,\sigma (S_n)\r ]=\E [S_n\|\sigma (S_n)]=S_n.
\end{align*}
Here we use the linearity of conditional expectation and the fact that \(S_n\) is \(\sigma (S_n)\) measurable. Hence,
</p>
<p>
\[\alpha =\E [X_1\|\sigma (S_n)]=\frac {S_n}{n}.\]
</p>
<p>


</p>
</li>
</ul>
<h5 id="autosec-269">Chapter <a href="Stochastic-processes.html#chap:stoch_procs">4</a></h5>
<a id="notes_1-autopage-269"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs.html#ps:rw_exponential"><b>4.1</b></a> Since \(Z_n=e^{S_n}\) and \(S_n\in m\mc {F}_n\), also \(Z_n\in m\mc {F}_n\). Since \(|S_n|\leq
n\), we have \(|e^{S_n}|\leq e^n&lt;\infty \), so \(Z_n\) is bounded and hence \(Z_n\in L^1\). Hence also \(M_n\in m\mc {F}_n\) and \(M_n\in L^1\).
</p>
<p>
Lastly,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                                E[eSn+1 |Fn ] = E[eXn+1 eSn | Fn ]

                                                                                                                = eSn E[eXn+1 | Fn ]

                                                                                                                = eSn E[eXn+1 ]
                                                                                                                                            
                                                                                                                = eSn        1
                                                                                                                             2e   + 12 e−1
                                                                                                                         e + 1e
                                                                                                                = eSn           .
                                                                                                                           2


-->


<p>


\begin{align*}
\E [e^{S_{n+1}}|\mc {F}_n] &amp;= \E [e^{X_{n+1}}e^{S_n}\|\mc {F}_n] \\ &amp;= e^{S_n}\E [e^{X_{n+1}}\|\mc {F}_n] \\ &amp;= e^{S_n}\E [e^{X_{n+1}}] \\ &amp;= e^{S_n}\l (\tfrac 12
e+\tfrac 12 e^{-1}\r )\\ &amp;= e^{S_n}\frac {e+\frac {1}{e}}{2}.
\end{align*}
Here, we use the taking out what is known rule and the fact that \(X_{n+1}\) (and hence also \(e^{X_{n+1}}\)) is independent of \(\mc {F}_n\). We also calculate the expectation of the discrete random variable
\(X_{n+1}\), using that \(\P [X_{n+1}=1]=\P [X_{n+1}=-1]=\frac 12\). This gives us that
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                                       E[Mn+1 | Fn ] = Mn

                                                                                                        E[Zn+1 | Fn ] ≥ Zn



-->


<p>


\begin{align*}
\E [M_{n+1}\|\mc {F}_n]&amp;= M_n\\ \E [Z_{n+1}\|\mc {F}_n]&amp;\geq Z_n
\end{align*}
where to deduce the last line we use that \(\frac {e+\frac {1}{e}}{2}&gt;1\). Thus \(M_n\) is a martingale and \(Z_n\) is a submartingale.
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs.html#ps:asym_rw_qpmart"><b>4.2</b></a> Since \(X_i\in m\mc {F}_n\) for all \(i\leq n\), we have \(M_n\in m\mc {F}_n\). We have
\(|X_i|\leq 1\) so \(|S_n|\leq n\) and \(|M_n|\leq \max \{(q/p)^n, (q/p)^{-n}\}=(q/p)^{-n}\) (note \(p&gt;q\)), which implies that \(S_n\in L^1\) and \(M_n\in L^1\) for all \(n\).
</p>
<p>
We have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                               E[Sn+1 | Fn ] = E[Sn + Xn+1 | Fn ]

                                                                                                                = Sn + E[Xn+1 | Fn ]

                                                                                                                = Sn + E[Xn+1 ]

                                                                                                                > Sn .



-->


<p>


\begin{align*}
\E [S_{n+1}\|\mc {F}_n] &amp;=\E [S_{n}+X_{n+1}\|\mc {F}_n]\\ &amp;=S_n+\E [X_{n+1}\|\mc {F}_n]\\ &amp;=S_n+\E [X_{n+1}]\\ &amp;&gt;S_n.
\end{align*}
Here we use the taking out what is known rule, followed by the fact that \(X_{n+1}\) is independent of \(\mc {F}_{n}\) and the relationship between conditional expectation and independence. To deduce the final
step we use that \(\E [X_{n+1}]=p(1)+q(-1)=p-q&gt;0\). Therefore, we have \(\E [S_{n+1}\|\mc {F}_n]\geq S_n\), which means that \(S_n\) is a submartingale.
</p>
<p>
We now look at \(M_n\). We have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                            E[Mn+1 |Fn ] = (q/p)Sn E[(q/p)Xn+1 |Fn ]

                                                                                                             = (q/p)Sn E[(q/p)Xn+1 ]

                                                                                                             = (q/p)Sn = Mn .



-->


<p>


\begin{align*}
\E [M_{n+1}|\mc {F}_n]&amp;=(q/p)^{S_n}\E [(q/p)^{X_{n+1}}|\mc {F}_n]\\ &amp;=(q/p)^{S_n}\E [(q/p)^{X_{n+1}}]\\ &amp;=(q/p)^{S_n}=M_n.
\end{align*}
Here we use the taking out what is known rule, followed by the fact that \(X_{n+1}\) is independent of \(\mc {F}_{n}\) and the relationship between conditional expectation and independence. To deduce the final
step we use that
</p>
<p>
\[\E \l [(q/p)^{X_{n+1}}\r ]=p(q/p)^1+q(q/p)^{-1}=p+q=1.\]
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs.html#ps:ex_biased_mart"><b>4.3</b></a> Since \(X_i\in \mc {F}_n\) for all \(i\leq n\) we have \(S_n\in m\mc {F}_n\) where \(\mc
{F}_n=\sigma (X_1,\ldots , X_n)\). Further, if we set \(m=\max (|a|,|b|)\) then \(|S_n|\leq nm\) so \(S_n\in L^1\).
</p>
<p>
We have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                               E[Sn+1 | Fn ] = E[Sn + Xn+1 | Fn ]

                                                                                                                = Sn + E[Xn+1 | Fn ]

                                                                                                                = Sn + E[Xn+1 ]

                                                                                                                = Sn + apa − bpb



-->


<p>


\begin{align*}
\E [S_{n+1}\|\mc {F}_n] &amp;=\E [S_n+X_{n+1}\|\mc {F}_n]\\ &amp;=S_n+\E [X_{n+1}\|\mc {F}_n]\\ &amp;=S_n+\E [X_{n+1}]\\ &amp;=S_n+ap_a-bp_b
\end{align*}
where \(p_b=1-p_a\). Therefore, \(S_n\) is a martingale if and only if \(ap_a=bp_b\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs.html#ps:rw_local_time_mart"><b>4.4</b></a> The natural filtration is \(\mc {F}_n=\sigma (X_1,\ldots ,X_n)\). We can write
</p>
<p>
\[S_{n+1}=\1\{S_n=0\}+\1\{S_n\neq 0\}(S_n+X_{n+1}).\]
</p>
<p>
Hence, if \(S_n\in m\mc {F}_n\) then \(S_{n+1}\in m\mc {F}_{n+1}\). Since \(S_1=X_1\in m\mc {F}_n\), a trivial induction shows that \(S_n\in m\mc {F}_n\) for all \(n\). hence, \(L_n\in m\mc {F}_n\)
and also \(S_n+L_n\in m\mc {F}_n\).
</p>
<p>
We have \(-n\leq S_n-L_n \leq n+1\) (because the walk is reflected at zero and can increase by at most \(1\) in each time step) so \(S_{n}-L_n\) is bounded and hence \(S_n-L_n\in L^1\).
</p>
<p>
Lastly,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                E[Sn+1 |Fn ] = E[1{Sn = 0} + 1{Sn 6= 0}(Sn + Xn+1 ) | Fn ]

                                                                                              = 1{Sn = 0} + 1{Sn 6= 0}(Sn + E [Xn+1 |Fn ])

                                                                                              = 1{Sn = 0} + 1{Sn 6= 0}Sn

                                                                                              = 1{Sn = 0} + Sn .                   (A.2)                                     --><a id="eq:SnLnpre"></a><!--



-->


<p>


\begin{align}
\E [S_{n+1}|\mc {F}_n] &amp;=\E [\1\{S_n=0\}+\1\{S_n\neq 0\}(S_n+X_{n+1})\|\mc {F}_n]\notag \\ &amp;=\1\{S_n=0\}+\1\{S_n\neq 0\}(S_n+\E \l [X_{n+1}|\mc {F}_n\r ])\notag \\
&amp;=\1\{S_n=0\}+\1\{S_n\neq 0\}S_n\notag \\ &amp;=\1\{S_n=0\}+S_n.\label {eq:SnLnpre}
\end{align}
Here we use linearity and the taking out what is known \((S_n\in m\mc {F}_n)\) rule, as well as that \(X_{n+1}\) is independent of \(\mc {F}_n\). Hence,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{2}\)</span>


<!--

                                                                                                                              n
                                                                                                                 "                                      #
                                                                                                                                    1{Si = 0} Fn
                                                                                                                              X
                                                                                  E[Sn+1 − Ln+1 | Fn ] = E Sn+1 −
                                                                                                                              i=0
                                                                                                                                             n
                                                                                                             = Sn + 1{Sn = 0} −                    1{Si = 0}
                                                                                                                                             X

                                                                                                                                             i=0
                                                                                                                      n−1
                                                                                                                             1{Si = 0}
                                                                                                                      X
                                                                                                             = Sn −
                                                                                                                      i=0

                                                                                                             = S n − Ln



-->


<p>


\begin{align*}
\E [S_{n+1}-L_{n+1}\|\mc {F}_n] &amp;=\E \l [S_{n+1}-\sum \limits _{i=0}^n \1\{S_i=0\}\,\Bigg |\,\mc {F}_n\r ]\\ &amp;=S_{n}+\1\{S_n=0\}-\sum \limits _{i=0}^n \1\{S_i=0\}\\
&amp;=S_n-\sum \limits _{i=0}^{n-1} \1\{S_i=0\}\\ &amp;=S_n-L_n
\end{align*}
Here we use <span class="textup" >(<a href="Solutions-exercises-part-one.html#eq:SnLnpre">A.2</a>)</span> to deduce the second line, as well as taking out what is known. Hence, \(S_n-L_n\) is a
martingale.
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs.html#ps:three_colour_urn"><b>4.5</b></a> Let \(B_n\) be number of red balls in the urn at time \(n\) (i.e.&nbsp;after the \(n^{th}\) draw is
complete). Then, the proportion of red balls in the urn just after the \(n^{th}\) step is
</p>
<p>
\[M_n=\frac {B_n}{n+3}.\]
</p>
<p>
Essentially the same argument as for the two colour urn of Section <a href="Urn-processes.html#sec:urn">4.2</a> shows that \(M_n\) is adapted to the natural filtration and also in \(L^1\). We have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{2}\)</span>


<!--



                                                               E[Mn+1 | Fn ] = E Mn+1 1{(n+1)th draw is red} + Mn+1 1{(n+1)th draw is not red} Fn
                                                                                      h                                                                           i


                                                                                   Bn + 1                         Bn
                                                                                                                                                                      
                                                                              =E          1     th             +     1    th                 Fn
                                                                                    n + 4 {(n+1) draw is red} n + 4 {(n+1) draw is not red}
                                                                                Bn + 1 h                              Bn
                                                                                       E 1{(n+1)th draw is red} Fn +     E 1{(n+1)th draw is not red} Fn
                                                                                                                  i       h                              i
                                                                              =
                                                                                 n+4                                 n+4
                                                                                Bn + 1 Bn       Bn             Bn
                                                                                                                  
                                                                              =              +          1−
                                                                                 n+4 n+3 n+4                  n+3
                                                                                  Bn (n + 4)
                                                                              =
                                                                                (n + 3)(n + 4)
                                                                              = Mn .



-->


<p>


\begin{align*}
\E [M_{n+1}\|\mc {F}_n] &amp;=\E \l [M_{n+1}\1_{\{(n+1)^{th}\text { draw is red}\}}+M_{n+1}\1_{\{(n+1)^{th}\text { draw is not red}\}}\,\Big |\,\mc {F}_n\r ]\\ &amp;=\E \l [\frac
{B_n+1}{n+4}\1_{\{(n+1)^{th}\text { draw is red}\}}+\frac {B_n}{n+4}\1_{\{(n+1)^{th}\text { draw is not red}\}}\,\Big |\,\mc {F}_n\r ]\\ &amp;=\frac {B_n+1}{n+4}\E \l
[\1_{\{(n+1)^{th}\text { draw is red}\}}\,\Big |\,\mc {F}_n\r ]+\frac {B_n}{n+4}\E \l [\1_{\{(n+1)^{th}\text { draw is not red}\}}\,\Big |\,\mc {F}_n\r ]\\ &amp;=\frac
{B_n+1}{n+4}\frac {B_n}{n+3}+\frac {B_n}{n+4}\l (1-\frac {B_n}{n+3}\r )\\ &amp;=\frac {B_n(n+4)}{(n+3)(n+4)}\\ &amp;=M_n.
\end{align*}
Hence \((M_n)\) is a martingale.
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs.html#ps:mart_test"><b>4.6</b></a> By the result of <a href="Exercises-on-Chapter-ref-chap-cond_exp.html#ps:doob_pre"><b>3.6</b></a>,
\(S_n^2\) is a submartingale.
</p>
<ul style="list-style-type:none">


<li>
<p>
(i) \(M_n=S_n^2+n\) is \(\mc {F}_n\) measurable (because \(S_n\in m\mc {F}_n\)) and since \(|S_n|\leq n\) we have \(|M_n|\leq n^2+n&lt;\infty \), so \(M_n\in L^1\). Further, using the submartingale
property of \(S_n^2\),
</p>
<p>
\[\E [S_{n+1}^2+n+1\|\mc {F}_n]=\E [S_{n+1}^2\|\mc {F}_n]+n+1\geq S_n^2+n\]
</p>
<p>
so \(M_n\) is a submartingale. But \(\E [M_1^2]=2\neq 0=\E [M_0^2]\), so (by Lemma <a href="Martingales.html#notmart">3.3.6</a>) we have that \((M_n)\) is not a martingale.
</p>


</li>
<li>
<p>
(ii) \(M_n=S_n^2-n\) has \(M_n\in L^1\) and \(M_n\in m\mc {F}_n\) for essentially the same reasons as we used in case (i). However,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{2}\)</span>


<!--


                                                                                   2
                                                                                E[Sn+1 | Fn ] = E[(Xn+1 + Sn )2 | Fn ]
                                                                                                    2
                                                                                               = E[Xn+1 | Fn ] + 2E[Xn+1 Sn | Fn ] + E[Sn2 | Fn ]

                                                                                               = 1 + 2Sn E[Xn+1 | Fn ] + E[Sn2 | Fn ]

                                                                                               = 1 + 2Sn E[Xn+1 ] + E[Sn2 | Fn ]

                                                                                               = 1 + E[Sn2 | Fn ]

                                                                                               = 1 + Sn2 .



-->


<p>


\begin{align*}
\E [S_{n+1}^2\|\mc {F}_n] &amp;=\E [(X_{n+1}+S_n)^2\|\mc {F}_n]\\ &amp;=\E [X_{n+1}^2\|\mc {F}_n]+2\E [X_{n+1}S_n\|\mc {F}_n]+\E [S_n^2\|\mc {F}_n]\\ &amp;=1+2S_n\E [X_{n+1}\|\mc
{F}_n]+\E [S_n^2\|\mc {F}_n]\\ &amp;=1+2S_n\E [X_{n+1}]+\E [S_n^2\|\mc {F}_n]\\ &amp;=1+\E [S_n^2\|\mc {F}_n]\\ &amp;=1+S_n^2.
\end{align*}
Here, we use the taking out what is known rule (since \(S_n\in m\mc {F}_n\)) along with the fact that \(X_{n+1}\) is independent of \(\mc {F}_n\) and \(\E [X_{n+1}]=0\). Therefore,
</p>
<p>
\[\E [S_{n+1}^2-(n+1)\|\mc {F}_n]=S_n^2-n\]
</p>
<p>
so as \((M_n)\) is a martingale, hence also a submartingale.
</p>


</li>
<li>
<p>
(iii) \(M_n=\frac {S_n}{n}\) is \(\mc {F}_n\) measurable (because \(S_n\in m\mc {F}_n\)) and since \(|S_n|\leq n\) we have \(|S_n|\leq 1\), so \(M_n\in L^1\). Since \(S_n\) is a martingale, we have
</p>
<p>
\[\E \l [\frac {S_{n+1}}{n+1}\,\Big {|}\,\mc {F}_n\r ]=\frac {1}{n+1}\E [S_{n+1}|\mc {F}_n]=\frac {S_n}{n+1}\neq \frac {S_n}{n}.\]
</p>
<p>
Hence \(M_n=\frac {S_n}{n}\) is not a martingale. Also, since \(S_n\) may be both positive or negative, we do not have \(\frac {S_n}{n+1}\geq \frac {S_n}{n}\), so \(S_n\) is also not a submartingale.
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs.html#ps:doob_S4"><b>4.7</b></a> We showed that \(S_n^2-n\) was a martingale in <a
href="Exercises-on-Chapter-ref-chap-stoch_procs.html#ps:mart_test"><b>4.6</b></a>. We try to mimic that calculation and find out what goes wrong in the cubic case. So, we look at
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{2}\)</span>


<!--


                                                                                              3
                                                                  E[(Sn+1 − Sn )3 | Fn ] = E[Sn+1                 2
                                                                                                  | Fn ] − 3Sn E[Sn+1 | Fn ] + 3Sn2 E[Sn+1 | Fn ] − E[Sn3 | Fn ]
                                                                                               3
                                                                                          = E[Sn+1 | Fn ] − 3Sn (Sn2 + 1) + 3Sn2 Sn − Sn3
                                                                                               3
                                                                                          = E[Sn+1 | Fn ] − Sn3 − 3Sn .



-->


<p>


\begin{align*}
\E [(S_{n+1}-S_n)^3\|\mc {F}_n] &amp;=\E [S_{n+1}^3\|\mc {F}_n]-3S_n\E [S_{n+1}^2\|\mc {F}_n]+3S_n^2\E [S_{n+1}\|\mc {F}_n]-\E [S_n^3\|\mc {F}_n]\\ &amp;=\E [S_{n+1}^3\|\mc
{F}_n]-3S_n(S_n^2+1)+3S_n^2S_n-S_n^3\\ &amp;=\E [S_{n+1}^3\|\mc {F}_n]-S_n^3-3S_n.
\end{align*}
Here we use linearity, taking out what is known and that fact that \(\E [S_{n+1}^2\|\mc {F}_n]=S_n^2+1\) (from <a
href="Exercises-on-Chapter-ref-chap-stoch_procs.html#ps:mart_test"><b>4.6</b></a>). However, also \(S_{n+1}-S_n=X_{n+1}\), so
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{2}\)</span>


<!--


                                                                                                                          3
                                                                                              E[(Sn+1 − Sn )3 | Fn ] = E[Xn+1 | Fn ]
                                                                                                                              3
                                                                                                                         = E[Xn+1 ]

                                                                                                                         = 0.



-->


<p>


\begin{align*}
\E [(S_{n+1}-S_n)^3\|\mc {F}_n] &amp;=\E [X_{n+1}^3\|\mc {F}_n]\\ &amp;=\E [X_{n+1}^3]\\ &amp;=0.
\end{align*}
because \(X_{n+1}\) is independent of \(\mc {F}_n\) and \(X_{n+1}^3=X_n\) (it takes values only \(1\) or \(-1\)) so \(\E [X_{n+1}^3]=0\). Putting these two calculations together, we have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{2}\)</span>
<!--


                                                           3
                                                        E[Sn+1 | Fn ] = Sn3 + 3Sn .                                                                            (A.3)        --><a id="eq:sn3_mart"></a><!--


-->
<p>


\begin{equation}
\label {eq:sn3_mart} \E [S_{n+1}^3\|\mc {F}_n]=S_n^3+3S_n.
\end{equation}


</p>
<p>
Suppose (aiming for a contradiction) that there is a deterministic function \(f:\N \to \R \) such that \(S_n^3-f(n)\) is a martingale. Then
</p>
<p>
\[\E [S_{n+1}^3-f(n+1)\|\mc {F}_n]=S_n^3-f(n).\]
</p>
<p>
Combining the above equation with <span class="textup" >(<a href="Solutions-exercises-part-one.html#eq:sn3_mart">A.3</a>)</span> gives
</p>
<p>
\[f(n+1)-f(n)=3S_n\]
</p>
<p>
but this is impossible because \(S_n\) is not deterministic. Thus we reach a contradiction; no such function \(f\) exists.
</p>
</li>
</ul>
<h5 id="autosec-270">Chapter <a href="The-binomial-model.html#chap:bin_model">5</a></h5>
<a id="notes_1-autopage-270"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-bin_model.html#ps:portfolio_recap"><b>5.1</b></a> The value of the portfolio \((1,3)\) at time \(1\) will be
</p>
<p>
\[1(1+r)+3S_1=(1+r)+3S_1\]
</p>
<p>
where \(S_1\) is a random variable with \(\P [S_1=su]=p_u\) and \(\P [S_1=sd]=p_d\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-bin_model.html#ps:contingent_1"><b>5.2</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> We need a portfolio \(h=(x,y)\) such that \(V^h_1=\Phi (S_1)=1\), so we need that
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{3}\)</span>


<!--



                                                                                                      x(1 + r) + ysu = 1

                                                                                                      x(1 + r) + ysd = 1.



-->


<p>


\begin{align*}
x(1+r)+ysu&amp;=1\\ x(1+r)+ysd&amp;=1.
\end{align*}
Solving this pair of linear equations gives \(x=\frac {1}{1+r}\) and \(y=0\). So our replicating portfolio consists simply of \(\frac {1}{1+r}\) cash and no stock.
</p>
<p>
Hence, the value of this contingent claim at time \(0\) is \(x+sy=\frac {1}{1+r}\).
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> Now we need that
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{3}\)</span>


<!--



                                                                                                      x(1 + r) + ysu = 3

                                                                                                      x(1 + r) + ysd = 1.



-->


<p>


\begin{align*}
x(1+r)+ysu=3\\ x(1+r)+ysd=1.
\end{align*}
Which has solution \(x=\frac {1}{1+r}\l (3-\frac {2su}{su-sd}\r )\) and \(y=\frac {2}{s(u-d)}\).
</p>
<p>
Hence, the value of this contingent claim at time \(0\) is \(x+sy=\frac {1}{1+r}\l (3-\frac {2su}{su-sd}\r )+\frac {2}{u-d}\).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-bin_model.html#ps:contingent_2"><b>5.3</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> If we buy two units of stock, at time \(1\), for a price \(K\) then our contingent claim is \(\Phi
(S_1)=2S_1-2K\).
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> In a European put option, with strike price \(K\in (sd,su)\), we have the option to sell a single unit of
stock for strike price \(K\). It is advantageous to us to do so only if \(K&gt;S_1\), so
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{3}\)</span>
<!--
                                                         
                                                         
                                                         0
                                                                   if S1 = su,
                                              Φ(S1 ) =                            = max(K − S1 , 0)                                        (A.4)                      --><a id="eq:eu_put_contingent"></a><!--
                                                         K − S1
                                                         
                                                                   if S1 = sd.

-->
<p>


\begin{equation}
\label {eq:eu_put_contingent} \Phi (S_1)=\begin{cases} 0 &amp; \text { if }S_1=su,\\ K-S_1 &amp; \text { if }S_1=sd.                       \end {cases} =\max (K-S_1, 0)
\end{equation}


</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(c)</span></span></span> We \(S_1=su\) we sell a unit of stock for strike price \(K\) and otherwise do nothing. So our contingent
claim is
</p>
<p>
\[\Phi (S_1)=\begin {cases} K-S_1 &amp; \text { if }S_1=su,\\ 0 &amp; \text { if }S_1=sd.                       \end {cases} \]
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(d)</span></span></span> Holding both the contracts in (b) and (c) at once, means that in both \(S_1=su\) and \(S-1=sd\), we end
up selling a single unit of stock for a fixed price \(K\). Our contingent claim for doing so is \(\Phi (S_1)=K-S_1\).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-bin_model.html#ps:put_call_parity"><b>5.4</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> The contingent claims for the call and put options respectively are \(\max (S_1-K,0)\) from <span
class="textup" >(<a href="Hedging-in-one-period-model.html#eq:eu_call_op_contigent_2">5.6</a>)</span> and \(\max (K-S_1,0)\) from <span class="textup" >(<a
href="Solutions-exercises-part-one.html#eq:eu_put_contingent">A.4</a>)</span>. Using the risk neutral valuation formula from Proposition <a
href="Hedging-in-one-period-model.html#prop:one_period_pricing">5.2.6</a> we have
</p>
<p>
\[\Pi ^{call}_0=\frac {1}{1+r}\E ^\Q \l [\max (S_1-K,0)\r ],\quad \Pi ^{put}_0=\frac {1}{1+r}\E ^\Q \l [\max (K-S_1,0)\r ].\]
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> Hence,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{4}\)</span>


<!--


                                                                                                1
                                                                                  Πcall
                                                                                   0    − Πput
                                                                                           0 =     EQ [max(S1 − K, 0) − max(K − S1 , 0)]
                                                                                               1+r
                                                                                                1
                                                                                             =     EQ [max(S1 − K, 0) + min(S1 − K, 0)]
                                                                                               1+r
                                                                                                1
                                                                                             =     EQ [S1 − K]
                                                                                               1+r
                                                                                                1  Q           
                                                                                             =      E [S1 ] − K
                                                                                               1+r




-->


<p>


\begin{align*}
\Pi ^{call}_0-\Pi ^{put}_0&amp;=\frac {1}{1+r}\E ^\Q \l [\max (S_1-K,0)-\max (K-S_1,0)\r ]\\ &amp;=\frac {1}{1+r}\E ^\Q \l [\max (S_1-K,0)+\min (S_1-K,0)\r ]\\ &amp;=\frac
{1}{1+r}\E ^\Q \l [S_1-K\r ]\\ &amp;=\frac {1}{1+r}\l (\E ^\Q \l [S_1\r ]-K\r )\\
\end{align*}
By <span class="textup" >(<a href="The-binomial-model.html#eq:risk_free_1">5.4</a>)</span> (or you can use Proposition <a
href="Hedging-in-one-period-model.html#prop:one_period_pricing">5.2.6</a> again) we have \(\frac {1}{1+r}\E ^\Q \l [S_1\r ]=S_0=s\), so we obtain
</p>
<p>
\[\Pi ^{call}_0-\Pi ^{put}_0=s-\frac {K}{1+r}.\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-bin_model.html#ps:contingent_3"><b>5.5</b></a> In the binomial model, the contingent claim of a European call option with strike price \(K\) is \(\Phi
(S_T)=\max (S_T-K)\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-bin_model.html#ps:binomial_hedging"><b>5.6</b></a> A tree-like diagram of the possible changes in stock price, with the value of the contingent claim \(\Phi
(S_T)=\max (K-S_1,0)\) written on for the final time step looks like
</p>
<div class="center" >
<p>


<a href="sud_ex_1.jpg" target="_blank" ><img
      src="sud_ex_1.jpg"
      style="
      width:173pt;
      "
      class="inlineimage"
      alt="(A recombining tree of the stock prices with the contingent claim marked)"
></a>
</p>
</div>
<p>
By <span class="textup" >(<a href="The-binomial-model.html#eq:q_probs">5.3</a>)</span>, the risk free probabilities in this case are
</p>
<p>
\[q_u=\frac {(1+0.25)-0.5}{2.0-0.5}=0.5,\quad q_d=1-q_u=0.5.\]
</p>
<p>
Using Proposition <a href="Hedging-in-one-period-model.html#prop:one_period_pricing">5.2.6</a> recursively, as in Section <a href="Hedging.html#sec:binom_hedge">5.6</a>, we can put in the value
of the European put option at all nodes back to time \(0\), giving
</p>
<div class="center" >
<p>


<a href="sud_ex_2.jpg" target="_blank" ><img
      src="sud_ex_2.jpg"
      style="
      width:173pt;
      "
      class="inlineimage"
      alt="(The previous picture with the value of the contract marked)"
></a>
</p>
</div>
<p>
Hence, the value of our European put option at time \(0\) is \(12\).
</p>
<p>
Changing the values of \(p_u\) and \(p_d\) do not alter value of our put option, since \(p_u\) and \(p_d\) do not appear in the pricing formulae.
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-bin_model.html#ps:binomial_hedging_3"><b>5.7</b></a> A tree-like diagram of the possible changes in stock price, with the value of the option at all nodes of the
tree shown in (green) square boxes, and hedging portfolios \(h=(x,y)\) written (in red) for each node of the tree:
</p>
<div class="center" >
<p>


<a href="sud_ex_4.jpg" target="_blank" ><img
      src="sud_ex_4.jpg"
      style="
      width:173pt;
      "
      class="inlineimage"
      alt="(The previous picture with the hedging portfolios marked)"
></a>
</p>
</div>
<p>
The hedging portfolios are found by solving linear equations of the form \(x+dSy=\wt {\Phi }(dS)\), \(x+uSy=\wt {\Phi }(uD)\), where \(\wt \Phi \) is the contingent claim and \(S\) the initial stock price of
the one period model associated to the given node. The value of the contingent claim at each node is then inferred from the hedging portfolio as \(V=x+Sy\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-bin_model.html#ps:binomial_hedging_2"><b>5.8</b></a> A tree-like diagram of the possible changes in stock price, with the value of the option at all nodes of the
tree shown in (green) square boxes, and hedging portfolios \(h=(x,y)\) written (in red) for each node of the tree:
</p>
<div class="center" >
<p>


<a href="sud_ex_3.jpg" target="_blank" ><img
      src="sud_ex_3.jpg"
      style="
      width:173pt;
      "
      class="inlineimage"
      alt="(A recombining tree of the stock price with the contingent claim and hedging portfolios marked)"
></a>
</p>
</div>
<p>
The hedging portfolios are found by solving linear equations of the form \(x+dSy=\wt {\Phi }(dS)\), \(x+uSy=\wt {\Phi }(uD)\), where \(\wt \Phi \) is the contingent claim and \(S\) the initial stock price of
the one period model associated to the given node. The value of the contingent claim at each node is then inferred from the hedging portfolio as \(V=x+Sy\).
</p>
<p>
The hedging portfolio is the same at each node. This is because the contingent claim of our call option always exceeds zero i.e.&nbsp;we are certain that at time \(T=2\) we would want to exercise our call option
and buy a single unit of stock for a price \(K=60\). Since there is no interest payable on cash, this means that we can replicate our option (at all times) by holding a single unit of stock, and \(-60\) in cash.
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-bin_model.html#ps:bin_mart_2"><b>5.9</b></a> We have that \((S_t)\) is adapted (see Section <a
href="The-binomial-model-definition.html#sec:binom">5.4</a>) and since \(S_t\in (d^t,u^t)\) we have also that \(S_t\in L^1\). Hence, \(S_t\) is a martingale under \(\P \) if and only if \(\E ^\P
[S_{t+1}\|\F _t]=S_t\). That is if
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{4}\)</span>


<!--



                                                                                               EP [St+1 | Ft ] = E[Zt+1 St | Ft ]

                                                                                                               = St EP [Z | Ft ]

                                                                                                               = St EP [Z]



-->


<p>


\begin{align*}
\E ^\P [S_{t+1}\|\F _t]&amp;=\E [Z_{t+1}S_{t}\|\mc {F}_t]\\ &amp;=S_t\E ^\P [Z\|\mc {F}_t]\\ &amp;=S_t\E ^\P [Z]
\end{align*}
is equal to \(S_t\). Hence, \(S_t\) is a martingale under \(\P \) if and only if \(\E ^\P [Z_{t+1}]=up_u+dp_d=1.\)
</p>
<p>
Now consider \(M_t=\log S_t\), and assume that \(0&lt;d&lt;u\) and \(0&lt;s\), which implies \(S_t&gt;0\). Since \(S_t\) is adapted, so is \(M_t\). Since \(S_t\in S_t\in (d^t,u^t)\) we have \(M_t\in
(t \log d, t\log u)\) so also \(M_t\in L^1\). We have
</p>
<p>
\[M_t=\log \l (S_0\prod \limits _{i=1}^tZ_i\r )=\log (S_0)+\sum \limits _{i=1}^t Z_i.\]
</p>
<p>
Hence,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{4}\)</span>


<!--

                                                                                                              t
                                                                                                              X
                                                                                  EP [Mt+1 |Ft ] = log S0 +         log(Zi ) + E[log(Zt+1 ) | Ft ]
                                                                                                              i=1

                                                                                               = Mt + EP [log Zt+1 ]

                                                                                               = Mt + pu log u + pd log d.



-->


<p>


\begin{align*}
\E ^\P [M_{t+1}|\mc {F}_t]&amp;=\log S_0+\sum \limits _{i=1}^{t}\log (Z_i)+\E [\log (Z_{t+1})\|\mc {F}_t]\\ &amp;=M_t+\E ^\P [\log Z_{t+1}]\\ &amp;=M_t+p_u\log u + p_d\log d.
\end{align*}
Here we use taking out what is known, since \(S_0\) and \(Z_i\in m\mc {F}_t\) for \(i\leq t\), and also that \(Z_{t+1}\) is independent of \(\mc {F}_t\). Hence, \(M_t\) is a martingale under \(\P \) if and
only if \(p_u\log u + p_d\log d=0\).
</p>
</li>
</ul>
<h5 id="autosec-275">Chapter <a href="Convergence-random-variables.html#chap:rv_conv">6</a></h5>
<a id="notes_1-autopage-275"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-rv_conv.html#ps:conv_simple"><b>6.1</b></a> Since \(X_n\geq 0\) we have
</p>
<p>
\[\E [|X_n-0|]=\E [X_n]=\tfrac 122^{-n}=2^{-(n+1)}\]
</p>
<p>
which tends to \(0\) as \(n\to \infty \). Hence \(X_n\to 0\) in \(L^1\). Lemma <a href="Convergence-random-variables.html#lem:conv_modes">6.1.2</a> then implies that also \(X_n\to 0\) in
probability and in distribution. Also, \(0\leq X_n\leq 2^{-n}\) and \(2^{-n}\to 0\), so by the sandwich rule we have \(X_n\to 0\) as \(n\to \infty \), and hence \(\P [X_n\to 0]=1\), which means that
\(X_n\to 0\) almost surely.
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-rv_conv.html#ps:conv_simple_3"><b>6.2</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> We have \(X_n\stackrel {L^1}{\to } X\) and hence
</p>
<p>
\[\l |\E [X_n]-\E [X]\r |=|\E [X_n-X]|\leq \E [|X_n-X|]\to 0.\]
</p>
<p>
Here we use the linearity and absolute value properties of expectation. Hence, \(\E [X_n]\to \E [X]\).
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> Let \(X_1\) be a random variable such that \(\P [X_1=1]=\P [X_1=-1]=\frac 12\) and note that \(\E
[X_1]=0\). Set \(X_n=X_1\) for all \(n\geq 2\), which implies that \(\E [X_n]=0\to 0\) as \(n\to \infty \). But \(\E [|X_n-0|]=\E [|X_n|]=1\) so \(X_n\) does not converge to \(0\) in \(L^1\).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-rv_conv.html#ps:conv_simple_4"><b>6.3</b></a> We have \(X_n=X\) when \(U=2\), and \(|X_n-X|=\frac {1}{n}\) when \(U=0,1\). Hence, \(|X_n-X|\leq
\frac {1}{n}\) and by monotonicity of \(\E \) we have
</p>
<p>
\[\E \l [|X_n-X|\r ]\leq \frac {1}{n}\to 0\quad \text { as }n\to \infty ,\]
</p>
<p>
which means that \(X_n\stackrel {L^1}{\to }X\).
</p>
<p>
We can write
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{4}\)</span>


<!--



                                                                                P[Xn → X] = P[Xn → X, U = 0] + P[Xn → X, U = 1] + P[Xn → X, U = 2]
                                                                                                    1                    1
                                                                                          = P[1 +     → 1, U = 0] + P[1 − → 1, U = 1] + P[0 → 0, U = 2]
                                                                                                    n                    n
                                                                                          = P[U = 0] + P[U = 1] + P[U = 2]

                                                                                          = 1.



-->


<p>


\begin{align*}
\P [X_n\to X] &amp;=\P [X_n\to X, U=0]+\P [X_n\to X, U=1] + \P [X_n\to X, U=2]\\ &amp;=\P [1+\frac {1}{n}\to 1, U=0]+\P [1-\frac {1}{n}\to 1, U=1] + \P [0\to 0, U=2]\\ &amp;=\P
[U=0]+\P [U=1]+\P [U=2]\\ &amp;=1.
\end{align*}
Hence, \(X_n\stackrel {a.s.}{\to } X\).
</p>
<p>
By Lemma <a href="Convergence-random-variables.html#lem:conv_modes">6.1.2</a>, it follows that also \(X_n\to X\) in probability and in distribution.
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-rv_conv.html#ps:D_notP"><b>6.4</b></a> We have \(X_n=X_1=1-Y\) for all \(n\), so \(Y=0\iff X_n=1\) and \(Y=1\iff X_n=0\). Hence \(|Y-X_n|=1\) for all
\(n\). So, for example, with \(a=\frac 12\) we have \(\P [|X_n-Y|\geq a]=1\), which does not tend to zero as \(n\to \infty \). Hence \(X_n\) does not converge to \(Y\) in probability.
</p>
<p>
However, \(X_n\) and \(y\) have the same distribution, given by
</p>
<p>
\[\P [Y\leq x]=\P [X_n\leq x]= \begin {cases} 0 &amp; \text { if } x&lt;0\\ \frac 12 &amp; \text { if } x\in [0,1)\\ 1 &amp; \text { if }x\geq 1 \end {cases} \]
</p>
<p>
so we do have \(\P [X_n\leq x]\to \P [Y\leq x]\) as \(n\to \infty \). Hence \(X_n\stackrel {d}{\to }Y\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-rv_conv.html#ps:conv_simple_2"><b>6.5</b></a> Let \((X_n)\) be the sequence of random variables from <a
href="Exercises-on-Chapter-ref-chap-rv_conv.html#ps:conv_simple"><b>6.1</b></a>. Define \(Y_n=X_1+X_2+\ldots +X_n\).
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> For each \(\omega \in \Omega \), we have \(Y_{n+1}(\omega )=Y_n(\omega )+X_{n+1}(\omega )\).
Hence, \((Y_n(\omega )\) is an increasing sequence. Since \(X_n(\omega )\leq 2^{-n}\) for all \(n\) we have
</p>
<p>
\[|Y_n(\omega )|\leq 2^{-1}+2^{-2}+\ldots +2^{-n}\leq \sum \limits _{i=1}^\infty 2^{-i}=1,\]
</p>
<p>
meaning that the sequence \(Y_n(\omega )\) is bounded above by \(1\).
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> Hence, since bounded increasing sequences converge, for any \(\omega \in \Omega \) the sequence
\(Y_n(\omega )\) converges.
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(c)</span></span></span> The value of \(Y_1\) is either \(0\) or \(\frac 12\), both with probability \(\frac 12\). The value of
\(Y_2\) is either \(0,\frac 14,\frac 12,\frac 34\), all with probability \(\frac 14\). The value of \(Y_3\) is either \(0,\frac 18,\frac 14,\frac 38,\frac 12,\frac 58,\frac 34,\frac 78\) all with
probability \(\frac 18\).
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(d)</span></span></span> We can guess from (c) that the \(Y_n\) are becoming more and more evenly spread across \([0,1]\), so
we expect them to approach a uniform distribution as \(n\to \infty \).
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(e)</span></span></span> We will use induction on \(n\). Our inductive hypothesis is
</p>
<ul style="list-style-type:none">


<li>
<p>
(IH)\(_n\): For all \(k=0,1,\ldots ,2^n-1\), it holds that \(\P [Y_n=k2^{-n}]=2^{-n}\).
</p>
</li>
</ul>
<p>
In words, this says that \(Y_n\) is uniformly distributed on \(\{k2^{-n}\-k=0,1,\ldots ,2^n-1\}\).
</p>
<p>
For the case \(n=1\), we have \(Y_1=X_1\) so \(\P [Y_1=0]=\P [Y_1=\tfrac 12]=\tfrac 12\), hence (IH)\(_1\) holds.
</p>
<p>
Now, assume that (IH)\(_n\) holds. We want to calculate \(\P [Y_{n+1}=k2^{-(n+1)}\) for \(k=0,1,\ldots ,2^{n+1}-1\). We consider two cases, dependent on whether \(k\) is even or odd.
</p>
<ul style="list-style-type:none">


<li>
<p>
• If \(k\) is even then we can write \(k=2j\) for some \(j=0,\ldots ,2^n-1\). Hence,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{4}\)</span>


<!--



                                                                                            P[Yn+1 = k2−(n+1) ] = P[Yn+1 = j2−n ]

                                                                                                               = P[Yn = j2−n and Xn+1 = 0]

                                                                                                               = P[Yn = j2−n ]P[Xn+1 = 0]

                                                                                                               = 2−n 21

                                                                                                               = 2−(n+1) .



-->


<p>


\begin{align*}
\P [Y_{n+1}=k2^{-(n+1)}] &amp;=\P [Y_{n+1}=j2^{-n}]\\ &amp;=\P [Y_n=j2^{-n}\text { and }X_{n+1}=0]\\ &amp;=\P [Y_n=j2^{-n}]\P [X_{n+1}=0]\\ &amp;=2^{-n}\tfrac 12\\
&amp;=2^{-(n+1)}.
\end{align*}
Here we use that \(Y_n\) and \(X_{n+1}\) are independent, and use (IH)\(_n\) to calculate \(\P [Y_n=j2^{-n}]\).
</p>


</li>
<li>
<p>
• Alternatively, if \(k\) is odd then we can write \(k=2j+1\) for some \(j=0,\ldots ,2^n-1\). Hence,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{4}\)</span>


<!--



                                                                                         P[Yn+1 = k2−(n+1) ] = P[Yn+1 = j2−n + 2−(n+1) ]

                                                                                                            = P[Yn = j2−n and Xn+1 = 2−(n+1) ]

                                                                                                            = P[Yn = j2−n ]P[Xn+1 = 2−(n+1) ]

                                                                                                            = 2−n 21

                                                                                                            = 2−(n+1) .



-->


<p>


\begin{align*}
\P [Y_{n+1}=k2^{-(n+1)}] &amp;=\P [Y_{n+1}=j2^{-n}+2^{-(n+1)}]\\ &amp;=\P [Y_n=j2^{-n}\text { and }X_{n+1}=2^{-(n+1)}]\\ &amp;=\P [Y_n=j2^{-n}]\P [X_{n+1}=2^{-(n+1)}]\\
&amp;=2^{-n}\tfrac 12\\ &amp;=2^{-(n+1)}.
\end{align*}
Here, again, we use that \(Y_n\) and \(X_{n+1}\) are independent, and use (IH)\(_n\) to calculate \(\P [Y_n=j2^{-n}]\).
</p>
</li>
</ul>
<p>
In both cases we have shown that \(\P [Y_{n+1}=k2^{-(n+1)}]=2^{-(n+1)}\), hence (IH)\(_{n+1}\) holds.
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(f)</span></span></span> We know that \(Y_n\to Y\) almost surely, hence by Lemma <a
href="Convergence-random-variables.html#lem:conv_modes">6.1.2</a> the convergence also holds in distribution. Hence \(\P [Y_n\leq a]\to \P [Y\leq a]\).
</p>
<p>
From part (e), for any \(a\in [0,1]\) we have
</p>
<p>
\[(k-1)2^{-n}\leq a&lt;k2^{-n}\quad \ra \quad \P [Y_n\leq a]=k2^{-n}.\]
</p>
<p>
Hence \(a\leq \P [Y_n\leq a]\leq a+2^{-n}\), and the sandwich rule tells us that \(\P [Y_n\leq a]\to a\). Hence \(\P [Y\leq a]=a\), for all \(a\in [0,1]\), which means that \(Y\) has a uniform
distribution on \([0,1]\).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-rv_conv.html#ps:mct_reveal_tail"><b>6.7</b></a> Since \(\1\{X\leq n+1\}\geq 1\{X\leq n\}\) we have \(X_{n+1}\geq X_n\), and since \(X&gt;0\) we
have \(X_n\geq 0\). Further, for all \(\omega \in \Omega \), whenever \(X(w)&lt;n\) we have \(X_n(\omega )=X(\omega )\), and since \(\P [X(\omega )&lt;\infty ]=1\) this means that \(\P \l
[X_n(\omega )\to X(\omega )\r ]=1\). Hence \(X_n\stackrel {a.s.}{\to } X\).
</p>
<p>
Therefore, the monotone convergence theorem applies and \(\E [X_n]\to \E [X]\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-rv_conv.html#ps:mct_down"><b>6.6</b></a> Define \(X_n=-Y_n\). Then \(X_n\geq 0\) and \(X_{n+1}\geq X_n\), almost surely. Hence, \((X_n)\) satisfies the
conditions for the dominated convergence theorem and there exists a random variable \(X\) such that \(X_n\stackrel {a.s.}{\to }X\) and \(\E [X_n]\to \E [X]\). Define \(Y=-X\) and then we have
\(Y_n\stackrel {a.s.}{\to }Y\) and \(\E [Y_n]\to \E [Y]\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-rv_conv.html#ps:mct_fubini"><b>6.8</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> The equation
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{4}\)</span>
<!--

                                                    n            n
                                                "          #
                                                    X            X
                                            E             Xi =         E[Xi ]                                          (A.5)                                          --><a id="eq:fubini_mct_pre"></a><!--
                                                    i=1          i=1


-->
<p>


\begin{equation}
\label {eq:fubini_mct_pre} \E \l [\sum _{i=1}^n X_i\r ]=\sum _{i=1}^n \E [X_i]
\end{equation}


</p>
<p>
follows by iterating the linearity property of \(\E \) \(n\) times (or by a trivial induction). However, in \(\E \big [\sum _{i=1}^\infty X_i\big ]=\sum _{i=1}^\infty \E [X_i]\) both sides are defined by
infinite sums, which are limits, and the linearity property does not tell us anything about limits.
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> We’ll take a limit as \(n\to \infty \) on each side of <span class="textup" >(<a
href="Solutions-exercises-part-one.html#eq:fubini_mct_pre">A.5</a>)</span>. We’ll have to justify ourselves in each case.
</p>
<p>
First, consider the right hand side. Since we have \(X_n\geq 0\) almost surely, \(\E [X_n]\geq 0\) by monotonicity of \(\E \). Therefore, \(\sum _{i=1}^n \E [X_i]\) is an increasing sequence of real numbers.
Hence it is convergent (either to a real number or to \(+\infty \)) and, by definition of an infinite series, the limit is written \(\sum _{i=1}^\infty \E [X_i]\).
</p>
<p>
Now, the left hand side, and here we’ll need the monotone convergence theorem. Define \(Y_n=\sum _{i=1}^n X_i\). Then \(Y_{n+1}-Y_n=X_{n+1}\geq 0\) almost surely, so \(Y_{n+1}\geq Y_n\). Since
\(X_i\geq 0\) for all \(i\), also \(Y_n\geq 0\), almost surely. Hence the monotone convergence theorem applies and \(\E [Y_n]\to \E [Y_\infty ]\) where \(Y_\infty =\sum _{i=1}^\infty X_i\).
</p>
<p>
Putting both sides together (and using the uniqueness of limits to tell us that the limits on both sides must be equal as \(n\to \infty \)) we obtain that \(\E \big [\sum _{i=1}^\infty X_i\big ]=\sum
_{i=1}^\infty \E [X_i]\).
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(c)</span></span></span> Since \(\P [X_i=1]=\P [X_i=-1]\), we have \(\E [X_i]=0\) and hence also \(\sum _{i=1}^\infty \E
[X_i]=0\). However, the limit \(\sum _{i=1}^\infty X_i\) is not well defined, because \(Y_n=\sum _{i=1}^n X_i\) oscillates (it jumps back and forth between \(0\) and \(1\)) and does not converge. Hence, in
this case \(\E \big [\sum _{i=1}^\infty X_i\big ]\) is not well defined.
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-rv_conv.html#ps:unique_limit"><b>6.9</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> We have both \(\P [X_n\leq x]\to \P [X\leq x]\) and \(\P [X_n\leq x]\to \P [Y\leq x]\), so by
uniqueness of limits for real sequences, we have \(\P [X\leq x]=\P [Y\leq x]\) for all \(x\in \R \). Hence, \(X\) and \(Y\) have the same distribution (i.e.&nbsp;they have the same distribution functions
\(F_X(x)=F_Y(x)\)).
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> By definition of convergence in probability, for any \(a&gt;0\), for any \(\epsilon &gt;0\) there exists
\(N\in \N \) such that, for all \(n\geq N\),
</p>
<p>
\[\P [|X_n-X|&gt;a]&lt;\epsilon \hspace {1pc}\text { and }\hspace {1pc}\P [|X_n-Y|&gt;a]&lt;\epsilon .\]
</p>
<p>
By the triangle inequality we have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{5}\)</span>
<!--


                P[|X − Y | > 2a] = P[|X − Xn + Xn − Y | > 2a] ≤ P[|X − Xn | + |Xn − Y | > 2a].                            (A.6)                                       --><a id="eq:ps_uniq_limit"></a><!--


-->
<p>


\begin{equation}
\label {eq:ps_uniq_limit} \P [|X-Y|&gt;2a]=\P [|X-X_n+X_n-Y|&gt;2a]\leq \P [|X-X_n|+|X_n-Y|&gt;2a].
\end{equation}


</p>
<p>
If \(|X-X_n|+|X_n-Y|&gt;2a\) then \(|X-X_n|&gt;a\) or \(|X_n-Y|&gt;a\) (or possibly both). Hence, continuing <span class="textup" >(<a
href="Solutions-exercises-part-one.html#eq:ps_uniq_limit">A.6</a>)</span>,
</p>
<p>
\[\P [|X-Y|&gt;2a]\leq \P [|X_n-X|&gt;a]+\P [|X_n-Y|&gt;a]\leq 2\epsilon .\]
</p>
<p>
Since this is true for any \(\epsilon &gt;0\) and any \(a&gt;0\), we have \(\P [X=Y]=1\).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-rv_conv.html#ps:not_conv"><b>6.10</b></a> Suppose (aiming for a contradiction) that there exists and random variable \(X\) such that \(X_n\to X\) in
probability. By the triangle inequality we have
</p>
<p>
\[|X_n-X_{n+1}|\leq |X_n-X|+|X-X_{n+1}|\]
</p>
<p>
Hence, if \(|X_n-X_{n+1}|&gt;1\) then \(|X_n-X|&gt;\frac 12\) or \(|X_{n+1}-X|&gt;\frac 12\) (or both). Therefore,
</p>
<p>
\[\P \l [|X_n-X_{n+1}|&gt;1\r ]\leq \P \l [|X_n-X|&gt;\tfrac 12\r ]+\P \l [|X_{n+1}-X|&gt;\tfrac 12\r ].\]
</p>
<p>
Since \(X_n\to X\) in probability, the right hand side of the above tends to zero as \(n\to \infty \). This implies that
</p>
<p>
\[\P \l [|X_n-X_{n+1}|&gt;1\r ]\to 0\]
</p>
<p>
as \(n\to \infty \). But, \(X_n\) and \(X_{n+1}\) are independent and \(\P [X_n=1,X_{n+1}=0]=\frac 14\) so \(\P [|X_n-X_{n+1}|&gt;1]\geq \frac 14\) for all \(n\). Therefore we have a contradiction,
so there is no \(X\) such that \(X_n\to X\) in probability.
</p>
<p>
Hence, by Lemma <a href="Convergence-random-variables.html#lem:conv_modes">6.1.2</a>, we can’t have any \(X\) such that \(X_n\to X\) almost surely or in \(L^1\) (since it would imply convergence in
probability).
</p>
<p>


</p>
</li>
</ul>
<h5 id="autosec-276">Chapter <a href="Stochastic-processes-martingale-theory.html#chap:stoch_procs_1">7</a></h5>
<a id="notes_1-autopage-276"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:mart_tranfs_calcs"><b>7.1</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> We have \((C\circ M)_n=\sum _{i=1}^n 0(S_i-S_{i-1})=0.\)
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> We have \((C\circ M)_n=\sum _{i=1}^n 1(S_i-S_{i-1})=S_n-S_0=S_n.\)
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(c)</span></span></span> We have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{6}\)</span>


<!--

                                                                                                      n
                                                                                                      X
                                                                                      (C ◦ M )n =           Si−1 (Si − Si−1 )
                                                                                                      i=1
                                                                                                      Xn
                                                                                                  =         (X1 + . . . + Xi−1 )Xi
                                                                                                      i=1
                                                                                                       n
                                                                                                                   !2          n
                                                                                                    1 X                     1X
                                                                                                  =       Xi            −        X2
                                                                                                    2 i=1                   2 i=1 i
                                                                                                      Sn2   n
                                                                                                  =       −
                                                                                                       2    2


-->


<p>


\begin{align*}
(C\circ M)_n &amp;=\sum _{i=1}^n S_{i-1}(S_i-S_{i-1})\\ &amp;=\sum _{i=1}^n (X_1+\ldots +X_{i-1})X_i\\ &amp;=\frac 12\l (\sum _{i=1}^n X_i\r )^2-\frac 12\sum _{i=1}^n X_i^2\\
&amp;=\frac {S_n^2}{2}-\frac {n}{2}
\end{align*}
In the last line we use that \(X_i^2=1\).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:mart_tranfs_linear"><b>7.2</b></a> We have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{6}\)</span>


<!--

                                                                                      n
                                                                                      X
                                                                        (X ◦ M )n =     (αCn−1 + βDn−1 )(Mn − Mn−1 )
                                                                                      i=1
                                                                                        Xn                                  n
                                                                                                                            X
                                                                                  =α         Cn−1 (Mn − Mn−1 ) + β                Dn−1 (Mn − Mn−1 )
                                                                                       i=1                                  i=1

                                                                                  = α(C ◦ M )n + β(C ◦ M )n



-->


<p>


\begin{align*}
(X\circ M)_n &amp;=\sum \limits _{i=1}^n (\alpha C_{n-1}+\beta D_{n-1})(M_n-M_{n-1})\\ &amp;=\alpha \sum \limits _{i=1}^n C_{n-1}(M_n-M_{n-1}) + \beta \sum \limits _{i=1}^n
D_{n-1}(M_n-M_{n-1})\\ &amp;=\alpha (C\circ M)_n + \beta (C\circ M)_n
\end{align*}
as required.
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:rw_converge"><b>7.3</b></a> We note that
</p>
<p>
\[|S_n|\leq \sum \limits _{i=1}^n\frac {1}{i^2}\leq \sum \limits _{i=1}^\infty \frac {1}{i^2}&lt;\infty .\]
</p>
<p>
Hence \(S_n\in L^1\) and, moreover, \(\E [|S_n|]\leq \sum _{i=1}^\infty \frac {1}{i^2}\) so \(S_n\) is bounded in \(L^1\). Since \(X_i\in \mc {F}_n\) for all \(i\leq n\), we have also that \(S_n\in
m\mc {F}_n\). Lastly,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{6}\)</span>


<!--


                                                                                                             Xn+1
                                                                                                                                   
                                                                                   E[Sn+1 | Fn ] = E Sn +             Fn
                                                                                                            (n + 1)2
                                                                                                            1
                                                                                                  = Sn +          E [Xn+1 | Fn ]
                                                                                                         (n + 1)2
                                                                                                            1
                                                                                                  = Sn +          E [Xn+1 ]
                                                                                                         (n + 1)2
                                                                                                  = Sn .



-->


<p>


\begin{align*}
\E [S_{n+1}\|\mc {F}_n] &amp;=\E \l [S_n+\frac {X_{n+1}}{(n+1)^2}\,\Big |\,\mc {F}_n\r ]\\ &amp;=S_n+\frac {1}{(n+1)^2}\E \l [X_{n+1}\|\mc {F}_n\r ]\\ &amp;=S_n+\frac
{1}{(n+1)^2}\E \l [X_{n+1}\r ]\\ &amp;=S_n.
\end{align*}
Therefore, \(S_n\) is a martingale, and (due to \(L^1\) boundedness from above) the martingale convergence theorem applies. Hence, there exists some real-valued random variable \(S_\infty \) such that
\(S_n\stackrel {a.s.}{\to }S_\infty \).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:rw_simulate"><b>7.4</b></a> Here’s some <kbd>R</kbd> code to plot \(1000\) time-steps of the symmetric random walk. Changing the
value given to <kbd>set.seed</kbd> will generate a new sample.
</p>
> T=1000
> set.seed(1)
<pre  class="verbatim">
> x=c(0,2*rbinom(T-1,1,0.5)-1)
> y=cumsum(x)
> par(mar=rep(2,4))
> plot(y,type="l")
</pre>


<p>
Adapting it to plot the asymmetric case and the random walk from exercise <a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:rw_converge"><b>7.3</b></a> is left for you.
</p>
<p>
In case you prefer Python, here’s some Python code that does the same job. (I prefer Python.)
</p>
import random
import numpy as np
<pre class="verbatim">
import matplotlib.pyplot as plt
%matplotlib inline


p = 0.5
rr = np.random.random(1000)
step = 2*(rr < p) - 1


start = 0
positions = [start]
for x in step:
      positions.append(positions[-1] + x)


plt.plot(positions)
plt.show()
</pre>


</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:rw_to_zero"><b>7.5</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> This is a trivial induction: if \(S_n\) is even then \(S_{n+1}\) is odd, and vice versa.
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> By part (a), \(p_{2n+1}=0\) since \(0\) is an even number and \(S_{2n+1}\) is an odd number.
</p>
<p>
If \(S_n\) is to start at zero and reach zero at time \(2n\), then during time \(1,2,\ldots ,2n\) it must have made precisely \(n\) steps upwards and precisely \(n\) steps downwards. The number of different
ways in which the walk can do this is \(\binom {2n}{n}\) (we are choosing exactly \(n\) steps on which to go upwards, out of \(2n\) total steps). Each one of these ways has probability \((\frac 12)^{2n}\) of
occurring, so we obtain \(p_{2n}=\binom {2n}{n}2^{-2n}\).
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(c)</span></span></span> From (b) we note that
</p>
<p>
\[p_{2(n+1)}=\frac {(2n+2)(2n+1)}{(n+1)(n+1)}2^{-2}p_{2n}=2\l (\frac {2n+1}{n+1}\r )2^{-2}p_n=\l (\frac {2n+1}{2n+2}\r )p_{2n}=\l (1-\frac {1}{2(n+1)}\r )\frac 12 p_{2n}.\]
</p>
<p>
Hence, using the hint, we have
</p>
<p>
\[p_{2(n+1)}\leq \exp \l (-\frac {1}{2(n+1)}\r )p_{2n}.\]
</p>
<p>
Iterating this formula obtains that
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{6}\)</span>


<!--


                                                                                                            1          1                  1
                                                                                                                                                                               
                                                                                             p2n   ≤ exp −    exp −          . . . exp −      p0
                                                                                                           2n       2(n − 1)             2(1)
                                                                                                              n
                                                                                                                         !
                                                                                                           1X    1
                                                                                                   = exp −         .                                         (A.7)                                           --><a id="eq:ssrw_p2n"></a><!--
                                                                                                           2 i=1 i


-->


<p>


\begin{align}
p_{2n} &amp;\leq \exp \l (-\frac {1}{2n}\r )\exp \l (-\frac {1}{2(n-1)}\r )\ldots \exp \l (-\frac {1}{2(1)}\r )p_0\notag \\ &amp;=\exp \l (-\frac {1}{2}\sum \limits _{i=1}^n\frac
{1}{i}\r ).\label {eq:ssrw_p2n}
\end{align}
Recall that \(\sum _{i=1}^n\frac {1}{i}\to \infty \) as \(n\to \infty \), and that \(e^{-x}\to 0\) as \(x\to \infty \). Hence, the right hand side of the expression above tends to zero as \(n\to
\infty \). Hence, since \(p_{n}\geq 0\), by the sandwich we have \(p_{2n}\to 0\) as \(n\to \infty \).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:gw_when_dies_out"><b>7.6</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> Take the offspring distribution to be \(\P [G=0]=1\). Then \(Z_1=0\), and hence \(Z_n=0\) for all
\(n\geq 1\), so \(Z_n\) dies out.
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> Take the offspring distribution to be \(\P [G=2]=1\). Then \(Z_{n+1}=2Z_n\), so \(Z_n=2^n\), and
hence \(Z_n\to \infty \).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:urn_mean_revert"><b>7.7</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> We have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{7}\)</span>


<!--



                                                                            E[Mn+1 | Fn ] = E Mn+1 1{nth draw is red} Fn + E Mn+1 1{nth draw is black} Fn
                                                                                              h                                                i            h                                     i


                                                                                               Bn                                  B +1
                                                                                                                                                                                                   
                                                                                         =E          1{nth draw is red} Fn + E n          1{nth draw is black} Fn
                                                                                              n+3                                   n+3
                                                                                            Bn                             i B +1 h
                                                                                                 E 1{nth draw is red} Fn +             E 1{nth draw is black} Fn
                                                                                                   h                                                             i
                                                                                                                                 n
                                                                                         =
                                                                                           n+3                                  n+3
                                                                                            Bn Bn         Bn + 1         Bn
                                                                                                                            
                                                                                         =             +           1−
                                                                                           n+3n+2          n+3          n+2
                                                                                                 Bn2         Bn (n + 2) + (n + 2) − Bn2 − Bn
                                                                                         =                +
                                                                                           (n + 2)(n + 3)             (n + 2)(n + 3)
                                                                                           (n + 1)Bn + (n + 2)
                                                                                         =
                                                                                              (n + 2)(n + 3)


-->


<p>


\begin{align*}
\E [M_{n+1}\|\mc {F}_n] &amp;=\E \l [M_{n+1}\1\{n^{th}\text { draw is red}\}\,\Big |\,\mc {F}_n\r ] +\E \l [M_{n+1}\1\{n^{th}\text { draw is black}\}\,\Big |\,\mc {F}_n\r ]\\
&amp;=\E \l [\frac {B_n}{n+3}\,\1\{n^{th}\text { draw is red}\}\,\Big |\,\mc {F}_n\r ] +\E \l [\frac {B_n+1}{n+3}\,\1\{n^{th}\text { draw is black}\}\,\Big |\,\mc {F}_n\r ]\\
&amp;=\frac {B_n}{n+3}\E \l [\1\{n^{th}\text { draw is red}\}\,\Big |\,\mc {F}_n\r ] +\frac {B_n+1}{n+3}\E \l [\,\1\{n^{th}\text { draw is black}\}\,\Big |\,\mc {F}_n\r ]\\
&amp;=\frac {B_n}{n+3}\frac {B_n}{n+2}+\frac {B_n+1}{n+3}\l (1-\frac {B_n}{n+2}\r )\\ &amp;=\frac {B_n^2}{(n+2)(n+3)}+\frac {B_n(n+2)+(n+2)-B_n^2-B_n}{(n+2)(n+3)}\\ &amp;=\frac
{(n+1)B_n+(n+2)}{(n+2)(n+3)}
\end{align*}
This is clearly not equal to \(M_n=\frac {B_n}{n+2}\), so \(M_n\) is not a martingale.
</p>
<p>
<i>This result might be rather surprising, but we should note that the urn process here is not ‘fair’, when considered in terms of the proportion of (say) red balls. If we started with more red balls than black balls,
then over time we would expect to see the proportion of red balls reduce towards \(\frac 12\), as we see in part (b):</i>
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> Your conjecture should be that, regardless of the initial state of the urn, \(\P [M_n\to \frac 12\text {
as }n\to \infty ]=1\). (It is true.)
</p>
<p>


</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:urn_moran"><b>7.8</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> Let \(B_n\) be the number of red balls in the urn at time \(n\), and then \(M_n=\frac {B_n}{K}\). Let
us abuse notation slightly and write \(B=r\) for the event that the ball \(X\) is red, and \(X=b\) for the event that the ball \(X\) is black. Let \(X_1\) denote the first ball drawn on the \((n+1)^{th}\) draw, and
\(X_2\) denote the second ball drawn.
</p>
<p>
Let \(\mc {F}_n=\sigma (B_1,\ldots ,B_n)\). It is clear that \(M_n\in [0,1]\), so \(M_n\in L^1\), and that \(M_n\) is measurable with respect to \(\mc {F}_n\). We have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{7}\)</span>


<!--



                                                                            E[Mn+1 | Fn ] = E Mn+1 1{X1 =r,X2 =r} | Fn + E Mn+1 1{X1 =r,X2 =b} | Fn
                                                                                              h                                  i             h                                        i


                                                                                               + E Mn+1 1{X1 =b,X2 =r} | Fn + E Mn+1 1{X1 =b,X2 =b} | Fn
                                                                                                       h                                   i            h                                     i

                                                                                                Bn                              B +1
                                                                                                                                                                                     
                                                                                         =E        1{X1 =r,X2 =r} | Fn + E n           1{X1 =r,X2 =b} | Fn
                                                                                                K                                 K
                                                                                                     Bn − 1                              B
                                                                                                                                                             
                                                                                               +E            1{X1 =b,X2 =r} | Fn + E n 1{X1 =b,X2 =b} | Fn
                                                                                                       K                                 K
                                                                                             Bn                             Bn + 1
                                                                                                E 1{X1 =r,X2 =r} | Fn +            E 1{X1 =r,X2 =b} | Fn
                                                                                                 h                    i              h                    i
                                                                                         =
                                                                                             K                                K
                                                                                                 Bn − 1 h                        i B
                                                                                                         E 1{X1 =b,X2 =r} | Fn +         E 1{X1 =b,X2 =b} | Fn
                                                                                                                                           h                    i
                                                                                                                                       n
                                                                                               +
                                                                                                    K                                K
                                                                                             Bn Bn Bn Bn + 1 Bn K − Bn
                                                                                         =             +
                                                                                             K K K            K K          K
                                                                                                 Bn − 1 K − Bn Bn Bn K − Bn K − Bn
                                                                                               +                         +
                                                                                                    K        K       K      K      K        K
                                                                                              1  3        2                                                                 2
                                                                                                                                                                               
                                                                                         =       B n +  2B n (K  −  B n ) + B n (K − B)n  −  B n (K  −  B n ) + B n (K − Bn )
                                                                                             K3
                                                                                              1  2 
                                                                                         =       K Bn
                                                                                             K3
                                                                                         = Mn



-->


<p>


\begin{align*}
\E [M_{n+1}\|\mc {F}_{n}] &amp;=\E \l [M_{n+1}\1_{\{X_1=r, X_2=r\}}\|\mc {F}_n\r ]+\E \l [M_{n+1}\1_{\{X_1=r, X_2=b\}}\|\mc {F}_n\r ]\\ &amp;\hspace {2pc}+\E \l
[M_{n+1}\1_{\{X_1=b, X_2=r\}}\|\mc {F}_n\r ]+\E \l [M_{n+1}\1_{\{X_1=b, X_2=b\}}\|\mc {F}_n\r ]\\ &amp;=\E \l [\frac {B_n}{K}\1_{\{X_1=r, X_2=r\}}\|\mc {F}_n\r ]+\E \l [\frac
{B_n+1}{K}\1_{\{X_1=r, X_2=b\}}\|\mc {F}_n\r ]\\ &amp;\hspace {2pc}+\E \l [\frac {B_n-1}{K}\1_{\{X_1=b, X_2=r\}}\|\mc {F}_n\r ]+\E \l [\frac {B_n}{K}\1_{\{X_1=b, X_2=b\}}\|\mc
{F}_n\r ]\\ &amp;=\frac {B_n}{K}\E \l [\1_{\{X_1=r, X_2=r\}}\|\mc {F}_n\r ]+\frac {B_n+1}{K}\E \l [\1_{\{X_1=r, X_2=b\}}\|\mc {F}_n\r ]\\ &amp;\hspace {2pc}+\frac {B_n-1}{K}\E \l
[\1_{\{X_1=b, X_2=r\}}\|\mc {F}_n\r ]+\frac {B_n}{K}\E \l [\1_{\{X_1=b, X_2=b\}}\|\mc {F}_n\r ]\\ &amp;=\frac {B_n}{K}\frac {B_n}{K}\frac {B_n}{K}+\frac {B_n+1}{K}\frac
{B_n}{K}\frac {K-B_n}{K}\\ &amp;\hspace {2pc}+\frac {B_n-1}{K}\frac {K-B_n}{K}\frac {B_n}{K}+\frac {B_n}{K}\frac {K-B_n}{K}\frac {K-B_n}{K}\\ &amp;=\frac {1}{K^3}\l
(B_n^3+2B_n^2(K-B_n)+B_n(K-B)n-B_n(K-B_n)+B_n(K-B_n)^2\r )\\ &amp;=\frac {1}{K^3}\l (K^2B_n\r )\\ &amp;=M_n
\end{align*}


</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> Since \(M_n\in [0,1]\) we have \(\E [|M_n|]\leq 1\), hence \(M_n\) is bounded in \(L^1\). Hence,
by the martingale convergence theorem there exists a random variable \(M_\infty \) such that \(M_n\stackrel {a.s.}{\to }M_\infty \).
</p>
<p>
Since \(M_n=\frac {B_n}{K}\), this means that also \(B_n\stackrel {a.s.}{\to }KM_\infty \). However, \(B_n\) is integer valued so, by Lemma <a
href="Long-term-behaviour-stochastic-processes.html#lem:Z_ev_const">7.4.1</a>, if \(B_n\) is to converge then it must eventually become constant. This can only occur if, eventually, the urn contains
either (i) only red balls or (ii) only black balls.
</p>
<p>
In case (i) we have \(M_n=0\) eventually, which means that \(M_\infty =0\). In case (ii) we have \(M_n=1\) eventually, which means that \(M_\infty =1\). Hence, \(M_\infty \in \{0,1\}\) almost surely.
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:urn_initial_conds"><b>7.9</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> Consider the <i>usual</i> Po&#x0301;lya urn, from Section <a
href="Urn-processes.html#sec:urn">4.2</a>, started with just one red ball and one black ball. After the first draw is complete we have two possibilities:
</p>
<ul style="list-style-type:none">


<li>
<p>
1. The urn contains one red ball and two black balls.
</p>


</li>
<li>
<p>
2. The urn contains two red balls and one black ball.
</p>
</li>
</ul>
<p>
In the first case we reach precisely the initial state of the urn described in <a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:urn_initial_conds"><b>7.9</b></a>. In the second case we also
reach the initial state of the urn described in <a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:urn_initial_conds"><b>7.9</b></a>, but with the colours red and black swapped.
</p>
<p>
If \(M_n\stackrel {a.s.}{\to } 0\), then in the first case the fraction of red balls would tend to zero, and (by symmetry of colours) in the second case the limiting fraction of black balls would tend to zero; thus
the limiting fraction of black balls would always be either \(1\) (the first case) or \(0\) (the second case). However, we know from Proposition <a
href="Long-term-behaviour-stochastic-processes.html#prop:urn_limit">7.4.3</a> that the limiting fraction of black balls is actually uniform on \((0,1)\). Hence, \(\P [M_n\to 0]=0\).
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> You should discover that having a higher proportion of initial red balls makes it more likely for
\(M_\infty \) (the limiting proportion of red balls) to be closer to \(1\). However, since \(M_\infty \) is a random variable in this case, you may need to take several samples of the process (for each initial
condition that you try) to see the effect.
</p>
<p>
<b>Follow-up question:</b> In fact, the limiting value \(M_\infty \) has a Beta distribution, \(Be(\alpha ,\beta )\). You may need to look up what this means, if you haven’t seen the Beta distribution
before. Consider starting the urn with \(n_1\) red and \(n_2\) black balls, and see if you can use your simulations to guess a formula for \(\alpha \) and \(\beta \) in terms of \(n_1\) and \(n_2\) (the answer is
a simple formula!).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:rw_sign"><b>7.10</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> We note that
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{7}\)</span>


<!--


                                                                                                   Sn+1            Sn        1
                                                                                                                    
                                                                                              E           Fn =           +         E[Xn+1 | Fn ]
                                                                                                f (n + 1)      f (n + 1) f (n + 1)
                                                                                                                   Sn        1
                                                                                                             =           +         E[Xn+1 ]
                                                                                                               f (n + 1) f (n + 1)
                                                                                                                   Sn
                                                                                                             =
                                                                                                               f (n + 1)


-->


<p>


\begin{align*}
\E \l [\frac {S_{n+1}}{f(n+1)}\,\Big |\,\mc {F}_n\r ]&amp;=\frac {S_n}{f(n+1)}+\frac {1}{f(n+1)}\E [X_{n+1}\|\mc {F}_n]\notag \\ &amp;=\frac {S_n}{f(n+1)}+\frac {1}{f(n+1)}\E
[X_{n+1}]\notag \\ &amp;=\frac {S_n}{f(n+1)}
\end{align*}
Here we use that \(S_n\in m\mc {F}_n\) and that \(X_{n+1}\) is independent of \(\mc {F}_n\).
</p>
<p>
Since \(\frac {S_n}{f(n)}\) is a martingale we must have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{7}\)</span>
<!--

                                                                   Sn       Sn
                                                                         =       .                                                                                     (A.8)                                    --><a id="eq:ESnfn"></a><!--
                                                               f (n + 1)   f (n)


-->
<p>


\begin{equation}
\label {eq:ESnfn} \frac {S_n}{f(n+1)}=\frac {S_n}{f(n)}.
\end{equation}


</p>
<p>
Since \(\P [S_n\neq 0]&gt;0\) this implies that \(\frac {1}{f(n+1)}=\frac {1}{f(n)}\), which implies that \(f(n+1)=f(n)\). Since this holds for any \(n\), we have \(f(1)=f(n)\) for all \(n\); so \(f\) is
constant.
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> If \(\frac {S_n}{f(n)}\) is to be a supermartingale then, in place of <span class="textup" >(<a
href="Solutions-exercises-part-one.html#eq:ESnfn">A.8</a>)</span>, we would need
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{8}\)</span>
<!--

                                                              Sn       Sn
                                                                    ≤       .                                                                           (A.9)                                             --><a id="eq:ssrw_fnmart"></a><!--
                                                          f (n + 1)   f (n)


-->
<p>


\begin{equation}
\label {eq:ssrw_fnmart} \frac {S_n}{f(n+1)}\leq \frac {S_n}{f(n)}.
\end{equation}


</p>
<p>
Note that we would need this equation to hold true with probability one. To handle the \(\geq \) we now need to care about the sign of \(S_n\). The key idea is that \(S_n\) can be both positive or negative, so
the only way that <span class="textup" >(<a href="Solutions-exercises-part-one.html#eq:ssrw_fnmart">A.9</a>)</span> can hold is if \(\frac {1}{f(n+1)}=\frac {1}{f(n)}\).
</p>
<p>
We will use some of the results from the solution of exercise <a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:rw_to_zero"><b>7.5</b></a>. In particular, from for odd \(n\in \N \) from
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:rw_to_zero"><b>7.5</b></a>(b) we have \(P[S_n=0]=0\), and for even \(n\in \N \) from <span class="textup" >(<a
href="Solutions-exercises-part-one.html#eq:ssrw_p2n">A.7</a>)</span> we have \(\P [S_n=0]&lt;1\). In both cases we have \(\P [S_n\neq 0]&gt;0\). Since \(S_n\) and \(-S_n\) have the same
distribution we have \(\P [S_n&lt;0]=\P [-S_n&lt;0]=\P [S_n&gt;0]\), and hence
</p>
<p>
\[\P [S_n\neq 0]=\P [S_n&lt;0]+\P [S_n&gt;0]=2\P [S_n&lt;0]=2\P [S_n&gt;0].\]
</p>
<p>
Hence, for all \(n\in \N \) we have \(\P [S_n&gt;0]&gt;0\) and \(\P [S_n&lt;0]&gt;0\).
</p>
<p>
Now, consider \(n\geq 1\). We have that here is positive probability that \(S_n&gt;0\). Hence, by <span class="textup" >(<a
href="Solutions-exercises-part-one.html#eq:ssrw_fnmart">A.9</a>)</span>, we must have \(\frac {1}{f(n+1)}\leq \frac {1}{f(n)}\), which means that \(f(n+1)\geq f(n)\). But, there is also
positive probability that \(S_n&lt;0\), hence by <span class="textup" >(<a href="Solutions-exercises-part-one.html#eq:ssrw_fnmart">A.9</a>)</span> we must have \(\frac {1}{f(n+1)}\geq \frac
{1}{f(n)}\), which means that \(f(n+1)\leq f(n)\). Hence \(f(n+1)=f(n)\), for all \(n\), and by a trivial induction we have \(f(1)=f(n)\) for all \(n\).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:gw_var"><b>7.11</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> Since \(M_n=\frac {Z_n}{\mu ^n}\) we note that
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{9}\)</span>


<!--


                                                                                                              1
                                                                                                                   (Zn+1 − µZn )2 = (Mn+1 − Mn )2 .                                 (A.10)                   --><a id="eq:gw_sqr_1"></a><!--
                                                                                                           µ2(n+1)


-->


<p>


\begin{align}
\label {eq:gw_sqr_1} \frac {1}{\mu ^{2(n+1)}}(Z_{n+1}-\mu Z_n)^2=(M_{n+1}-M_n)^2.
\end{align}
Further,
</p>
<p>
\[Z_{n+1}-\mu Z_n=\sum \limits _{i=1}^{Z_n}(X^{n+1}_i-\mu )\]
</p>
<p>
so it makes sense to define \(Y_i=X^{n+1}_i-\mu \). Then \(\E [Y_i]=0\),
</p>
<p>
\[\E [Y_i^2]=\E [(X^{n+1}_i-\mu )^2]=\var (X^{n+1}_i)=\sigma ^2,\]
</p>
<p>
and \(Y_1,Y_2,\ldots ,Y_n\) are independent. Moreover, the \(Y_i\) are identically distributed and independent of \(\mc {F}_n\). Hence, by taking out what is known (\(Z_n\in m\mc {F}_n\)) we have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{10}\)</span>


<!--

                                                                                                                                      
                                                                                                                                               Zn
                                                                                                                                                            !2         
                                                                                               h                         i                     X
                                                                                             E (Zn+1 − µZn )2 | Fn = E                                Yi           Fn 
                                                                                                                                               i=1
                                                                                                                                     Zn
                                                                                                                                     X                               Zn
                                                                                                                                                                     X
                                                                                                                             =             E[Yi2       | Fn ] +              E[Yi Yj | Fn ]
                                                                                                                                     i=1                             i,j=1
                                                                                                                                                                     i6=j
                                                                                                                                     Zn
                                                                                                                                     X                       Zn
                                                                                                                                                             X
                                                                                                                             =             E[Yi2 ] +                 E[Yi ]E[Yj ]
                                                                                                                                     i=1                     i,j=1
                                                                                                                                                             i6=j

                                                                                                                             = Zn E[Y1 ]2 + 0

                                                                                                                             = Zn σ 2 .



-->


<p>


\begin{align*}
\E \l [(Z_{n+1}-\mu Z_n)^2\|\mc {F}_n\r ] &amp;=\E \l [\l (\sum \limits _{i=1}^{Z_n}Y_{i}\r )^2\,\bigg |\,\mc {F}_n\r ]\\ &amp;=\sum \limits _{i=1}^{Z_n}\E [Y_{i}^2\|\mc
{F}_n]+\sum \limits _{\stackrel {i,j=1}{i\neq j}}^{Z_n}\E [Y_iY_j\|\mc {F}_n]\\ &amp;=\sum \limits _{i=1}^{Z_n}\E [Y_{i}^2]+\sum \limits _{\stackrel {i,j=1}{i\neq j}}^{Z_n}\E
[Y_i]\E [Y_j]\\ &amp;=Z_n\E [Y_1]^2+0\\ &amp;=Z_n\sigma ^2.
\end{align*}
So, from <span class="textup" >(<a href="Solutions-exercises-part-one.html#eq:gw_sqr_1">A.10</a>)</span> we obtain
</p>
<p>
\[\E [(M_{n+1}-M_n)^2\|\mc {F}_n]=\frac {Z_n\sigma ^2}{\mu ^{2(n+1)}}.\]
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> By part (a) and <span class="textup" >(<a
href="Exercises-on-Chapter-ref-chap-cond_exp.html#eq:doob_decomp_pre">3.4</a>)</span>, we have
</p>
<p>
\[\E [M_{n+1}^2\|\mc {F}_n]=M_n^2+\frac {Z_n\sigma ^2}{\mu ^{2(n+1)}}.\]
</p>
<p>
Taking expectations, and using that \(\E [Z_n]=\mu ^n\), we obtain
</p>
<p>
\[\E [M_{n+1}^2]=\E [M_n^2]+\frac {\sigma ^2}{\mu ^{n+2}}.\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:fatou_subst"><b>7.12</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> Note that \(X_{n+1}\geq X_n\geq 0\) for all \(n\), because taking an \(\inf \) over \(k&gt;n+1\) is
an infimum over a smaller set than over \(k&gt;n\). Hence, \((X_n)\) is monotone increasing and hence the limit \(X_\infty (\omega )=\lim _{n\to \infty }X_n(\omega )\) exists (almost surely).
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> Since \(|M_n|\leq \inf _{k&gt;n}|M_k|\), we have \(X_n\leq |M_n|\). By definition of \(\inf \),
for each \(\epsilon &gt;0\) and \(n\in \N \) there exists some \(n’\geq n\) such that \(X_n\geq |X_{n’}|-\epsilon \). Combining these two properties, we obtain
</p>
<p>
\[|M_{n’}|-\epsilon \leq X_n\leq |M_n|.\]
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(c)</span></span></span> Letting \(n\to \infty \), in the above equation, which means that also \(n’\to \infty \) because
\(n’\geq n\), we take an almost sure limit to obtain \(|M_\infty |-\epsilon \leq X_\infty \leq |M_\infty |\). Since we have this inequality for any \(\epsilon &gt;0\), in fact \(X_\infty =|M_\infty
|\) almost surely.
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(d)</span></span></span> We have shown that \((X_n)\) is an increasing sequence and \(X_n\stackrel {a.s.}{\to } X_\infty
\), and since each \(|M_n|\geq 0\), we have also \(X_n\geq 0\). Hence, the monotone convergence theorem applies to \(X_n\) and \(X_\infty \), and therefore \(\E [X_n]\to \E [X_\infty ]\).
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(e)</span></span></span> From the monotone convergence theorem, writing in terms of \(M_n\) rather than \(X_n\), we obtain
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{10}\)</span>
<!--

                                                                   
                                                 E inf |Mn | → E[|M∞ |].                                                                                     (A.11)                                            --><a id="eq:Mnconv"></a><!--
                                                      k≥n



-->
<p>


\begin{equation}
\label {eq:Mnconv} \E \l [\inf _{k\geq n}|M_n|\r ]\to \E [|M_\infty |].
\end{equation}


</p>
<p>
However, since \(\inf _{k\geq n}|M_n|\leq |M_n|\leq C=\sup _{j\in \N } \E [|M_j|]\), the monotonicity property of expectation gives us that, for all \(n\),
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{11}\)</span>
<!--

                                                                       
                                                      E inf |Mn | ≤ C.                                                                                 (A.12)                                                --><a id="eq:Mn_bound"></a><!--
                                                            k≥n



-->
<p>


\begin{equation}
\label {eq:Mn_bound} \E \l [\inf _{k\geq n}|M_n|\r ]\leq C.
\end{equation}


</p>
<p>
Combining <span class="textup" >(<a href="Solutions-exercises-part-one.html#eq:Mnconv">A.11</a>)</span> and <span class="textup" >(<a
href="Solutions-exercises-part-one.html#eq:Mn_bound">A.12</a>)</span>, with the fact that limits preserve weak inequalities, we obtain that \(\E [|M_\infty |]\leq C\), as required.
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:gw_subcrit"><b>7.13</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> All the \(\wt {X}^{n+1}_i\) have the same distribution, and are independent by independence of the
\(X^{n+1}_i\) and \(C^{n+1}_i\). Thus, \(\wt {Z}_n\) is a Galton-Watson process with off-spring distribution \(\wt {G}\) given by the common distribution of the \(X^{n+1}_i\).
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> By definition of \(\wt {X}^{n+1}_i\) we have \(X^{n+1}_i\leq \wt {X}^{n+1}_i\). Hence, from
<span class="textup" >(<a href="Long-term-behaviour-stochastic-processes.html#eq:gw_iterative_defn">7.8</a>)</span>, if \(Z_{n}\leq \wt {Z}_n\) then also \(Z_{n+1}\leq \wt {Z}_{n+1}\).
Since \(Z_0=\wt {Z}_0=1\), an easy induction shows that \(Z_n\leq \wt {Z}_n\) for all \(n\in \N \).
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(c)</span></span></span> We have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{12}\)</span>


<!--


                                                                                                                    h            i
                                                                                                                     e n+1
                                                                                                           f (α) = E X i
                                                                                                                                                        ∞
                                                                                                                                                        X
                                                                                                                 = P[G = 0]P[C = 1] +                             iP[G = i]
                                                                                                                                                            i=1
                                                                                                                                           ∞
                                                                                                                                           X
                                                                                                                 = αP[G = 0] +                     iP[G = i].
                                                                                                                                           i=1



-->


<p>


\begin{align*}
f(\alpha )&amp;=\E \l [\wt {X}^{n+1}_i\r ]\\ &amp;=\P [G=0]\P [C=1]+\sum \limits _{i=1}^\infty i\P [G=i]\\ &amp;=\alpha \P [G=0]+\sum \limits _{i=1}^\infty i\P [G=i].
\end{align*}
Recall that each \(X^{n+1}_i\) has the same distribution as \(G\). Hence,
</p>
<p>
\[\mu =\E [X^{n+1}_i]=\E [G]=\sum _{i=1}^\infty i\P [G=i]=f(0).\]
</p>
<p>
Since \(\mu &lt;1\) we therefore have \(f(0)&lt;1\). Moreover,
</p>
<p>
\[f(1)=\P [G=0]+\sum _{i=1}^\infty i\P [G=i]\geq \sum _{i=0}^\infty \P [G=i]=1.\]
</p>
<p>
It is clear that \(f\) is continuous (in fact, \(f\) is linear). Hence, by the intermediate value theorem, there is some \(\alpha \in [0,1]\) such that \(f(\alpha )=1\).
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(d)</span></span></span> By (c), we can choose \(\alpha \) such that \(f(\alpha )=1\). Hence, \(\E [\wt {X}^{n+1}_i]=1\),
so by (a) \(\wt {Z}_n\) is a Galton-Watson process with an offspring distribution that has mean \(\hat {\mu }=1\). By Lemma <a
href="Long-term-behaviour-stochastic-processes.html#lem:gw_crit">7.4.6</a>, \(\wt {Z}\) dies out, almost surely. Since, from (b), \(0\leq Z_n\leq \wt {Z}_n\), this means \(Z_n\) must also die out,
almost surely.
</p>
</li>
</ul>
</li>
</ul>
<h5 id="autosec-280">Chapter <a href="Further-theory-stochastic-processes.html#chap:stoch_procs_2">8</a></h5>
<a id="notes_1-autopage-280"></a>



<ul style="list-style-type:none">


<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_2.html#ps:dct"><b>8.1</b></a> We have \(U^{-1}&lt;1\) so, for all \(\omega \in \Omega \), \(X_n(\omega )=U^{-n}(\omega )\to 0\) as
\(n\to \infty \). Hence \(X_n(\omega )\to 0\) almost surely as \(n\to \infty \). Further, \(|X_n|\leq 1\) for all \(n\in \N \) and \(\E [1]=1&lt;\infty \), so the dominated convergence theorem
applies and shows that \(\E [X_n]\to \E [0]=0\) as \(n\to \infty \).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_2.html#ps:dct_L1"><b>8.2</b></a> We check the two conditions of the dominated convergence theorem. To check the first condition, note that if
\(n\geq |X|\) then \(X\1_{\{|X|\leq n\}}=X\). Hence, since \(|X|&lt;\infty \), as \(n\to \infty \) we have \(X_n=X\1_{\{|X|\leq n\}}\to X\) almost surely.
</p>
<p>
To check the second condition, set \(Y=|X|\) and then \(\E [|Y|]=\E [|X|]&lt;\infty \) so \(Y\in L^1\). Also, \(|X_n|=|X\1_{\{|X|\leq n\}}|\leq |X|=Y\), so \(Y\) is a dominated random variable for
\((X_n)\). Hence, the dominated convergence theorem applies and \(\E [X_n]\to \E [X]\).
</p>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_2.html#ps:dct_necsuff"><b>8.3</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> We look to use the dominated convergence theorem. For any \(\omega \in \Omega \) we have
\(Z(\omega )&lt;\infty \), hence for all \(n\in \N \) such that \(n&gt;Z(\omega )\) we have \(X_n(\omega )=0\). Therefore, as \(n\to \infty \), \(X_n(\omega )\to 0\), which means that \(X_n\to 0\)
almost surely.
</p>
<p>
We have \(|X_n|\leq Z\) and \(Z\in L^1\), so we can use \(Z\) are the dominating random variable. Hence, by the dominated convergence theorem, \(\E [X_n]\to \E [0]=0\).
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> We have
</p>
<p>
\[\E [Z]=\int _1^\infty x f(x)\,dx=\int _1^\infty x^{-1}\,dx=\l [\log x\r ]_1^\infty =\infty \]
</p>
<p>
which means \(Z\notin L^1\) and also that
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{12}\)</span>


<!--

                                                                                                                     Z   ∞
                                                                          E[Xn ] = E[1{Z∈[n,n+1)} Z] =                         1{x∈[n,n+1)} xf (x) dx
                                                                                                                     1
                                                                                     Z   n+1                                                              
                                                                                                                                                           n+1
                                                                                                                                                                    
                                                                                 =             x   −1
                                                                                                        dx =   [log x]n+1
                                                                                                                      n         = log(n + 1) − log n = log     .
                                                                                     n                                                                      n


-->


<p>


\begin{align*}
\E [X_n]&amp;=\E [\1_{\{Z\in [n,n+1)\}}Z]=\int _1^\infty \1_{\{x\in [n,n+1)\}}x f(x)\,dx\\ &amp;=\int _{n}^{n+1} x^{-1}\,dx=\l [\log x\r ]_{n}^{n+1}=\log (n+1)-\log n=\log \l
(\frac {n+1}{n}\r ).
\end{align*}
As \(n\to \infty \), we have \(\frac {n+1}{n}=1+\frac {1}{n}\to 1\), hence (using that \(\log \) is a continuous function) we have \(\log (\frac {n+1}{n})\to \log 1=0\). Hence, \(\E [X_n]\to 0\).
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(c)</span></span></span> Suppose that we wanted to use the DCT in (b). We still have \(X_n\to 0\) almost surely, but any
dominating random variable \(Y\) would have to satisfy \(Y\geq |X_n|\) for all \(n\), meaning that also \(Y\geq Z\), which means that \(\E [Y]\geq \E [Z]=\infty \); thus there is no dominating random
variable \(Y\in L^1\). Therefore, we can’t use the DCT here, but we have shown in (b) that the conclusion of the DCT does hold: we have that \(\E [X_n]\) does tend to zero.
</p>
<p>
We obtain that the conditions of the DCT are <i>sufficient</i> but not <i>necessary</i> for its conclusion to hold.
</p>
<p>


</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_2.html#ps:biased_walk"><b>8.4</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> Since \(T\) is a stopping time, and (from Section <a
href="Stochastic-processes.html#sec:asrw">4.1</a>) \(S_n-(p-q)n\) is a martingale, Theorem <a href="The-optional-stopping-theorem.html#stoppedmart">8.2.4</a> tells us that \(M_n=S_{T\wedge
n}-(T\wedge n)(p-q)\) is a martingale. We have \(\E [T]&lt;\infty \) from Lemma <a href="Hitting-probabilities-random-walks.html#lem:E_T_biased">8.3.1</a> and
</p>
<p>
\[|M_{n+1}-M_n|\leq |S_{n+1}-S_n|+|p-q|= 1+|p-q|.\]
</p>
<p>
To deduce the above equation we note that if \(T\leq n\) then \(M_n=M_{n+1}=M_T\), whereas if \(T&gt;n\) then \(M_n=S_n-n(p-q)\). Thus, condition (c) for the optional stopping theorem applies and \(\E
[M_T]=\E [M_0]\). That is,
</p>
<p>
\[\E [S_{T\wedge T}-(T\wedge T)(p-q)]=\E [S_{T\wedge 0}-(T\wedge 0)(p-q)]=\E [S_0-0(p-q)]=0.\]
</p>
<p>
We thus have
</p>
<p>
\[\E [S_T]=(p-q)\E [T].\]
</p>
<p>
as required.
</p>
</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> We have
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{12}\)</span>


<!--



                                                                                          E[ST ] = E[ST 1{T = Ta } + ST 1{T = Tb }]

                                                                                                        = E[a1{T = Ta } + b1{T = Tb }]

                                                                                                        = aP[T = Ta ] + bP[T = Tb ]
                                                                                                               (q/p)b − 1          1 − (q/p)a
                                                                                                        =a                   + b                 .
                                                                                                             (q/p)b − (q/p)a     (q/p)b − (q/p)a


-->


<p>


\begin{align*}
\E [S_T] &amp;=\E [S_T\1\{T=T_a\}+S_T\1\{T=T_b\}]\\ &amp;=\E [a\1\{T=T_a\}+b\1\{T=T_b\}]\\ &amp;=a\P [T=T_a]+b\P [T=T_b]\\ &amp;=a\frac {(q/p)^b-1}{(q/p)^b-(q/p)^a}+b\frac
{1-(q/p)^a}{(q/p)^b-(q/p)^a}.
\end{align*}
where in the last line we use equation <span class="textup" >(<a href="Hitting-probabilities-random-walks.html#eq:asym_rw_stopped">8.3</a>)</span>. This gives us
</p>
<p>
\[\E [T]=\frac {1}{p-q}\l (a\frac {(q/p)^b-1}{(q/p)^b-(q/p)^a}+b\frac {1-(q/p)^a}{(q/p)^b-(q/p)^a}\r ).\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_2.html#ps:urn_sec"><b>8.5</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> We have
</p>
<p>
\[T=\inf \{n&gt;0\-B_n=2\}.\]
</p>
<p>
Note that \(T\geq n\) if and only if \(B_i=1\) for all \(i=1,2,\ldots ,n-1\). That is, if and only if we pick a red ball out of the urn at times \(i-1,2,\ldots ,n-1\). Hence,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{12}\)</span>


<!--


                                                                                                                                12     n−1
                                                                                                             P[T ≥ n] =            ...
                                                                                                                                23      n
                                                                                                                                1
                                                                                                                               = .
                                                                                                                                n


-->


<p>


\begin{align*}
\P [T\geq n] &amp;=\frac {1}{2}\frac {2}{3}\ldots \frac {n-1}{n}\\ &amp;=\frac {1}{n}.
\end{align*}
Therefore, since \(\P [T=\infty ]\leq \P [T\geq n]\) for all \(n\), we have \(\P [T=\infty ]=0\) and \(\P [T&lt;\infty ]=1\).
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> We note that \(T\) is a stopping time, with respect to \(\mc {F}_t=\sigma (B_i\-i\leq n)\), since
</p>
<p>
\[\{T\leq n\}=\{\text {a red ball was drawn at some time }i\leq n\}=\bigcup \limits _{i=1}^n\{B_i=2\}.\]
</p>
<p>
We showed in Section <a href="Urn-processes.html#sec:urn">4.2</a> that \(M_n\) was a martingale. Since \(M_n\in [0,1]\) the process \((M_n)\) is bounded. By part (a) for any \(n\in \N \) we have \(\P
[T=\infty ]\leq \P [T\geq n]=\frac {1}{n}\), hence \(\P [T=\infty ]=0\) and \(\P [T&lt;\infty ]=1\). Hence, we have condition (b) of the optional stopping theorem, so
</p>
<p>
\[\E [M_T]=\E [M_0]=\frac 12.\]
</p>
<p>
By definition of \(T\) we have \(B_T=2\). Hence \(M_T=\frac {2}{T+2}\), so we obtain \(\E [\frac {1}{T+2}]=\frac 14\).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_2.html#ps:urn_moran_fixation"><b>8.6</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> We showed in exercise <a
href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:urn_moran"><b>7.8</b></a> that, almost surely, there was a point in time at which the urn contained either all red balls or all black balls.
Hence, \(\P [T&lt;\infty ]=1\).
</p>
<p>
Moreover, almost surely, since the urn eventually contains either all red balls or all black balls, we have either \(M_T=1\) (all red balls) or \(M_T=0\) (all black balls).
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> We note that \(T\) is a stopping time, since
</p>
<p>
\[\{T\leq n\}=\big \{\text {for some }i\leq n\text { we have }M_i\in \{0,1\}\big \}=\bigcup _{i=0}^n M_i^{-1}(\{0,1\}).\]
</p>
<p>
In exercise <a href="Exercises-on-Chapter-ref-chap-stoch_procs_1.html#ps:urn_moran"><b>7.8</b></a> we showed that \(M_n\) was a martingale. Since \(M_n\in [0,1]\) in fact \(M_n\) is a bounded
martingale, and we have \(\P [T&lt;\infty ]\) from above, so conditions (b) of the optional stopping theorem apply. Hence,
</p>
<p>
\[\E [M_T]=\E [M_0]=\frac {r}{r+b}.\]
</p>
<p>
Lastly, since \(M_T\) is either \(0\) or \(1\) (almost surely) we note that
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{12}\)</span>


<!--



                                                                                               E[MT ] = E[MT 1{MT =1} + MT 1{MT ]=0} ]

                                                                                                             = E[1{MT =1} ] + 0

                                                                                                             = P[MT = 1].



-->


<p>


\begin{align*}
\E [M_T] &amp;=\E [M_T\1_{\{M_T=1\}}+M_T\1_{\{M_T]=0\}}]\\ &amp;=\E [\1_{\{M_T=1\}}]+0\\ &amp;=\P [M_T=1].
\end{align*}
Hence, \(\P [M_T=1]=\frac {r}{r+b}\).
</p>
</li>
</ul>
</li>
<li>
<p>
<a href="Exercises-on-Chapter-ref-chap-stoch_procs_2.html#ps:urn_remove"><b>8.7</b></a>
</p>
<ul style="list-style-type:none">


<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(a)</span></span></span> We use the filtration \(\mc {F}_n=\sigma (N_i\-i\leq n)\). We have \(P_n\in m\mc {F}_n\) and
since \(0\leq P_n\leq 1\) we have also that \(P_n\in L^1\). Also,
</p>
<span class="hidden" > \(\seteqnumber{0}{A.}{12}\)</span>


<!--



                                                        E[Pn+1 | Fn ] = E[Pn+1 1{the nth ball was red} | Fn ] + E[Pn+1 1{the nth ball was blue} | Fn ]
                                                                            Nn − 1                                          Nn
                                                                                                                                                                     
                                                                     =E              1{the nth ball was red} Fn + E                 1{the nth ball was blue} Fn
                                                                         2m − n − 1                                     2m − n − 1
                                                                         Nn − 1                                         Nn
                                                                     =            E[1{the nth ball was red} | Fn ] +            E[1{the nth ball was blue} | Fn ]
                                                                       2m − n − 1                                    2m − n − 1
                                                                         Nn − 1     Nn          Nn       2m − n − Nn
                                                                     =                    +
                                                                       2m − n − 1 2m − n 2m − n − 1 2m − n
                                                                         Nn
                                                                     =
                                                                       2m − n
                                                                     = Pn .



-->


<p>


\begin{align*}
\E [P_{n+1}\|\mc {F}_n] &amp;=\E [P_{n+1}\1\{\text {the }n^{th}\text { ball was red}\}\|\mc {F}_n]+\E [P_{n+1}\1\{\text {the }n^{th}\text { ball was blue}\}\|\mc {F}_n]\\
&amp;=\E \l [\frac {N_n-1}{2m-n-1}\1\{\text {the }n^{th}\text { ball was red}\}\,\Big {|}\,\mc {F}_n\r ] +\E \l [\frac {N_n}{2m-n-1}\1\{\text {the }n^{th}\text { ball was
blue}\}\,\Big {|}\,\mc {F}_n\r ]\\ &amp;=\frac {N_n-1}{2m-n-1}\E [\1\{\text {the }n^{th}\text { ball was red}\}\|\mc {F}_n]+\frac {N_n}{2m-n-1}\E [\1\{\text {the }n^{th}\text {
ball was blue}\}\|\mc {F}_n]\\ &amp;=\frac {N_n-1}{2m-n-1}\frac {N_n}{2m-n}+\frac {N_n}{2m-n-1}\frac {2m-n-N_n}{2m-n}\\ &amp;=\frac {N_n}{2m-n}\\ &amp;=P_n.
\end{align*}
Here we use taking out what is known (since \(N_n\in m\mc {F}_n\)), along with the definition of our urn process to calculate e.g.&nbsp;\(\E [\1\{\text {the }n^{th}\text { ball was red}\}\|\mc
{F}_n]\) as a function of \(N_n\). Hence \((P_n)\) is a martingale.
</p>


</li>
<li>
<p>
<span class="textmd" ><span class="textrm" ><span class="textup" >(b)</span></span></span> Since \(0\leq P_n\leq 1\), the process \((P_n)\) is a bounded martingale. The time \(T\) is bounded
above by \(2m\), hence condition (b) for the optional stopping theorem holds and \(\E [P_T]=\E [P_0]\). Since \(P_0=\frac 12\) this gives us \(\E [P_T]=\frac 12\). Hence,
</p>
</li>
</ul>
<span class="hidden" > \(\seteqnumber{0}{A.}{12}\)</span>


<!--



                                                                              P[(T + 1)st ball is red] = P[NT +1 = NT + 1]
                                                                                                                     2m−1
                                                                                                                      X
                                                                                                                 =              P[NT +1 = NT + 1 | T = i]P[T = i]
                                                                                                                      i=1
                                                                                                                     2m−1
                                                                                                                      X         m−1
                                                                                                                 =                     P[T = i]
                                                                                                                         i=1
                                                                                                                                2m − i

                                                                                                                 = E[PT ]
                                                                                                                  1
                                                                                                                 = .
                                                                                                                  2


-->


<p>


\begin{align*}
\P [(T+1)^{st}\text { ball is red}] &amp;=\P [N_{T+1}=N_T+1]\\ &amp;=\sum \limits _{i=1}^{2m-1}\P [N_{T+1}=N_T+1\|T=i]\P [T=i]\\ &amp;=\sum \limits _{i=1}^{2m-1}\frac
{m-1}{2m-i}\P [T=i]\\ &amp;=\E [P_T]\\ &amp;=\frac 12.
\end{align*}
as required.
</p>
</li>
</ul>

</section>

</div>

</div>

<footer>

<p>
Copyright Nic Freeman, Sheffield University, last updated January 24, 2022
</p>

</footer>



<nav class="botnavigation" ><a href="notes_1.html" class="linkhome" >
Home</a></nav>

</body>
</html>
