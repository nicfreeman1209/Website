<!DOCTYPE html>
<html lang="en-US">
<head>
<meta charset="UTF-8" />
<meta name="author" content="Nic Freeman" />
<meta name="generator" content="LaTeX Lwarp package" />
<meta name="description" content="MAS352/61023 Stochastic Processes and Financial Mathematics, Sheffield University, April 22, 2025." />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<title>MAS352/61023 — Solutions to exercises (part one)</title>
<link rel="stylesheet" type="text/css" href="sans-serif-lwarp-sagebrush.css" />
<script>
// Lwarp MathJax emulation code
//
// Based on code by Davide P. Cervone.
// Equation numbering: https://github.com/mathjax/MathJax/issues/2427
// Starred and ifnextchar macros: https://github.com/mathjax/MathJax/issues/2428
// \left, \right delimiters: https://github.com/mathjax/MathJax/issues/2535
//
// Modified by Brian Dunn to adjust equation numbering and add subequations.
//
// LaTeX can use \seteqnumber{subequations?}{section}{number} before each equation.
// subequations? is 0 usually, 1 if inside subequations.
// section is a string printed as-is, or empty.
// number is auto-incremented by MathJax between equations.
//
MathJax = {
  subequations: "0",
  section: "",
  loader: {
    load: ['[tex]/tagformat', '[tex]/textmacros'],
  },
  startup: {
    ready() {
      // These would be replaced by import commands if you wanted to make
      // a proper extension.
      const Configuration = MathJax._.input.tex.Configuration.Configuration;
      const CommandMap = MathJax._.input.tex.SymbolMap.CommandMap;
      const Macro = MathJax._.input.tex.Symbol.Macro;
      const TexError = MathJax._.input.tex.TexError.default;
      const ParseUtil = MathJax._.input.tex.ParseUtil.default;
      const expandable = MathJax._.util.Options.expandable;

        // Insert the replacement string into the TeX string, and check
        // that there haven't been too many maxro substitutions (prevents
        // infinite loops).
        const useArgument = (parser, text) => {
          parser.string = ParseUtil.addArgs(parser, text, parser.string.slice(parser.i));
          parser.i = 0;
          if (++parser.macroCount > parser.configuration.options.maxMacros) {
            throw new TexError('MaxMacroSub1',
            'MathJax maximum macro substitution count exceeded; ' +
            'is there a recursive macro call?');
          }
        }

        // Create the command map for:
        //     \ifstar, \ifnextchar, \ifblank, \ifstrequal, \gsub, \seteqnumber
        new CommandMap('Lwarp-macros', {
          ifstar: 'IfstarFunction',
          ifnextchar: 'IfnextcharFunction',
          ifblank: 'IfblankFunction',
          ifstrequal: 'IfstrequalFunction',
          gsubstitute: 'GsubstituteFunction',
          seteqnumber: 'SeteqnumberFunction'
        }, {
          // This function implements an ifstar macro.
          IfstarFunction(parser, name) {
             const resultstar = parser.GetArgument(name);
             const resultnostar = parser.GetArgument(name);
             const star = parser.GetStar();                 // true if there is a *
             useArgument(parser, star ? resultstar : resultnostar);
          },

          // This function implements an ifnextchar macro.
          IfnextcharFunction(parser, name) {
            let whichchar = parser.GetArgument(name);
            if (whichchar.match(/^(?:0x[0-9A-F]+|[0-9]+)$/i)) {
              // $ syntax highlighting
              whichchar = String.fromCodePoint(parseInt(whichchar));
            }
            const resultnextchar = parser.GetArgument(name);
            const resultnotnextchar = parser.GetArgument(name);
            const gotchar = (parser.GetNext() === whichchar);
            useArgument(parser, gotchar ? resultnextchar : resultnotnextchar);
          },

          // This function implements an ifblank macro.
          IfblankFunction(parser, name) {
            const blankarg = parser.GetArgument(name);
            const resultblank = parser.GetArgument(name);
            const resultnotblank = parser.GetArgument(name);
            const isblank = (blankarg.trim() == "");
            useArgument(parser, isblank ? resultblank : resultnotblank);
          },

          // This function implements an ifstrequal macro.
          IfstrequalFunction(parser, name) {
            const strequalfirst = parser.GetArgument(name);
            const strequalsecond = parser.GetArgument(name);
            const resultequal = parser.GetArgument(name);
            const resultnotequal = parser.GetArgument(name);
            const isequal = (strequalfirst == strequalsecond);
            useArgument(parser, isequal ? resultequal : resultnotequal);
          },

          // This function implements a gsub macro.
          GsubstituteFunction(parser, name) {
            const gsubfirst = parser.GetArgument(name);
            const gsubsecond = parser.GetArgument(name);
            const gsubthird = parser.GetArgument(name);
            let gsubresult=gsubfirst.replace(gsubsecond, gsubthird);
            useArgument(parser, gsubresult);
          },

          // This function modifies the equation numbers.
          SeteqnumberFunction(parser, name) {
              // Get the macro parameters
              const star = parser.GetStar();                  // true if there is a *
              const optBrackets = parser.GetBrackets(name);   // contents of optional brackets
              const newsubequations = parser.GetArgument(name); // the subequations argument
              const neweqsection = parser.GetArgument(name); // the eq section argument
              const neweqnumber = parser.GetArgument(name);   // the eq number argument
              MathJax.config.subequations=newsubequations ;   // a string with boolean meaning
              MathJax.config.section=neweqsection ;           // a string with numeric meaning
              parser.tags.counter = parser.tags.allCounter = neweqnumber ;
          }

        });

        // Create the Lwarp-macros package
        Configuration.create('Lwarp-macros', {
          handler: {macro: ['Lwarp-macros']}
        });

        MathJax.startup.defaultReady();

        // For forward references:
        MathJax.startup.input[0].preFilters.add(({math}) => {
          if (math.inputData.recompile){
              MathJax.config.subequations = math.inputData.recompile.subequations;
              MathJax.config.section = math.inputData.recompile.section;
          }
        });
        MathJax.startup.input[0].postFilters.add(({math}) => {
          if (math.inputData.recompile){
              math.inputData.recompile.subequations = MathJax.config.subequations;
              math.inputData.recompile.section = MathJax.config.section;
          }
        });

          // For \left, \right with unicode-math:
          const {DelimiterMap} = MathJax._.input.tex.SymbolMap;
          const {Symbol} = MathJax._.input.tex.Symbol;
          const {MapHandler} = MathJax._.input.tex.MapHandler;
          const delimiter = MapHandler.getMap('delimiter');
          delimiter.add('\\lBrack', new Symbol('\\lBrack', '\u27E6'));
          delimiter.add('\\rBrack', new Symbol('\\rBrack', '\u27E7'));
          delimiter.add('\\lAngle', new Symbol('\\lAngle', '\u27EA'));
          delimiter.add('\\rAngle', new Symbol('\\rAngle', '\u27EB'));
          delimiter.add('\\lbrbrak', new Symbol('\\lbrbrak', '\u2772'));
          delimiter.add('\\rbrbrak', new Symbol('\\rbrbrak', '\u2773'));
          delimiter.add('\\lbag', new Symbol('\\lbag', '\u27C5'));
          delimiter.add('\\rbag', new Symbol('\\rbag', '\u27C6'));
          delimiter.add('\\llparenthesis', new Symbol('\\llparenthesis', '\u2987'));
          delimiter.add('\\rrparenthesis', new Symbol('\\rrparenthesis', '\u2988'));
          delimiter.add('\\llangle', new Symbol('\\llangle', '\u2989'));
          delimiter.add('\\rrangle', new Symbol('\\rrangle', '\u298A'));
          delimiter.add('\\Lbrbrak', new Symbol('\\Lbrbrak', '\u27EC'));
          delimiter.add('\\Rbrbrak', new Symbol('\\Rbrbrak', '\u27ED'));
          delimiter.add('\\lBrace', new Symbol('\\lBrace', '\u2983'));
          delimiter.add('\\rBrace', new Symbol('\\rBrace', '\u2984'));
          delimiter.add('\\lParen', new Symbol('\\lParen', '\u2985'));
          delimiter.add('\\rParen', new Symbol('\\rParen', '\u2986'));
          delimiter.add('\\lbrackubar', new Symbol('\\lbrackubar', '\u298B'));
          delimiter.add('\\rbrackubar', new Symbol('\\rbrackubar', '\u298C'));
          delimiter.add('\\lbrackultick', new Symbol('\\lbrackultick', '\u298D'));
          delimiter.add('\\rbracklrtick', new Symbol('\\rbracklrtick', '\u298E'));
          delimiter.add('\\lbracklltick', new Symbol('\\lbracklltick', '\u298F'));
          delimiter.add('\\rbrackurtick', new Symbol('\\rbrackurtick', '\u2990'));
          delimiter.add('\\langledot', new Symbol('\\langledot', '\u2991'));
          delimiter.add('\\rangledot', new Symbol('\\rangledot', '\u2992'));
          delimiter.add('\\lparenless', new Symbol('\\lparenless', '\u2993'));
          delimiter.add('\\rparengtr', new Symbol('\\rparengtr', '\u2994'));
          delimiter.add('\\Lparengtr', new Symbol('\\Lparengtr', '\u2995'));
          delimiter.add('\\Rparenless', new Symbol('\\Rparenless', '\u2996'));
          delimiter.add('\\lblkbrbrak', new Symbol('\\lblkbrbrak', '\u2997'));
          delimiter.add('\\rblkbrbrak', new Symbol('\\rblkbrbrak', '\u2998'));
          delimiter.add('\\lvzigzag', new Symbol('\\lvzigzag', '\u29D8'));
          delimiter.add('\\rvzigzag', new Symbol('\\rvzigzag', '\u29D9'));
          delimiter.add('\\Lvzigzag', new Symbol('\\Lvzigzag', '\u29DA'));
          delimiter.add('\\Rvzigzag', new Symbol('\\Rvzigzag', '\u29DB'));
          delimiter.add('\\lcurvyangle', new Symbol('\\lcurvyangle', '\u29FC'));
          delimiter.add('\\rcurvyangle', new Symbol('\\rcurvyangle', '\u29FD'));
          delimiter.add('\\Vvert', new Symbol('\\Vvert', '\u2980'));
    }     // ready
  },      // startup

  tex: {
    packages: {'[+]': ['tagformat', 'Lwarp-macros', 'textmacros']},
    tags: "ams",
         tagformat: {
             number: function (n) {
                 if(MathJax.config.subequations==0)
                     return(MathJax.config.section + n);
                 else
                     return(MathJax.config.section + String.fromCharCode(96+n));
             },
         },
  }
}
</script>

<script
    id="MathJax-script"
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"
></script>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-J4222H8D03"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-J4222H8D03');
</script>
<!-- Google tag (gtag.js) -->


</head>
<body>



<a id="notes_1-autopage-299"></a>
<nav class="topnavigation"><a href="notes_1.html" class="linkhome" >
Home</a></nav>

<header>

<p>
last updated: April 22, 2025
</p>

</header>



<div class="bodyandsidetoc">
<div class="sidetoccontainer">



<nav class="sidetoc">



<div class="sidetoctitle">

<p>
<span class="sidetocthetitle">Stochastic Processes and Financial Mathematics<br />
(part one)</span>
</p>

<p>
Contents
</p>
</div>



<div class="sidetoccontents">

<p>
<a href="notes_1.html" class="linkhome" >
Home</a>
</p>

</div>

</nav>

</div>



<main class="bodycontainer">



<section class="textbody">

<h1>Stochastic Processes and Financial Mathematics<br />
(part one)</h1>

<!--MathJax customizations:-->
<div data-nosnippet
      style="display:none"
>

\(\newcommand{\footnotename}{footnote}\)

\(\def \LWRfootnote {1}\)

\(\newcommand {\footnote }[2][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\newcommand {\footnotemark }[1][\LWRfootnote ]{{}^{\mathrm {#1}}}\)

\(\let \LWRorighspace \hspace \)

\(\renewcommand {\hspace }{\ifstar \LWRorighspace \LWRorighspace }\)

\(\newcommand {\mathnormal }[1]{{#1}}\)

\(\newcommand \ensuremath [1]{#1}\)

\(\newcommand {\LWRframebox }[2][]{\fbox {#2}} \newcommand {\framebox }[1][]{\LWRframebox } \)

\(\newcommand {\setlength }[2]{}\)

\(\newcommand {\addtolength }[2]{}\)

\(\newcommand {\setcounter }[2]{}\)

\(\newcommand {\addtocounter }[2]{}\)

\(\newcommand {\arabic }[1]{}\)

\(\newcommand {\number }[1]{}\)

\(\newcommand {\noalign }[1]{\text {#1}\notag \\}\)

\(\newcommand {\cline }[1]{}\)

\(\newcommand {\directlua }[1]{\text {(directlua)}}\)

\(\newcommand {\luatexdirectlua }[1]{\text {(directlua)}}\)

\(\newcommand {\protect }{}\)

\(\def \LWRabsorbnumber #1 {}\)

\(\def \LWRabsorbquotenumber &quot;#1 {}\)

\(\newcommand {\LWRabsorboption }[1][]{}\)

\(\newcommand {\LWRabsorbtwooptions }[1][]{\LWRabsorboption }\)

\(\def \mathchar {\ifnextchar &quot;\LWRabsorbquotenumber \LWRabsorbnumber }\)

\(\def \mathcode #1={\mathchar }\)

\(\let \delcode \mathcode \)

\(\let \delimiter \mathchar \)

\(\def \oe {\unicode {x0153}}\)

\(\def \OE {\unicode {x0152}}\)

\(\def \ae {\unicode {x00E6}}\)

\(\def \AE {\unicode {x00C6}}\)

\(\def \aa {\unicode {x00E5}}\)

\(\def \AA {\unicode {x00C5}}\)

\(\def \o {\unicode {x00F8}}\)

\(\def \O {\unicode {x00D8}}\)

\(\def \l {\unicode {x0142}}\)

\(\def \L {\unicode {x0141}}\)

\(\def \ss {\unicode {x00DF}}\)

\(\def \SS {\unicode {x1E9E}}\)

\(\def \dag {\unicode {x2020}}\)

\(\def \ddag {\unicode {x2021}}\)

\(\def \P {\unicode {x00B6}}\)

\(\def \copyright {\unicode {x00A9}}\)

\(\def \pounds {\unicode {x00A3}}\)

\(\let \LWRref \ref \)

\(\renewcommand {\ref }{\ifstar \LWRref \LWRref }\)

\( \newcommand {\multicolumn }[3]{#3}\)

\(\require {textcomp}\)

\(\newcommand {\intertext }[1]{\text {#1}\notag \\}\)

\(\let \Hat \hat \)

\(\let \Check \check \)

\(\let \Tilde \tilde \)

\(\let \Acute \acute \)

\(\let \Grave \grave \)

\(\let \Dot \dot \)

\(\let \Ddot \ddot \)

\(\let \Breve \breve \)

\(\let \Bar \bar \)

\(\let \Vec \vec \)

\(\DeclareMathOperator {\var }{var}\)

\(\DeclareMathOperator {\cov }{cov}\)

\(\def \ra {\Rightarrow }\)

\(\def \to {\rightarrow }\)

\(\def \iff {\Leftrightarrow }\)

\(\def \sw {\subseteq }\)

\(\def \wt {\widetilde }\)

\(\def \mc {\mathcal }\)

\(\def \mb {\mathbb }\)

\(\def \sc {\setminus }\)

\(\def \v {\textbf }\)

\(\def \p {\partial }\)

\(\def \E {\mb {E}}\)

\(\def \P {\mb {P}}\)

\(\def \R {\mb {R}}\)

\(\def \C {\mb {C}}\)

\(\def \N {\mb {N}}\)

\(\def \Q {\mb {Q}}\)

\(\def \Z {\mb {Z}}\)

\(\def \B {\mb {B}}\)

\(\def \~{\sim }\)

\(\def \-{\,;\,}\)

\(\def \|{\,|\,}\)

\(\def \qed {$\blacksquare $}\)

\(\def \1{\unicode {x1D7D9}}\)

\(\def \cadlag {c\&grave;{a}dl\&grave;{a}g}\)

\(\def \p {\partial }\)

\(\def \l {\left }\)

\(\def \r {\right }\)

\(\def \F {\mc {F}}\)

\(\def \G {\mc {G}}\)

\(\def \H {\mc {H}}\)

\(\def \Om {\Omega }\)

\(\def \om {\omega }\)

</div>

<!--
......     chapter Solutions to exercises (part one) ......
-->
<h3 id="autosec-300">Chapter&nbsp;<span class="sectionnumber">A&#x2003;</span>Solutions to exercises (part one)</h3>
<a id="notes_1-autopage-300"></a>
<a id="notes_1-autofile-48"></a>

<a id="sec:solutions_1"></a>
<!--
......    subsection Chapter <a href=notes_1.html#??>??</a> ......
-->
<h5 id="autosec-301">Chapter <a href="notes_1.html#??">??</a></h5>
<a id="notes_1-autopage-301"></a>



<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> At time \(1\) we would have \(10(1+r)\) in cash and \(5\) units of stock. This gives the value of our assets at time \(1\) as
\(10(1+r)+5S_1\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> Assuming we don’t buy or sell anything at time \(0\), the value of our portfolio at time \(1\) will be \(x(1+r)+yS_1\), where
\(S_1\) is as in the solution to <a href="notes_1.html#??">??</a>. Since we need to be certain of paying off our debt, we should assume a worst case scenario for \(S_1\), that is \(S_1=sd\). So, we are certain
to pay off our debt if and only if
</p>
<p>
\[x(1+r)+ysd \geq K.\]
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Since certainty requires that we cover the worst case scenario of our stock performance, and \(d&lt;1+r\), our best strategy is to
exchange all our assets for cash at time \(t=0\). This gives us \(x+ys\) in cash at time \(0\). At time \(1\) we would then have \((1+r)(x+ys)\) in cash at time \(0\) and could pay our debt providing that
</p>
<p>
\[(1+r)(x+ys) \geq K.\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We borrow \(s\) cash at time \(t=0\) and use it to buy one stock. Then wait until time \(t=1\), at which point we will a stock
worth \(S_1\), where \(S_1\) is as in <a href="notes_1.html#??">??</a>. We sell our stock, which gives us \(S_1\) in cash. Since \(S_1\geq sd&gt;s(1+r)\), we repay or debt (plus interest) which costs us
\(s(1+r)\). We then have
</p>
<p>
\[S_1-s(1+r)&gt;0\]
</p>
<p>
in cash. This is an arbitrage.
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> We perform the same strategy as in (a), but with the roles of cash an stock swapped: We borrow \(1\) stock at time \(t=0\) and
then sell it for \(s\) in cash. Then wait until time \(t=1\), at which point we will have \(s(1+r)\) in cash. Since the price of a stock is now \(S_1\), where \(S_1\) is as in <a href="notes_1.html#??">??</a>,
and \(S_1\leq us&lt;(1+r)s\), we now buy one stock costing \(S_1\) and repay the stockbroker, leaving us with
</p>
<p>
\[s(1+r)-S_1&gt;0\]
</p>
<p>
in cash. This is an arbitrage.
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We can calculate, using integration by parts, that
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{0}\)</span>


<!--

                                                                                           Z ∞                    Z ∞
                                                                                                                                        1
                                                                                  E[X] =         xfX (x) dx =             xλe−λx dx =     .
                                                                                            −∞                     0                    λ


-->


<p>


\begin{align*}
\E [X]=\int _{-\infty }^\infty x f_X(x)\,dx=\int _0^\infty x\lambda e^{-\lambda x}\,dx=\frac {1}{\lambda }.
\end{align*}
Similarly, we can calculate that
</p>
<p>
\[ \E [X^2]=\int _{-\infty }^\infty x^2 f_X(x)\,dx=\int _0^\infty x^2\lambda e^{-\lambda x}\,dx=\frac {2}{\lambda ^2} \]
</p>
<p>
This gives that
</p>
<p>
\[\var (X)=\E [X^2]-\E [X]^2=\frac {1}{\lambda ^2}.\]
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We can calculate
</p>
<p>
\[\E [X_n]=0\cdot \P [X_n=0]+n^2\P [X_n=n^2]=\frac {n^2}{n}=n\to \infty \]
</p>
<p>
as \(n\to \infty \). Also,
</p>
<p>
\[\P [|X_n|&gt;0]=\P [X_n=n^2]=\frac {1}{n}\to 0\]
</p>
<p>
as \(n\to \infty \).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{0}\)</span>


<!--


                                                                                                           X −µ
                                                                                                                         
                                                                                      P[Y ≤ y] = P              ≤y
                                                                                                             σ
                                                                                                 = P[X ≤ µ + yσ]
                                                                                                     Z µ+yσ
                                                                                                                  1           (x−µ)2
                                                                                                 =            √           e− 2σ2 dx.
                                                                                                      −∞          2πσ 2


-->


<p>


\begin{align*}
\P [Y\leq y] &amp;=\P \l [\frac {X-\mu }{\sigma }\leq y\r ]\\ &amp;=\P [X\leq \mu +y\sigma ]\\ &amp;=\int _{-\infty }^{\mu +y\sigma }\frac {1}{\sqrt {2\pi \sigma ^2}}e^{-\frac
{(x-\mu )^2}{2\sigma ^2}}\,dx.
\end{align*}
We want to turn the above integral into the distribution function of the standard normal. To do so, substitute \(z=\frac {x-\mu }{\sigma }\), and we obtain
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{0}\)</span>


<!--

                                                                                                       Z y
                                                                                                              1      z2
                                                                                          P[Y ≤ y] =       √      e− 2 σ dz
                                                                                                        −∞
                                                                                                       Z y
                                                                                                            2πσ 2
                                                                                                            1     z2
                                                                                                     =     √ e− 2 dz.
                                                                                                        −∞  2π


-->


<p>


\begin{align*}
\P [Y\leq y]&amp;=\int _{-\infty }^y\frac {1}{\sqrt {2\pi \sigma ^2}}e^{-\frac {z^2}{2}}\sigma \,dz\\ &amp;=\int _{-\infty }^y \frac {1}{\sqrt {2\pi }}e^{-\frac {z^2}{2}}\,dz.
\end{align*}
Hence, \(Y\) has the distribution function of \(N(0,1)\), which means \(Y\sim N(0,1)\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> For \(p\neq 1\),
</p>
<p>
\[\int _1^\infty x^{-p}\,dx = \l [\frac {x^{-p+1}}{-p+1}\r ]_{x=1}^\infty \]
</p>
<p>
and we obtain a finite answer only if \(-p+1&lt;0\). When \(p=1\), we have
</p>
<p>
\[\int _1^\infty x^{-1}\,dx = \l [\log x\r ]_{x=1}^\infty =\infty ,\]
</p>
<p>
so we conclude that \(\int _1^\infty x^{-p}\,dx\) is finite if and only if \(p&gt;1\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> As \(n\to \infty \),
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">•</span> \(e^{-n}\to 0\) because \(e&gt;1\).
</p>


</li>
<li>


<p>
<span class="listmarker">•</span> \(\sin \l (\frac {n\pi }{2}\r )\) oscillates through \(0,1,0,-1\) and does not converge.
</p>


</li>
<li>


<p>
<span class="listmarker">•</span> \(\frac {\cos (n\pi )}{n}\) converges to zero by the sandwich rule since \(|\frac {\cos (n\pi )}{n}|\leq \frac {1}{n}\to 0\).
</p>


</li>
<li>


<p>
<span class="listmarker">•</span> \(\sum \limits _{i=1}^n 2^{-i}=1-2^{-n}\) is a geometric series, and \(1-2^{-n}\to 1-0=1\).
</p>


</li>
<li>


<p>
<span class="listmarker">•</span> \(\sum \limits _{i=1}^n\frac {1}{i}\) tends to infinity, because
</p>
<p>
\[\overbrace {\frac 12}+\overbrace {\frac 13+\frac 14}+\overbrace {\frac 15+\frac 16+\frac 17+\frac 18}+\overbrace {\frac 19+\frac {1}{10}++\frac {1}{11}+\frac {1}{12}+\frac
{1}{13}+\frac {1}{14}+\frac {1}{15}+\frac {1}{16}}+\ldots .\]
</p>
<p>
each term contained in a \(\overbrace {\cdot }\) is greater than or equal to \(\frac 12\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Recall that \(\sum _{n=1}^\infty n^{-2}&lt;\infty \). Define \(n_1=1\) and then
</p>
<p>
\[n_{r+1}=\inf \{k\in \N \-k&gt;n_r\text { and }|x_k|\leq r^{-2}\}.\]
</p>
<p>
Then for all \(r\) we have \(|x_{n_r}|\leq r^{-2}\). Hence,
</p>
<p>
\[\sum \limits _{r=1}^\infty \l |x_{n_r}\r |\leq \sum \limits _{r=1}^\infty r^{-2}&lt;\infty .\]
</p>
<p>


</p>
</li>
</ul>
<!--
......   subsection Chapter <a href=notes_1.html#??>??</a> ......
-->
<h5 id="autosec-302">Chapter <a href="notes_1.html#??">??</a></h5>
<a id="notes_1-autopage-302"></a>



<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> The result of rolling a pair of dice can be written \(\omega =(\omega _1,\omega _2)\) where \(\omega _1,\omega _2\in
\{1,2,3,4,5,6\}\). So a suitable \(\Omega \) would be
</p>
<p>
\[\Omega =\Big \{(\omega _1,\omega _2)\-\omega _1,\omega _2\in \{1,2,3,4,5,6\}\Big \}.\]
</p>
<p>
Of course other choices are possible too. Since our choice of \(\Omega \) is finite, a suitable \(\sigma \)-field is \(\mc {F}=\mc {P}(\Omega )\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> Consider the case of \(\mc {F}\). We need to check the three properties in the definition of a \(\sigma \)-field.
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> We have \(\emptyset \in \mc {F}\) so the first property holds automatically.
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> For the second we check compliments: \(\Omega \sc \Omega =\emptyset \), \(\Omega \sc \{1\}=\{2,3\}\), \(\Omega \sc \{2,3\}=\{1\}\) and \(\Omega \sc
\Omega =\emptyset \); in all cases we obtain an element of \(\mc {F}\).
</p>


</li>
<li>


<p>
<span class="listmarker">3.</span> For the third, we check unions. We have \(\{1\}\cup \{2,3\}=\Omega \). Including \(\emptyset \) into a union doesn’t change anything; including \(\Omega \) into a
union results in \(\Omega \). This covers all possible cases, in each case resulting in an element of \(\mc {F}\).
</p>
</li>
</ul>
<p>
So, \(\mc {F}\) is a \(\sigma \)-field. Since \(\mc {F}&apos;\) is just \(\mc {F}\) with the roles of \(1\) and \(2\) swapped, by symmetry \(\mc {F}&apos;\) is also a \(\sigma \)-field.
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> We have \(\{1\}\in \mc {F}\cup \mc {F}&apos;\) and \(\{2\}\in \mc {F}\cup \mc {F}&apos;\), but \(\{1\}\cup
\{2\}=\{1,2\}\) and \(\{1,2\}\notin \mc {F}\cup \mc {F}&apos;\). Hence \(\mc {F}\cup \mc {F}&apos;\) is not closed under unions; it fails property 3 of the definition of a \(\sigma \)-field.
</p>
<p>
However, \(\mc {F}\cap \mc {F}&apos;\) is the intersection of two \(\sigma \)-fields, so is automatically itself a \(\sigma \)-field. (Alternatively, note that \(\mc {F}\cap \mc {F}&apos;=\{\emptyset
,\Omega \}\), and check the definition.)
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> Let \(A_m\) be the event that the sequence \((X_n)\) contains precisely \(m\) heads. Let \(A_{m,k}\) be the event that we see
precisely \(m\) heads during the first \(k\) tosses and, from then on, only tails. Then,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{0}\)</span>


<!--



                                                                          P[Am,k ] = P[m heads in X1 , . . . , Xk , no heads in Xk+1 , Xk+2 , . . .]

                                                                                   = P[m heads in X1 , . . . , Xk , ] P[ no heads in Xk+1 , Xk+2 , . . .]

                                                                                   = P[m heads in X1 , . . . , Xk , ] × 0

                                                                                   = 0.



-->


<p>


\begin{align*}
\P [A_{m,k}] &amp;=\P [m\text { heads in }X_1,\ldots ,X_k,\text { no heads in }X_{k+1},X_{k+2},\ldots ]\\ &amp;=\P [m\text { heads in }X_1,\ldots ,X_k,]\,\P [\text { no heads in
}X_{k+1},X_{k+2},\ldots ]\\ &amp;=\P [m\text { heads in }X_1,\ldots ,X_k,]\times 0\\ &amp;=0.
\end{align*}
If \(A_m\) occurs then at least one of \(A_{m,1},A_{m,2},\ldots \) occurs. Hence,
</p>
<p>
\[\P [A_m]\leq \sum \limits _{k=0}^\infty \P [A_{m,k}]=0\]
</p>
<p>
.
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Let \(A\) be the event that the sequence \((X_n)\) contains finitely many heads. Then, if \(A\) occurs, precisely one of
\(A_1,A_2,\ldots \) occurs. Hence,
</p>
<p>
\[\P [A]=\sum \limits _{m=0}^\infty \P [A_m]=0.\]
</p>
<p>
That is, the probability of having only finite many heads is zero. Hence, almost surely, the sequence \((X_n)\) contains infinitely many heads.
</p>
<p>
By symmetry (exchange the roles of heads and tails) almost surely the sequence \((X_n)\) also contains infinitely many tails.
</p>
</li>
</ul>
</li>
</ul>

<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> \(\F _1\) contains the information of whether a \(6\) was rolled.
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> \(\F _2\) gives the information of which value was rolled if a \(1\), \(2\) or \(3\) was rolled, but if the roll was in \(\{4,5,6\}\)
then \(\F _1\) cannot tell the difference between these values.
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(c)</span></span> \(\F _3\) contains the information of which of the sets \(\{1,2\}\), \(\{3,4\}\) and \(\{5,6\}\) contains the value rolled, but
gives no more information than that.
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> The values taken by \(X_1\) are
</p>
<p>
\[X_1(\omega )= \begin {cases} 0 &amp; \text { if }\omega =3,\\ 1 &amp; \text { if }\omega =2,4,\\ 4 &amp; \text { if }\omega =1,5.\\ \end {cases} \]
</p>
<p>
Hence, the pre-images are \(X_1^{-1}(0)=\{3\}\), \(X_1^{-1}(1)=\{2,4\}\) and \(X_1^{-1}(4)=\{1,5\}\). These are all elements of \(\G _1\), so \(X_1\in m\G _1\). However, they are not all elements of
\(\G _2\) (in fact, none of them are!) so \(X_1\notin m\G _2\).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Define
</p>
<p>
\[X_2(\omega )= \begin {cases} 0 &amp; \text { if }\omega =1,2,\\ 1 &amp; \text { otherwise}.                              \end {cases} \]
</p>
<p>
Then the pre-images are \(X_2^{-1}(0)=\{1,2\}\) and \(X_2^{-1}(1)=\{3,4,5\}\). Hence \(X_2\in m\G _2\).
</p>
<p>
It isn’t possible to use unions/intersections/complements to make the set \(\{3,4,5\}\) out of \(\{1,5\},\{2,4\},\{3\}\) (one way to see this is note that as soon as we tried to include \(5\) we would also have
to include \(1\), meaning that we can’t make \(\{3,4,5\}\)). Hence \(X_2\notin m\G _1\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We have \(X(TT)=0\), \(X(HT)=X(TH)=1\) and \(X(HH)=2\). This gives
</p>
<p>
\[\sigma (X)=\Big \{\emptyset , \{TT\}, \{TH, HT\}, \{HH\}, \{TT, TH, HT\}, \{TT, HH\}, \{TH, HT, HH\}, \Omega \Big \}.\]
</p>
<p>
To work this out, you could note that \(X^{-1}(i)\) must be in \(\sigma (X)\) for \(i=0,1,2\), then also include \(\emptyset \) and \(\Omega \), then keep adding unions and complements of the events until
find you have added them all. (Of course, this only works because \(\sigma (X)\) is finite!)
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Since sums, divisions and products of random variables are random variables, \(X^2+1\) is a random variable. Since \(X^2+1\) is
non-zero, we have that \(\frac {1}{X^2+1}\) is a random variable, and using products again, \(\frac {X}{X^2+1}\) is a random variable.
</p>
<p>
For \(\sin (X)\), recall that for any \(x\in \R \) we have
</p>
<p>
\[\sin (x)=\sum _{n=0}^\infty \frac {(-1)^n}{(2n+1)!}x^{2n+1}.\]
</p>
<p>
Since \(X\) is a random variable so is \(\frac {(-1)^n}{(2n+1)!}X^{2n+1}\). Since limits of random variables are also random variables, so is
</p>
<p>
\[\sin X=\lim \limits _{N\to \infty }\sum _{n=0}^N\frac {(-1)^n}{(2n+1)!}X^{2n+1}.\]
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Since \(X\geq 0\) we have \(\E [X^p]=\E [|X|^p]\) for all \(p\).
</p>
<p>
\[\E [X]=\int _1^\infty x^12x^{-3}\,dx=2\int _1^\infty x^{-2}\,dx=2\l [\frac {x^{-1}}{-1}\r ]_1^\infty =2&lt;\infty \]
</p>
<p>
so \(X\in L^1\), but
</p>
<p>
\[\E [X^2]=\int _1^\infty x^22x^{-3}\,dx=2\int _1^\infty x^{-1}\,dx=2\l [\log x\r ]_1^\infty =\infty \]
</p>
<p>
so \(X\notin L^2\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Let \(1\leq p\leq q&lt;\infty \) and \(X\in L^q\). Our plan is to generalize the argument that we used to prove <span
class="textup">(<a href="notes_1.html#??">??</a>)</span>.
</p>
<p>
First, we condition to see that
</p>
<p>
\[|X|^p=|X|^p\1\{|X|\leq 1\}+|X|^p\1\{|X|&gt;1\}.\]
</p>
<p>
Hence,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{0}\)</span>


<!--



                                                                                     E[|X|p ] = E [|X|p 1{|X| ≤ 1} + |X|p 1{|X| > 1}]

                                                                                              ≤ E [1 + |X|q ]

                                                                                              = 1 + E|X|q ] < ∞



-->


<p>


\begin{align*}
\E [|X|^p] &amp;=\E \l [|X|^p\1\{|X|\leq 1\}+|X|^p\1\{|X|&gt;1\}\r ]\\ &amp;\leq \E \l [1+|X|^q\r ]\\ &amp;=1+\E |X|^q]&lt;\infty
\end{align*}
Here we use that, if \(|X|\leq 1\) then \(|X|^p\leq 1\), and if \(|X|&gt;1\) then since \(p\leq q\) we have \(|X|^p&lt;|X|^q\). So \(X\in L^p\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> First, we condition to see that
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{0}\)</span>
<!--


                                                X = X 1{X<a} + X 1{X≥a} .                                                           (A.1)                                 --><a id="eq:markovs_pre"></a><!--

-->
<p>


\begin{equation}
\label {eq:markovs_pre} X=X\1_{\{X&lt; a\}}+X\1_{\{X\geq a\}}.
\end{equation}


</p>
<p>
From <span class="textup">(<a href="notes_1.html#??">??</a>)</span> since \(X\geq 0\) we have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                                       X ≥ X 1{X≥a}

                                                                                                          ≥ a1{X ≥ a}.



-->


<p>


\begin{align*}
X &amp;\geq X\1_{\{X\geq a\}}\\ &amp;\geq a\1\{X\geq a\}.
\end{align*}
This second inequality follows by looking at two cases: if \(X&lt;a\) then both sides are zero, but if \(X\geq a\) then we can use that \(X\geq a\). Using monotonicity of \(\E \), we then have
</p>
<p>
\[\E [X]\geq \E [a\1_{\{X\geq a\}}]=a\E [1_{\{X\geq a\}}]=a\P [X\geq a].\]
</p>
<p>
Dividing through by \(a\) finishes the proof.
</p>
<p>
<i>This result is known as Markov’s inequality.</i>
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Since \(X\geq 0\), Markov’s inequality (from <a href="notes_1.html#??">??</a>) gives us that
</p>
<p>
\[\P [X\geq a]\leq \frac {1}{a}\E [X]=0\]
</p>
<p>
for any \(a&gt;0\). Hence,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                     P[X > 0] = P[X > 1] + P[1 ≥ X > 0]
                                                                                                                   "∞                             #
                                                                                                                    [n                           o
                                                                                                                                1        1
                                                                                                = P[X > 1] + P                  n ≥ X > n+1
                                                                                                                    n=1
                                                                                                                 ∞
                                                                                                                 X         h                 i
                                                                                                                               1        1
                                                                                                = P[X > 1] +           P       n ≥ X > n+1
                                                                                                                n=1
                                                                                                                X∞         h          i
                                                                                                                              1
                                                                                                ≤ P[X ≥ 1] +           P X > n+1
                                                                                                                 n=1

                                                                                                = 0.



-->


<p>


\begin{align*}
\P [X&gt;0]&amp;=\P [X&gt;1]+\P [1\geq X&gt;0]\\ &amp;=\P [X&gt;1]+\P \l [\bigcup \limits _{n=1}^\infty \l \{\tfrac {1}{n}\geq X&gt;\tfrac {1}{n+1}\r \}\r ]\\ &amp;=\P
[X&gt;1]+\sum \limits _{n=1}^\infty \P \l [\tfrac {1}{n}\geq X&gt;\tfrac {1}{n+1}\r ]\\ &amp;\leq \P [X\geq 1]+\sum \limits _{n=1}^\infty \P \l [X&gt;\tfrac {1}{n+1}\r ]\\
&amp;=0.
\end{align*}


</p>
</li>
</ul>
<!--
......   subsection Chapter <a href=notes_1.html#??>??</a> ......
-->
<h5 id="autosec-303">Chapter <a href="notes_1.html#??">??</a></h5>
<a id="notes_1-autopage-303"></a>



<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We have \(S_2=X_1+X_2\) so
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                             E[S2 | σ(X1 )] = E[X1 | σ(X1 )] + E[X2 | σ(X1 )]

                                                                                                             = X1 + E[X2 ]

                                                                                                             = X1 .



-->


<p>


\begin{align*}
\E [S_2\|\sigma (X_1)] &amp;=\E [X_1\|\sigma (X_1)]+\E [X_2\|\sigma (X_1)]\\ &amp;=X_1+\E [X_2]\\ &amp;=X_1.
\end{align*}
Here, we use the linearity of conditional expectation, followed by the fact that \(X_1\) is \(\sigma (X_1)\) measurable and \(X_2\) is independent of \(\sigma (X_1)\) with \(\E [X_2]=0\).
</p>
<p>
Secondly, \(S_2^2=X_1^2+2X_1X_2+X_2^2\) so
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                  E[S22 ] = E[X12 | σ(X1 )] + 2E[X1 X2 | σ(X1 )] + E[X22 | σ(X1 )]

                                                                                             = X12 + 2X1 E[X2 | σ(X1 )] + E[X22 ]

                                                                                             = X12 + 1



-->


<p>


\begin{align*}
\E [S_2^2] &amp;=\E [X_1^2\|\sigma (X_1)]+2\E [X_1X_2\|\sigma (X_1)]+\E [X_2^2\|\sigma (X_1)]\\ &amp;=X_1^2+2X_1\E [X_2\|\sigma (X_1)]+\E [X_2^2]\\ &amp;=X_1^2+1
\end{align*}
Here, we again use linearity of conditional expectation to deduce the first line. To deduce the second line, we use that \(X_1^2\) and \(X_1\) are \(\sigma (X_1)\) measurable (using the taking out what is known
rule for the middle term), whereas \(X_2\) is independent of \(\sigma (X_1)\). The final line comes from \(\E [X_2\|\sigma (X_1)]=0\) (which we already knew from above) and that \(\E [X_2^2]=1\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> For \(i\leq n\), we have \(X_i\in m\mc {F}_n\), so \(S_n=X_1+X_2+\ldots +X_n\) is also \(\mc {F}_n\) measurable. For each
\(i\) we have \(|X_i|\leq 2\) so \(|S_n|\leq 2n\), hence \(S_n\in L^1\). Lastly,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                       E[Sn+1 | Fn ] = E[X1 + . . . + Xn | Fn ] + E[Xn+1 | Fn ]

                                                                                                         = (X1 + . . . + Xn ) + E[Xn+1 ]

                                                                                                         = Sn .



-->


<p>


\begin{align*}
\E [S_{n+1}\|\mc {F}_n] &amp;=\E [X_1+\ldots +X_n\|\mc {F}_n]+\E [X_{n+1}\|\mc {F}_n]\\ &amp;=(X_1+\ldots +X_n) + \E [X_{n+1}]\\ &amp;=S_n.
\end{align*}
Here, we use linearity of conditional expectation in the first line. To deduce the second, we use that \(X_i\in m\mc {F}_n\) for \(i\leq n\) and that \(X_{n+1}\) is independent of \(\mc {F_n}\) in the second.
For the final line we note that \(\E [X_{n+1}]=\frac 132+\frac 23(-1)=0\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> Since \(X_i\in m\mc {F}_n\) for all \(i\leq n\), we have \(M_n=X_1X_2\ldots X_n\in m\mc {F}_n\). Since \(|X_i|\leq c\) for
all \(i\), we have \(|M_n|\leq c^n&lt;\infty \), so \(M_n\) is bounded and hence \(M_n\in L^1\) for all \(n\). Lastly,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                               E[Mn+1 | Fn ] = E[X1 X2 . . . Xn Xn+1 | Fn ]

                                                                                                                  = X1 . . . Xn E[Xn+1 | Fn ]

                                                                                                                  = X1 . . . Xn E[Xn+1 ]

                                                                                                                  = X1 . . . Xn

                                                                                                                  = Mn .



-->


<p>


\begin{align*}
\E [M_{n+1}\|\mc {F}_n] &amp;=\E [X_1X_2\ldots X_nX_{n+1}\|\mc {F}_n]\\ &amp;=X_1\ldots X_n\E [X_{n+1}\|\mc {F}_n]\\ &amp;=X_1\ldots X_n\E [X_{n+1}]\\ &amp;=X_1\ldots X_n\\
&amp;=M_n.
\end{align*}
Here, to deduce the second line we use that \(X_i\in m\mc {F}_n\) for \(i\leq n\). To deduce the third line we use that \(X_{n+1}\) is independent of \(\mc {F}_n\) and then to deduce the forth line we use
that \(\E [X_{n+1}]=1\).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> By definition of conditional expectation (Theorem <a href="notes_1.html#??">??</a>), we have \(M_n\in L^1\) and \(M_n\in
m\mc {G}_n\) for all \(n\). It remains only to check that
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                                   E[Mn+1 | Gn ] = E[E[Z | Gn+1 ] | Gn ]

                                                                                                                     = E[Z | Gn ]

                                                                                                                     = Mn .



-->


<p>


\begin{align*}
\E [M_{n+1}\|\mc {G}_n] &amp;=\E [\E [Z\|\mc {G}_{n+1}]\|\mc {G}_n]\\ &amp;=\E [Z\|\mc {G}_n]\\ &amp;=M_n.
\end{align*}
Here, to get from the first to the second line we use the tower property.
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Note that the first and second conditions in Definition <a href="notes_1.html#??">??</a> are the same for super/sub-martingales as
for martingales. Hence, \(M\) satisfies these conditions. For the third condition, we have that \(\E [M_{n+1}\|\mc {F}_n]\leq M_n\) (because \(M\) is a supermartingale) and \(\E [M_{n+1}\|\mc {F}_n]\geq
M_n\) (because \(M\) is a submartingale. Hence \(\E [M_{n+1}\|\mc {F}_n]=M_n\), so \(M\) is a martingale.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> If \(m=n\) then \(\E [M_n\|\mc {F}_n]=M_n\) because \(M_n\) is \(\mc {F}_n\) measurable. For \(m&gt;n\), we have
</p>
<p>
\[\E [M_m|\mc {F}_n]=\E [\E [M_m\|\mc {F}_{m-1}]\|\mc {F}_n]=\E [M_{m-1}\|\mc {F}_n].\]
</p>
<p>
Here we use the tower property to deduce the first equality (note that \(m-1\geq n\) so \(\mc {F}_{m-1}\supseteq \mc {F}_n\)) and the fact that \((M_n)\) is a martingale to deduce the second inequality
(i.e.&nbsp;\(\E [M_{m}\|\mc {F}_{m-1}]=M_{m-1}\)). Iterating, from \(m,m-1\ldots ,n+1\) we obtain that
</p>
<p>
\[\E [M_m\|\mc {F}_n]=\E [M_{n+1}\|\mc {F}_n]\]
</p>
<p>
and the martingale property then gives that this is equal to \(M_n\), as required.
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> If \((M_n)\) is a submartingale then \(\E [M_m\|\mc {F}_n]\geq M_n\), whereas if \((M_n)\) is a supermartingale then \(\E
[M_m\|\mc {F}_n]\leq M_n\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We have \((M_{n+1}-M_n)^2=M_{n+1}^2-2M_{n+1}M_n+M_n^2\) so
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--


                                                                            h                         i
                                                                        E (Mn+1 − Mn )2 | Fn = E[Mn+1
                                                                                                  2
                                                                                                      | Fn ] − 2E[Mn+1 Mn | Fn ] + E[Mn2 | Fn ]
                                                                                                               2
                                                                                                          = E[Mn+1 | Fn ] − 2Mn E[Mn+1 | Fn ] + Mn2
                                                                                                               2
                                                                                                          = E[Mn+1 | Fn ] − 2Mn2 + Mn2
                                                                                                               2
                                                                                                          = E[Mn+1 | Fn ] − Mn2



-->


<p>


\begin{align*}
\E \l [(M_{n+1}-M_n)^2\|\mc {F}_n\r ] &amp;=\E [M_{n+1}^2\|\mc {F}_n]-2\E [M_{n+1}M_n\|\mc {F}_n]+\E [M_n^2\|\mc {F}_n]\\ &amp;=\E [M_{n+1}^2\|\mc {F}_n]-2M_n\E [M_{n+1}\|\mc
{F}_n]+M_n^2\\ &amp;=\E [M_{n+1}^2\|\mc {F}_n]-2M_n^2+M_n^2\\ &amp;=\E [M_{n+1}^2\|\mc {F}_n]-M_n^2
\end{align*}
as required. Here, we use the taking out what is known rule (since \(M_n\in m\mc {F}_n\)) and the martingale property of \((M_n)\).
</p>
<p>
It follows that \(\E [M_{n+1}^2\|\mc {F}_n]-M_n^2=\E [(M_{n+1}-M_n)^2\|\mc {F}_n]\geq 0\). We have \(M_n^2\in m\mc {F}_n\) and since \(M_n\in L^2\) we have \(M_n^2\in L^1\). Hence
\((M_n^2)\) is a submartingale.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Using that \(\E [X_{n+1}\|\mc {F}_n]=aX_n+bX_{n-1}\) we can calculate
</p>
<p>
\[\E [S_{n+1}\|\mc {F}_n]=\E [\alpha X_{n+1}+X_{n}\|\mc {F}_n]=\alpha (aX_n+bX_{n-1})+X_n=(\alpha a+1)X_n+\alpha b X_{n-1}.\]
</p>
<p>
We want this to be equal to \(S_n\), and \(S_n=\alpha X_n+X_{n-1}\). So we need
</p>
<p>
\[\alpha a+1=\alpha \hspace {1pc}\text { and }\hspace {1pc}\alpha b = 1.\]
</p>
<p>
Hence, \(\alpha =\frac {1}{b}\) is our choice. It is then easy to check that
</p>
<p>
\[\E [S_{n+1}\|\mc {F}_n]=\tfrac {1}{b}(aX_n+bX_{n-1})+X_n=(\tfrac {a}{b}+1)X_n+X_{n-1}=\tfrac {1}{b}X_n+X_{n-1}=S_n\]
</p>
<p>
and thus \(S_n\) is a martingale, as required.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Note that
</p>
<p>
\[\E [X_1\|\sigma (S_n)]=\E [X_2\|\sigma (S_n)]=\ldots =\E [X_n\|\sigma (S_n)]\]
</p>
<p>
is constant, by symmetry (i.e.&nbsp;permuting the \(X_1,X_2,\ldots ,X_n\) does not change the distribution of \(S_n\)). Hence, if we set \(\E [X_i\|\sigma (S_n)]=\alpha \) then
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--

                                                                                       n
                                                                                                                   " n              #
                                                                                       X                            X
                                                                                nα =         E[Xi | σ(Sn )] = E            Xi σ(Sn ) = E[Sn | σ(Sn )] = Sn .
                                                                                       i=1                           i=1



-->


<p>


\begin{align*}
n\alpha =\sum \limits _{i=1}^n\E [X_i\|\sigma (S_n)]=\E \l [\sum \limits _{i=1}^n X_i\,\Big |\,\sigma (S_n)\r ]=\E [S_n\|\sigma (S_n)]=S_n.
\end{align*}
Here we use the linearity of conditional expectation and the fact that \(S_n\) is \(\sigma (S_n)\) measurable. Hence,
</p>
<p>
\[\alpha =\E [X_1\|\sigma (S_n)]=\frac {S_n}{n}.\]
</p>
<p>


</p>
</li>
</ul>
<!--
......    subsection Chapter <a href=notes_1.html#??>??</a> ......
-->
<h5 id="autosec-304">Chapter <a href="notes_1.html#??">??</a></h5>
<a id="notes_1-autopage-304"></a>



<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Since \(Z_n=e^{S_n}\) and \(S_n\in m\mc {F}_n\), also \(Z_n\in m\mc {F}_n\). Since \(|S_n|\leq n\), we have \(|e^{S_n}|\leq
e^n&lt;\infty \), so \(Z_n\) is bounded and hence \(Z_n\in L^1\). Hence also \(M_n\in m\mc {F}_n\) and \(M_n\in L^1\).
</p>
<p>
Lastly,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                                 E[eSn+1 |Fn ] = E[eXn+1 eSn | Fn ]

                                                                                                               = eSn E[eXn+1 | Fn ]

                                                                                                               = eSn E[eXn+1 ]
                                                                                                                                         
                                                                                                                               1 −1
                                                                                                               = eSn      1
                                                                                                                          2e + 2e
                                                                                                                             1
                                                                                                                     Sn e + e
                                                                                                               =e                 .
                                                                                                                            2


-->


<p>


\begin{align*}
\E [e^{S_{n+1}}|\mc {F}_n] &amp;= \E [e^{X_{n+1}}e^{S_n}\|\mc {F}_n] \\ &amp;= e^{S_n}\E [e^{X_{n+1}}\|\mc {F}_n] \\ &amp;= e^{S_n}\E [e^{X_{n+1}}] \\ &amp;= e^{S_n}\l (\tfrac 12
e+\tfrac 12 e^{-1}\r )\\ &amp;= e^{S_n}\frac {e+\frac {1}{e}}{2}.
\end{align*}
Here, we use the taking out what is known rule and the fact that \(X_{n+1}\) (and hence also \(e^{X_{n+1}}\)) is independent of \(\mc {F}_n\). We also calculate the expectation of the discrete random variable
\(X_{n+1}\), using that \(\P [X_{n+1}=1]=\P [X_{n+1}=-1]=\frac 12\). This gives us that
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                                       E[Mn+1 | Fn ] = Mn

                                                                                                        E[Zn+1 | Fn ] ≥ Zn



-->


<p>


\begin{align*}
\E [M_{n+1}\|\mc {F}_n]&amp;= M_n\\ \E [Z_{n+1}\|\mc {F}_n]&amp;\geq Z_n
\end{align*}
where to deduce the last line we use that \(\frac {e+\frac {1}{e}}{2}&gt;1\). Thus \(M_n\) is a martingale and \(Z_n\) is a submartingale.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Since \(X_i\in m\mc {F}_n\) for all \(i\leq n\), we have \(M_n\in m\mc {F}_n\). We have \(|X_i|\leq 1\) so \(|S_n|\leq n\)
and \(|M_n|\leq \max \{(q/p)^n, (q/p)^{-n}\}=(q/p)^{-n}\) (note \(p&gt;q\)), which implies that \(S_n\in L^1\) and \(M_n\in L^1\) for all \(n\).
</p>
<p>
We have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                                E[Sn+1 | Fn ] = E[Sn + Xn+1 | Fn ]

                                                                                                               = Sn + E[Xn+1 | Fn ]

                                                                                                               = Sn + E[Xn+1 ]

                                                                                                               > Sn .



-->


<p>


\begin{align*}
\E [S_{n+1}\|\mc {F}_n] &amp;=\E [S_{n}+X_{n+1}\|\mc {F}_n]\\ &amp;=S_n+\E [X_{n+1}\|\mc {F}_n]\\ &amp;=S_n+\E [X_{n+1}]\\ &amp;&gt;S_n.
\end{align*}
Here we use the taking out what is known rule, followed by the fact that \(X_{n+1}\) is independent of \(\mc {F}_{n}\) and the relationship between conditional expectation and independence. To deduce the final
step we use that \(\E [X_{n+1}]=p(1)+q(-1)=p-q&gt;0\). Therefore, we have \(\E [S_{n+1}\|\mc {F}_n]\geq S_n\), which means that \(S_n\) is a submartingale.
</p>
<p>
We now look at \(M_n\). We have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                             E[Mn+1 |Fn ] = (q/p)Sn E[(q/p)Xn+1 |Fn ]

                                                                                                            = (q/p)Sn E[(q/p)Xn+1 ]

                                                                                                            = (q/p)Sn = Mn .



-->


<p>


\begin{align*}
\E [M_{n+1}|\mc {F}_n]&amp;=(q/p)^{S_n}\E [(q/p)^{X_{n+1}}|\mc {F}_n]\\ &amp;=(q/p)^{S_n}\E [(q/p)^{X_{n+1}}]\\ &amp;=(q/p)^{S_n}=M_n.
\end{align*}
Here we use the taking out what is known rule, followed by the fact that \(X_{n+1}\) is independent of \(\mc {F}_{n}\) and the relationship between conditional expectation and independence. To deduce the final
step we use that
</p>
<p>
\[\E \l [(q/p)^{X_{n+1}}\r ]=p(q/p)^1+q(q/p)^{-1}=p+q=1.\]
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Since \(X_i\in \mc {F}_n\) for all \(i\leq n\) we have \(S_n\in m\mc {F}_n\) where \(\mc {F}_n=\sigma (X_1,\ldots , X_n)\).
Further, if we set \(m=\max (|a|,|b|)\) then \(|S_n|\leq nm\) so \(S_n\in L^1\).
</p>
<p>
We have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                                E[Sn+1 | Fn ] = E[Sn + Xn+1 | Fn ]

                                                                                                               = Sn + E[Xn+1 | Fn ]

                                                                                                               = Sn + E[Xn+1 ]

                                                                                                               = Sn + apa − bpb



-->


<p>


\begin{align*}
\E [S_{n+1}\|\mc {F}_n] &amp;=\E [S_n+X_{n+1}\|\mc {F}_n]\\ &amp;=S_n+\E [X_{n+1}\|\mc {F}_n]\\ &amp;=S_n+\E [X_{n+1}]\\ &amp;=S_n+ap_a-bp_b
\end{align*}
where \(p_b=1-p_a\). Therefore, \(S_n\) is a martingale if and only if \(ap_a=bp_b\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Clearly \(S_n\in m\mc {F}_n\) (recall that, by default, we take \((\mc {F}_n)\) to be the generated filtration of \((S_n)\)) hence
\(S_n^2\in m\mc {F}_n\) and \(S_n^2-n\in \mc {F}_n\). We have \(|S_n|leq n\) so \(\E [|S_n^2-n]\leq \E [S_n^2]+\E [n]=2n\) by the triangle law and monotonicity of expectation, hence \(S_n^2-n\in
\ L^1\). Similarly \(S_n^2\in L^2\),
</p>
<p>
We have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--


                                                                         2
                                                                      E[Sn+1 − (n + 1) | Fn ] = E[(Xn+1 + Sn )2 − (n + 1) | Fn ]
                                                                                                     2
                                                                                                = E[Xn+1 | Fn ] + 2E[Sn Xn+1 | Fn ] + E[Sn2 | Fn ] − (n + 1)
                                                                                                     2
                                                                                                = E[Xn+1 ] + 2Sn E[Xn+1 ] + Sn2 − (n + 1)

                                                                                                = 1 + 2Sn (0) + Sn2 − (n + 1)

                                                                                                = Sn2 − n



-->


<p>


\begin{align*}
\E [S_{n+1}^2-(n+1)\|\mc {F}_n] &amp;= \E [(X_{n+1}+S_n)^2-(n+1)\|\mc {F}_n] \\ &amp;= \E [X_{n+1}^2\|\mc {F}_n]+2\E [S_nX_{n+1}\|\mc {F}_n]+\E [S_n^2\|\mc {F}_n] - (n+1)\\
&amp;= \E [X_{n+1}^2]+2S_n\E [X_{n+1}]+S_n^2 - (n+1)\\ &amp;= 1 + 2S_n(0)+S_n^2 - (n+1) \\ &amp;= S_n^2-n
\end{align*}
Here, the second line follows by linearity. The third line follows by the taking out what is known rule and the independence rule, using that \(X_{n+1}\) is independent of \(\mc {F}_n\), whilst \(S_n\in m\mc
{F}_n\). The fourth line follows because \(X_{n+1}^2=1\) and \(\E [X_{n+1}]=\frac 12(1)+\frac 12(-1)=0\). Hence \(M_n=S_n^2-n\) is a martingale.
</p>
<p>
Subtracting \(n\) from both sides, we obtain \(\E [S_{n+1}^2\|\mc {F}_n]=S_n^2+1\). Hence \((S_n^2)\) is a submartingale.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> The natural filtration is \(\mc {F}_n=\sigma (X_1,\ldots ,X_n)\). We can write
</p>
<p>
\[S_{n+1}=\1\{S_n=0\}+\1\{S_n\neq 0\}(S_n+X_{n+1}).\]
</p>
<p>
Hence, if \(S_n\in m\mc {F}_n\) then \(S_{n+1}\in m\mc {F}_{n+1}\). Since \(S_1=X_1\in m\mc {F}_n\), a trivial induction shows that \(S_n\in m\mc {F}_n\) for all \(n\). hence, \(L_n\in m\mc {F}_n\)
and also \(S_n-L_n\in m\mc {F}_n\).
</p>
<p>
We have \(-n\leq S_n-L_n \leq n+1\) (because the walk is reflected at zero and can increase by at most \(1\) in each time step) so \(S_{n}-L_n\) is bounded and hence \(S_n-L_n\in L^1\).
</p>
<p>
Lastly,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{1}\)</span>


<!--



                                                                                 E[Sn+1 |Fn ] = E[1{Sn = 0} + 1{Sn ̸= 0}(Sn + Xn+1 ) | Fn ]

                                                                                               = 1{Sn = 0} + 1{Sn ̸= 0}(Sn + E [Xn+1 |Fn ])

                                                                                               = 1{Sn = 0} + 1{Sn ̸= 0}Sn

                                                                                               = 1{Sn = 0} + Sn .                 (A.2)                                          --><a id="eq:SnLnpre"></a><!--



-->


<p>


\begin{align}
\E [S_{n+1}|\mc {F}_n] &amp;=\E [\1\{S_n=0\}+\1\{S_n\neq 0\}(S_n+X_{n+1})\|\mc {F}_n]\notag \\ &amp;=\1\{S_n=0\}+\1\{S_n\neq 0\}(S_n+\E \l [X_{n+1}|\mc {F}_n\r ])\notag \\
&amp;=\1\{S_n=0\}+\1\{S_n\neq 0\}S_n\notag \\ &amp;=\1\{S_n=0\}+S_n.\label {eq:SnLnpre}
\end{align}
Here we use linearity and the taking out what is known \((S_n\in m\mc {F}_n)\) rule, as well as that \(X_{n+1}\) is independent of \(\mc {F}_n\). Hence,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{2}\)</span>


<!--

                                                                                                                                n
                                                                                                                "                                    #
                                                                                                                                      1{Si = 0} Fn
                                                                                                                                X
                                                                                   E[Sn+1 − Ln+1 | Fn ] = E Sn+1 −
                                                                                                                                i=0
                                                                                                                                          n
                                                                                                            = Sn + 1{Sn = 0} −                  1{Si = 0}
                                                                                                                                          X

                                                                                                                                          i=0
                                                                                                                     n−1
                                                                                                                            1{Si = 0}
                                                                                                                     X
                                                                                                            = Sn −
                                                                                                                     i=0

                                                                                                            = S n − Ln



-->


<p>


\begin{align*}
\E [S_{n+1}-L_{n+1}\|\mc {F}_n] &amp;=\E \l [S_{n+1}-\sum \limits _{i=0}^n \1\{S_i=0\}\,\Bigg |\,\mc {F}_n\r ]\\ &amp;=S_{n}+\1\{S_n=0\}-\sum \limits _{i=0}^n \1\{S_i=0\}\\
&amp;=S_n-\sum \limits _{i=0}^{n-1} \1\{S_i=0\}\\ &amp;=S_n-L_n
\end{align*}
Here we use <span class="textup">(<a href="notes_1.html#??">??</a>)</span> to deduce the second line, as well as taking out what is known. Hence, \(S_n-L_n\) is a martingale.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Let \(B_n\) be number of red balls in the urn at time \(n\) (i.e.&nbsp;after the \(n^{th}\) draw is complete). Then, the proportion of
red balls in the urn just after the \(n^{th}\) step is
</p>
<p>
\[M_n=\frac {B_n}{n+3}.\]
</p>
<p>
Essentially the same argument as for the two colour urn of Section <a href="notes_1.html#??">??</a> shows that \(M_n\) is adapted to the natural filtration and also in \(L^1\). We have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{2}\)</span>


<!--



                                                                E[Mn+1 | Fn ] = E Mn+1 1{(n+1)th draw is red} + Mn+1 1{(n+1)th draw is not red} Fn
                                                                                       h                                                                       i


                                                                                    Bn + 1                           B
                                                                                                                                                                   
                                                                               =E          1{(n+1)th draw is red} + n 1{(n+1)th draw is not red} Fn
                                                                                     n+4                            n+4
                                                                                 Bn + 1 h                                Bn
                                                                                        E 1{(n+1)th draw is red} Fn +       E 1{(n+1)th draw is not red} Fn
                                                                                                                     i       h                              i
                                                                               =
                                                                                  n+4                                   n+4
                                                                                 Bn + 1 Bn        Bn              Bn
                                                                                                                     
                                                                               =              +           1−
                                                                                  n+4 n+3 n+4                   n+3
                                                                                   Bn (n + 4)
                                                                               =
                                                                                 (n + 3)(n + 4)
                                                                               = Mn .



-->


<p>


\begin{align*}
\E [M_{n+1}\|\mc {F}_n] &amp;=\E \l [M_{n+1}\1_{\{(n+1)^{th}\text { draw is red}\}}+M_{n+1}\1_{\{(n+1)^{th}\text { draw is not red}\}}\,\Big |\,\mc {F}_n\r ]\\ &amp;=\E \l [\frac
{B_n+1}{n+4}\1_{\{(n+1)^{th}\text { draw is red}\}}+\frac {B_n}{n+4}\1_{\{(n+1)^{th}\text { draw is not red}\}}\,\Big |\,\mc {F}_n\r ]\\ &amp;=\frac {B_n+1}{n+4}\E \l
[\1_{\{(n+1)^{th}\text { draw is red}\}}\,\Big |\,\mc {F}_n\r ]+\frac {B_n}{n+4}\E \l [\1_{\{(n+1)^{th}\text { draw is not red}\}}\,\Big |\,\mc {F}_n\r ]\\ &amp;=\frac
{B_n+1}{n+4}\frac {B_n}{n+3}+\frac {B_n}{n+4}\l (1-\frac {B_n}{n+3}\r )\\ &amp;=\frac {B_n(n+4)}{(n+3)(n+4)}\\ &amp;=M_n.
\end{align*}
Hence \((M_n)\) is a martingale.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> By the result of <a href="notes_1.html#??">??</a>, \(S_n^2\) is a submartingale.
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">(i)</span> \(M_n=S_n^2+n\) is \(\mc {F}_n\) measurable (because \(S_n\in m\mc {F}_n\)) and since \(|S_n|\leq n\) we have \(|M_n|\leq n^2+n&lt;\infty \), so \(M_n\in
L^1\). Further, using the submartingale property of \(S_n^2\),
</p>
<p>
\[\E [S_{n+1}^2+n+1\|\mc {F}_n]=\E [S_{n+1}^2\|\mc {F}_n]+n+1\geq S_n^2+n\]
</p>
<p>
so \(M_n\) is a submartingale. But \(\E [M_1]=2\neq 0=\E [M_0]\), so (by Lemma <a href="notes_1.html#??">??</a>) we have that \((M_n)\) is not a martingale.
</p>


</li>
<li>


<p>
<span class="listmarker">(ii)</span> We have shown in Section <a href="notes_1.html#??">??</a> that \((S_n)\) is a martingale, and in Exercise <a href="notes_1.html#??">??</a> that
\(M_n=S_n^2-n\) is a martingale. Hence \(S_n^2-n+S_n\in \mc {F}_n\) and \(\E [|S_n^2+S_n-n|]\leq \E [|S_n^2-n|]+\E [|S_n|]&lt;\infty \) by the triangle law, so also \(S_n^2+S_n-n\in L^1\). Also,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{2}\)</span>


<!--


                                                                              2                                2
                                                                           E[Sn+1 + Sn+1 − (n + 1) | Fn ] = E[Sn+1 − (n + 1) | Fn ] + E[Sn+1 | Fn ]

                                                                                                              = Sn2 − n + Sn



-->


<p>


\begin{align*}
\E [S_{n+1}^2+S_{n+1}-(n+1)\|\mc {F}_n] &amp;= \E [S_{n+1}^2-(n+1)\|\mc {F}_n]+\E [S_{n+1}\|\mc {F}_n] \\ &amp;= S_n^2-n+S_n
\end{align*}
as required. To deduce the second line we use the martingale properties of \((S_n)\) and \((M_n)\). As all martingales are submartingales, this case is also a submartingale.
</p>
<p>
<i>More generally, the sum of two martingales is always a martingale. The same is true for submartingales and also for supermartingales.</i>
</p>
</li>
<li>


<p>
<span class="listmarker">(iii)</span> \(M_n=\frac {S_n}{n}\) is \(\mc {F}_n\) measurable (because \(S_n\in m\mc {F}_n\)) and since \(|S_n|\leq n\) we have \(|S_n|\leq 1\), so \(M_n\in L^1\).
Since \(S_n\) is a martingale, we have
</p>
<p>
\[\E \l [\frac {S_{n+1}}{n+1}\,\Big {|}\,\mc {F}_n\r ]=\frac {1}{n+1}\E [S_{n+1}|\mc {F}_n]=\frac {S_n}{n+1}\neq \frac {S_n}{n}.\]
</p>
<p>
Hence \(M_n=\frac {S_n}{n}\) is not a martingale. Also, since \(S_n\) may be both positive or negative, we do not have \(\frac {S_n}{n+1}\geq \frac {S_n}{n}\), so \(S_n\) is also not a submartingale.
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We showed that \(S_n^2-n\) was a martingale in <a href="notes_1.html#??">??</a>. We try to mimic that calculation and find out
what goes wrong in the cubic case. So, we look at
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{2}\)</span>


<!--


                                                                                               3
                                                                   E[(Sn+1 − Sn )3 | Fn ] = E[Sn+1                 2
                                                                                                   | Fn ] − 3Sn E[Sn+1 | Fn ] + 3Sn2 E[Sn+1 | Fn ] − E[Sn3 | Fn ]
                                                                                                3
                                                                                           = E[Sn+1 | Fn ] − 3Sn (Sn2 + 1) + 3Sn2 Sn − Sn3
                                                                                                3
                                                                                           = E[Sn+1 | Fn ] − Sn3 − 3Sn .



-->


<p>


\begin{align*}
\E [(S_{n+1}-S_n)^3\|\mc {F}_n] &amp;=\E [S_{n+1}^3\|\mc {F}_n]-3S_n\E [S_{n+1}^2\|\mc {F}_n]+3S_n^2\E [S_{n+1}\|\mc {F}_n]-\E [S_n^3\|\mc {F}_n]\\ &amp;=\E [S_{n+1}^3\|\mc
{F}_n]-3S_n(S_n^2+1)+3S_n^2S_n-S_n^3\\ &amp;=\E [S_{n+1}^3\|\mc {F}_n]-S_n^3-3S_n.
\end{align*}
Here we use linearity, taking out what is known and that fact that \(\E [S_{n+1}^2\|\mc {F}_n]=S_n^2+1\) (from <a href="notes_1.html#??">??</a>). However, also \(S_{n+1}-S_n=X_{n+1}\), so
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{2}\)</span>


<!--


                                                                                                                           3
                                                                                               E[(Sn+1 − Sn )3 | Fn ] = E[Xn+1 | Fn ]
                                                                                                                             3
                                                                                                                        = E[Xn+1 ]

                                                                                                                        = 0.



-->


<p>


\begin{align*}
\E [(S_{n+1}-S_n)^3\|\mc {F}_n] &amp;=\E [X_{n+1}^3\|\mc {F}_n]\\ &amp;=\E [X_{n+1}^3]\\ &amp;=0.
\end{align*}
because \(X_{n+1}\) is independent of \(\mc {F}_n\) and \(X_{n+1}^3=X_n\) (it takes values only \(1\) or \(-1\)) so \(\E [X_{n+1}^3]=0\). Putting these two calculations together, we have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{2}\)</span>
<!--


                                                            3
                                                         E[Sn+1 | Fn ] = Sn3 + 3Sn .                                                                        (A.3)               --><a id="eq:sn3_mart"></a><!--

-->
<p>


\begin{equation}
\label {eq:sn3_mart} \E [S_{n+1}^3\|\mc {F}_n]=S_n^3+3S_n.
\end{equation}


</p>
<p>
Suppose (aiming for a contradiction) that there is a deterministic function \(f:\N \to \R \) such that \(S_n^3-f(n)\) is a martingale. Then
</p>
<p>
\[\E [S_{n+1}^3-f(n+1)\|\mc {F}_n]=S_n^3-f(n).\]
</p>
<p>
Combining the above equation with <span class="textup">(<a href="notes_1.html#??">??</a>)</span> gives
</p>
<p>
\[f(n+1)-f(n)=3S_n\]
</p>
<p>
but this is impossible because \(S_n\) is not deterministic. Thus we reach a contradiction; no such function \(f\) exists.
</p>
</li>
</ul>
<!--
......    subsection Chapter <a href=notes_1.html#??>??</a> ......
-->
<h5 id="autosec-305">Chapter <a href="notes_1.html#??">??</a></h5>
<a id="notes_1-autopage-305"></a>



<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> The value of the portfolio \((1,3)\) at time \(1\) will be
</p>
<p>
\[1(1+r)+3S_1=(1+r)+3S_1\]
</p>
<p>
where \(S_1\) is a random variable with \(\P [S_1=su]=p_u\) and \(\P [S_1=sd]=p_d\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We need a portfolio \(h=(x,y)\) such that \(V^h_1=\Phi (S_1)=1\), so we need that
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{3}\)</span>


<!--



                                                                                                       x(1 + r) + ysu = 1

                                                                                                       x(1 + r) + ysd = 1.



-->


<p>


\begin{align*}
x(1+r)+ysu&amp;=1\\ x(1+r)+ysd&amp;=1.
\end{align*}
Solving this pair of linear equations gives \(x=\frac {1}{1+r}\) and \(y=0\). So our replicating portfolio consists simply of \(\frac {1}{1+r}\) cash and no stock.
</p>
<p>
Hence, the value of this contingent claim at time \(0\) is \(x+sy=\frac {1}{1+r}\).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Now we need that
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{3}\)</span>


<!--



                                                                                                       x(1 + r) + ysu = 3

                                                                                                       x(1 + r) + ysd = 1.



-->


<p>


\begin{align*}
x(1+r)+ysu=3\\ x(1+r)+ysd=1.
\end{align*}
Which has solution \(x=\frac {1}{1+r}\l (3-\frac {2su}{su-sd}\r )\) and \(y=\frac {2}{s(u-d)}\).
</p>
<p>
Hence, the value of this contingent claim at time \(0\) is \(x+sy=\frac {1}{1+r}\l (3-\frac {2su}{su-sd}\r )+\frac {2}{u-d}\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> If we buy two units of stock, at time \(1\), for a price \(K\) then our contingent claim is \(\Phi (S_1)=2S_1-2K\).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> In a European put option, with strike price \(K\in (sd,su)\), we have the option to sell a single unit of stock for strike price \(K\).
It is advantageous to us to do so only if \(K&gt;S_1\), so
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{3}\)</span>
<!--
                                                           
                                                           
                                                           0
                                                                    if S1 = su,
                                                Φ(S1 ) =                           = max(K − S1 , 0)                                        (A.4)                     --><a id="eq:eu_put_contingent"></a><!--
                                                           K − S1
                                                           
                                                                    if S1 = sd.
-->
<p>


\begin{equation}
\label {eq:eu_put_contingent} \Phi (S_1)=\begin{cases} 0 &amp; \text { if }S_1=su,\\ K-S_1 &amp; \text { if }S_1=sd.                        \end {cases} =\max (K-S_1, 0)
\end{equation}


</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(c)</span></span> We \(S_1=su\) we sell a unit of stock for strike price \(K\) and otherwise do nothing. So our contingent claim is
</p>
<p>
\[\Phi (S_1)=\begin {cases} K-S_1 &amp; \text { if }S_1=su,\\ 0 &amp; \text { if }S_1=sd.                        \end {cases} \]
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(d)</span></span> Holding both the contracts in (b) and (c) at once, means that in both \(S_1=su\) and \(S-1=sd\), we end up selling a single unit of
stock for a fixed price \(K\). Our contingent claim for doing so is \(\Phi (S_1)=K-S_1\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> The contingent claims for the call and put options respectively are \(\max (S_1-K,0)\) from <span class="textup">(<a
href="notes_1.html#??">??</a>)</span> and \(\max (K-S_1,0)\) from <span class="textup">(<a href="notes_1.html#??">??</a>)</span>. Using the risk neutral valuation formula from Proposition <a
href="notes_1.html#??">??</a> we have
</p>
<p>
\[\Pi ^{call}_0=\frac {1}{1+r}\E ^\Q \l [\max (S_1-K,0)\r ],\quad \Pi ^{put}_0=\frac {1}{1+r}\E ^\Q \l [\max (K-S_1,0)\r ].\]
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Hence,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{4}\)</span>


<!--


                                                                                                 1
                                                                                   Πcall
                                                                                    0    − Πput
                                                                                            0 =     EQ [max(S1 − K, 0) − max(K − S1 , 0)]
                                                                                                1+r
                                                                                                 1
                                                                                              =     EQ [max(S1 − K, 0) + min(S1 − K, 0)]
                                                                                                1+r
                                                                                                 1
                                                                                              =     EQ [S1 − K]
                                                                                                1+r
                                                                                                 1  Q           
                                                                                              =      E [S1 ] − K
                                                                                                1+r




-->


<p>


\begin{align*}
\Pi ^{call}_0-\Pi ^{put}_0&amp;=\frac {1}{1+r}\E ^\Q \l [\max (S_1-K,0)-\max (K-S_1,0)\r ]\\ &amp;=\frac {1}{1+r}\E ^\Q \l [\max (S_1-K,0)+\min (S_1-K,0)\r ]\\ &amp;=\frac
{1}{1+r}\E ^\Q \l [S_1-K\r ]\\ &amp;=\frac {1}{1+r}\l (\E ^\Q \l [S_1\r ]-K\r )\\
\end{align*}
By <span class="textup">(<a href="notes_1.html#??">??</a>)</span> (or you can use Proposition <a href="notes_1.html#??">??</a> again) we have \(\frac {1}{1+r}\E ^\Q \l [S_1\r ]=S_0=s\), so
we obtain
</p>
<p>
\[\Pi ^{call}_0-\Pi ^{put}_0=s-\frac {K}{1+r}.\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> In the binomial model, the contingent claim of a European call option with strike price \(K\) is \(\Phi (S_T)=\max (S_T-K)\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> A tree-like diagram of the possible changes in stock price, with the value of the contingent claim \(\Phi (S_T)=\max (K-S_1,0)\)
written on for the final time step looks like
</p>
<div class="center">
<p>


<a href="sud_ex_1.jpg" target="_blank" ><img
      src="sud_ex_1.jpg"
      style="
      width:173pt;
      "
      class="inlineimage"
      alt="(A recombining tree of the stock prices with the contingent claim marked)"
></a>
</p>
</div>
<p>
By <span class="textup">(<a href="notes_1.html#??">??</a>)</span>, the risk free probabilities in this case are
</p>
<p>
\[q_u=\frac {(1+0.25)-0.5}{2.0-0.5}=0.5,\quad q_d=1-q_u=0.5.\]
</p>
<p>
Using Proposition <a href="notes_1.html#??">??</a> recursively, as in Section <a href="notes_1.html#??">??</a>, we can put in the value of the European put option at all nodes back to time \(0\), giving
</p>
<div class="center">
<p>


<a href="sud_ex_2.jpg" target="_blank" ><img
      src="sud_ex_2.jpg"
      style="
      width:173pt;
      "
      class="inlineimage"
      alt="(The previous picture with the value of the contract marked)"
></a>
</p>
</div>
<p>
Hence, the value of our European put option at time \(0\) is \(12\).
</p>
<p>
Changing the values of \(p_u\) and \(p_d\) do not alter value of our put option, since \(p_u\) and \(p_d\) do not appear in the pricing formulae.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> A tree-like diagram of the possible changes in stock price, with the value of the option at all nodes of the tree shown in (green) square
boxes, and hedging portfolios \(h=(x,y)\) written (in red) for each node of the tree:
</p>
<div class="center">
<p>


<a href="sud_ex_4.jpg" target="_blank" ><img
      src="sud_ex_4.jpg"
      style="
      width:173pt;
      "
      class="inlineimage"
      alt="(The previous picture with the hedging portfolios marked)"
></a>
</p>
</div>
<p>
The hedging portfolios are found by solving linear equations of the form \(x+dSy=\wt {\Phi }(dS)\), \(x+uSy=\wt {\Phi }(uD)\), where \(\wt \Phi \) is the contingent claim and \(S\) the initial stock price of
the one period model associated to the given node. The value of the contingent claim at each node is then inferred from the hedging portfolio as \(V=x+Sy\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> A tree-like diagram of the possible changes in stock price, with the value of the option at all nodes of the tree shown in (green) square
boxes, and hedging portfolios \(h=(x,y)\) written (in red) for each node of the tree:
</p>
<div class="center">
<p>


<a href="sud_ex_3.jpg" target="_blank" ><img
      src="sud_ex_3.jpg"
      style="
      width:173pt;
      "
      class="inlineimage"
      alt="(A recombining tree of the stock price with the contingent claim and hedging portfolios marked)"
></a>
</p>
</div>
<p>
The hedging portfolios are found by solving linear equations of the form \(x+dSy=\wt {\Phi }(dS)\), \(x+uSy=\wt {\Phi }(uD)\), where \(\wt \Phi \) is the contingent claim and \(S\) the initial stock price of
the one period model associated to the given node. The value of the contingent claim at each node is then inferred from the hedging portfolio as \(V=x+Sy\).
</p>
<p>
The hedging portfolio is the same at each node. This is because the contingent claim of our call option always exceeds zero i.e.&nbsp;we are certain that at time \(T=2\) we would want to exercise our call option
and buy a single unit of stock for a price \(K=60\). Since there is no interest payable on cash, this means that we can replicate our option (at all times) by holding a single unit of stock, and \(-60\) in cash.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We have that \((S_t)\) is adapted (see Section <a href="notes_1.html#??">??</a>) and since \(S_t\in (d^t,u^t)\) we have also
that \(S_t\in L^1\). Hence, \(S_t\) is a martingale under \(\P \) if and only if \(\E ^\P [S_{t+1}\|\F _t]=S_t\). That is if
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{4}\)</span>


<!--



                                                                                               EP [St+1 | Ft ] = E[Zt+1 St | Ft ]

                                                                                                               = St EP [Z | Ft ]

                                                                                                               = St EP [Z]



-->


<p>


\begin{align*}
\E ^\P [S_{t+1}\|\F _t]&amp;=\E [Z_{t+1}S_{t}\|\mc {F}_t]\\ &amp;=S_t\E ^\P [Z\|\mc {F}_t]\\ &amp;=S_t\E ^\P [Z]
\end{align*}
is equal to \(S_t\). Hence, \(S_t\) is a martingale under \(\P \) if and only if \(\E ^\P [Z_{t+1}]=up_u+dp_d=1.\)
</p>
<p>
Now consider \(M_t=\log S_t\), and assume that \(0&lt;d&lt;u\) and \(0&lt;s\), which implies \(S_t&gt;0\). Since \(S_t\) is adapted, so is \(M_t\). Since \(S_t\in S_t\in (d^t,u^t)\) we have \(M_t\in
(t \log d, t\log u)\) so also \(M_t\in L^1\). We have
</p>
<p>
\[M_t=\log \l (S_0\prod \limits _{i=1}^tZ_i\r )=\log (S_0)+\sum \limits _{i=1}^t Z_i.\]
</p>
<p>
Hence,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{4}\)</span>


<!--

                                                                                                              t
                                                                                                              X
                                                                                  EP [Mt+1 |Ft ] = log S0 +         log(Zi ) + E[log(Zt+1 ) | Ft ]
                                                                                                              i=1

                                                                                               = Mt + EP [log Zt+1 ]

                                                                                               = Mt + pu log u + pd log d.



-->


<p>


\begin{align*}
\E ^\P [M_{t+1}|\mc {F}_t]&amp;=\log S_0+\sum \limits _{i=1}^{t}\log (Z_i)+\E [\log (Z_{t+1})\|\mc {F}_t]\\ &amp;=M_t+\E ^\P [\log Z_{t+1}]\\ &amp;=M_t+p_u\log u + p_d\log d.
\end{align*}
Here we use taking out what is known, since \(S_0\) and \(Z_i\in m\mc {F}_t\) for \(i\leq t\), and also that \(Z_{t+1}\) is independent of \(\mc {F}_t\). Hence, \(M_t\) is a martingale under \(\P \) if and
only if \(p_u\log u + p_d\log d=0\).
</p>
</li>
</ul>
<!--
......    subsection Chapter <a href=notes_1.html#??>??</a> ......
-->
<h5 id="autosec-310">Chapter <a href="notes_1.html#??">??</a></h5>
<a id="notes_1-autopage-310"></a>



<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Since \(X_n\geq 0\) we have
</p>
<p>
\[\E [|X_n-0|]=\E [X_n]=\tfrac 122^{-n}=2^{-(n+1)}\]
</p>
<p>
which tends to \(0\) as \(n\to \infty \). Hence \(X_n\to 0\) in \(L^1\). Lemma <a href="notes_1.html#??">??</a> then implies that also \(X_n\to 0\) in probability and in distribution. Also, \(0\leq
X_n\leq 2^{-n}\) and \(2^{-n}\to 0\), so by the sandwich rule we have \(X_n\to 0\) as \(n\to \infty \), and hence \(\P [X_n\to 0]=1\), which means that \(X_n\to 0\) almost surely.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We have \(X_n\stackrel {L^1}{\to } X\) and hence
</p>
<p>
\[\l |\E [X_n]-\E [X]\r |=|\E [X_n-X]|\leq \E [|X_n-X|]\to 0.\]
</p>
<p>
Here we use the linearity and absolute value properties of expectation. Hence, \(\E [X_n]\to \E [X]\).
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Let \(X_1\) be a random variable such that \(\P [X_1=1]=\P [X_1=-1]=\frac 12\) and note that \(\E [X_1]=0\). Set
\(X_n=X_1\) for all \(n\geq 2\), which implies that \(\E [X_n]=0\to 0\) as \(n\to \infty \). But \(\E [|X_n-0|]=\E [|X_n|]=1\) so \(X_n\) does not converge to \(0\) in \(L^1\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We have \(X_n=X\) when \(U=2\), and \(|X_n-X|=\frac {1}{n}\) when \(U=0,1\). Hence, \(|X_n-X|\leq \frac {1}{n}\) and by
monotonicity of \(\E \) we have
</p>
<p>
\[\E \l [|X_n-X|\r ]\leq \frac {1}{n}\to 0\quad \text { as }n\to \infty ,\]
</p>
<p>
which means that \(X_n\stackrel {L^1}{\to }X\).
</p>
<p>
We can write
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{4}\)</span>


<!--



                                                                                P[Xn → X] = P[Xn → X, U = 0] + P[Xn → X, U = 1] + P[Xn → X, U = 2]
                                                                                                    1                    1
                                                                                          = P[1 +     → 1, U = 0] + P[1 − → 1, U = 1] + P[0 → 0, U = 2]
                                                                                                    n                    n
                                                                                          = P[U = 0] + P[U = 1] + P[U = 2]

                                                                                          = 1.



-->


<p>


\begin{align*}
\P [X_n\to X] &amp;=\P [X_n\to X, U=0]+\P [X_n\to X, U=1] + \P [X_n\to X, U=2]\\ &amp;=\P [1+\frac {1}{n}\to 1, U=0]+\P [1-\frac {1}{n}\to 1, U=1] + \P [0\to 0, U=2]\\ &amp;=\P
[U=0]+\P [U=1]+\P [U=2]\\ &amp;=1.
\end{align*}
Hence, \(X_n\stackrel {a.s.}{\to } X\).
</p>
<p>
By Lemma <a href="notes_1.html#??">??</a>, it follows that also \(X_n\to X\) in probability and in distribution.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We have \(X_n=X_1=1-Y\) for all \(n\), so \(Y=0\iff X_n=1\) and \(Y=1\iff X_n=0\). Hence \(|Y-X_n|=1\) for all \(n\). So, for
example, with \(a=\frac 12\) we have \(\P [|X_n-Y|\geq a]=1\), which does not tend to zero as \(n\to \infty \). Hence \(X_n\) does not converge to \(Y\) in probability.
</p>
<p>
However, \(X_n\) and \(y\) have the same distribution, given by
</p>
<p>
\[\P [Y\leq x]=\P [X_n\leq x]= \begin {cases} 0 &amp; \text { if } x&lt;0\\ \frac 12 &amp; \text { if } x\in [0,1)\\ 1 &amp; \text { if }x\geq 1 \end {cases} \]
</p>
<p>
so we do have \(\P [X_n\leq x]\to \P [Y\leq x]\) as \(n\to \infty \). Hence \(X_n\stackrel {d}{\to }Y\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Let \((X_n)\) be the sequence of random variables from <a href="notes_1.html#??">??</a>. Define \(Y_n=X_1+X_2+\ldots +X_n\).
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> For each \(\omega \in \Omega \), we have \(Y_{n+1}(\omega )=Y_n(\omega )+X_{n+1}(\omega )\). Hence, \((Y_n(\omega )\)
is an increasing sequence. Since \(X_n(\omega )\leq 2^{-n}\) for all \(n\) we have
</p>
<p>
\[|Y_n(\omega )|\leq 2^{-1}+2^{-2}+\ldots +2^{-n}\leq \sum \limits _{i=1}^\infty 2^{-i}=1,\]
</p>
<p>
meaning that the sequence \(Y_n(\omega )\) is bounded above by \(1\).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Hence, since bounded increasing sequences converge, for any \(\omega \in \Omega \) the sequence \(Y_n(\omega )\) converges.
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(c)</span></span> The value of \(Y_1\) is either \(0\) or \(\frac 12\), both with probability \(\frac 12\). The value of \(Y_2\) is either \(0,\frac
14,\frac 12,\frac 34\), all with probability \(\frac 14\). The value of \(Y_3\) is either \(0,\frac 18,\frac 14,\frac 38,\frac 12,\frac 58,\frac 34,\frac 78\) all with probability \(\frac 18\).
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(d)</span></span> We can guess from (c) that the \(Y_n\) are becoming more and more evenly spread across \([0,1]\), so we expect them to approach
a uniform distribution as \(n\to \infty \).
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(e)</span></span> We will use induction on \(n\). Our inductive hypothesis is
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
(IH)\(_n\): For all \(k=0,1,\ldots ,2^n-1\), it holds that \(\P [Y_n=k2^{-n}]=2^{-n}\).
</p>
</li>
</ul>
<p>
In words, this says that \(Y_n\) is uniformly distributed on \(\{k2^{-n}\-k=0,1,\ldots ,2^n-1\}\).
</p>
<p>
For the case \(n=1\), we have \(Y_1=X_1\) so \(\P [Y_1=0]=\P [Y_1=\tfrac 12]=\tfrac 12\), hence (IH)\(_1\) holds.
</p>
<p>
Now, assume that (IH)\(_n\) holds. We want to calculate \(\P [Y_{n+1}=k2^{-(n+1)}\) for \(k=0,1,\ldots ,2^{n+1}-1\). We consider two cases, dependent on whether \(k\) is even or odd.
</p>
<ul class="itemize" style="list-style-type:none">


<li>
<p>
<span class="listmarker">•</span> If \(k\) is even then we can write \(k=2j\) for some \(j=0,\ldots ,2^n-1\). Hence,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{4}\)</span>


<!--



                                                                                            P[Yn+1 = k2−(n+1) ] = P[Yn+1 = j2−n ]

                                                                                                               = P[Yn = j2−n and Xn+1 = 0]

                                                                                                               = P[Yn = j2−n ]P[Xn+1 = 0]

                                                                                                               = 2−n 12

                                                                                                               = 2−(n+1) .



-->


<p>


\begin{align*}
\P [Y_{n+1}=k2^{-(n+1)}] &amp;=\P [Y_{n+1}=j2^{-n}]\\ &amp;=\P [Y_n=j2^{-n}\text { and }X_{n+1}=0]\\ &amp;=\P [Y_n=j2^{-n}]\P [X_{n+1}=0]\\ &amp;=2^{-n}\tfrac 12\\
&amp;=2^{-(n+1)}.
\end{align*}
Here we use that \(Y_n\) and \(X_{n+1}\) are independent, and use (IH)\(_n\) to calculate \(\P [Y_n=j2^{-n}]\).
</p>


</li>
<li>


<p>
<span class="listmarker">•</span> Alternatively, if \(k\) is odd then we can write \(k=2j+1\) for some \(j=0,\ldots ,2^n-1\). Hence,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{4}\)</span>


<!--



                                                                                         P[Yn+1 = k2−(n+1) ] = P[Yn+1 = j2−n + 2−(n+1) ]

                                                                                                            = P[Yn = j2−n and Xn+1 = 2−(n+1) ]

                                                                                                            = P[Yn = j2−n ]P[Xn+1 = 2−(n+1) ]

                                                                                                            = 2−n 21

                                                                                                            = 2−(n+1) .



-->


<p>


\begin{align*}
\P [Y_{n+1}=k2^{-(n+1)}] &amp;=\P [Y_{n+1}=j2^{-n}+2^{-(n+1)}]\\ &amp;=\P [Y_n=j2^{-n}\text { and }X_{n+1}=2^{-(n+1)}]\\ &amp;=\P [Y_n=j2^{-n}]\P [X_{n+1}=2^{-(n+1)}]\\
&amp;=2^{-n}\tfrac 12\\ &amp;=2^{-(n+1)}.
\end{align*}
Here, again, we use that \(Y_n\) and \(X_{n+1}\) are independent, and use (IH)\(_n\) to calculate \(\P [Y_n=j2^{-n}]\).
</p>
</li>
</ul>
<p>
In both cases we have shown that \(\P [Y_{n+1}=k2^{-(n+1)}]=2^{-(n+1)}\), hence (IH)\(_{n+1}\) holds.
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(f)</span></span> We know that \(Y_n\to Y\) almost surely, hence by Lemma <a href="notes_1.html#??">??</a> the convergence also holds in
distribution. Hence \(\P [Y_n\leq a]\to \P [Y\leq a]\).
</p>
<p>
From part (e), for any \(a\in [0,1]\) we have
</p>
<p>
\[(k-1)2^{-n}\leq a&lt;k2^{-n}\quad \ra \quad \P [Y_n\leq a]=k2^{-n}.\]
</p>
<p>
Hence \(a\leq \P [Y_n\leq a]\leq a+2^{-n}\), and the sandwich rule tells us that \(\P [Y_n\leq a]\to a\). Hence \(\P [Y\leq a]=a\), for all \(a\in [0,1]\), which means that \(Y\) has a uniform
distribution on \([0,1]\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Since \(\1\{X\leq n+1\}\geq 1\{X\leq n\}\) we have \(X_{n+1}\geq X_n\), and since \(X&gt;0\) we have \(X_n\geq 0\). Further,
for all \(\omega \in \Omega \), whenever \(X(w)&lt;n\) we have \(X_n(\omega )=X(\omega )\), and since \(\P [X(\omega )&lt;\infty ]=1\) this means that \(\P \l [X_n(\omega )\to X(\omega )\r
]=1\). Hence \(X_n\stackrel {a.s.}{\to } X\).
</p>
<p>
Therefore, the monotone convergence theorem applies and \(\E [X_n]\to \E [X]\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Define \(X_n=-Y_n\). Then \(X_n\geq 0\) and \(X_{n+1}\geq X_n\), almost surely. Hence, \((X_n)\) satisfies the conditions for the
dominated convergence theorem and there exists a random variable \(X\) such that \(X_n\stackrel {a.s.}{\to }X\) and \(\E [X_n]\to \E [X]\). Define \(Y=-X\) and then we have \(Y_n\stackrel
{a.s.}{\to }Y\) and \(\E [Y_n]\to \E [Y]\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> The equation
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{4}\)</span>
<!--
                                                    " n    #     n
                                                     X           X
                                                E         Xi =         E[Xi ]                                          (A.5)                                          --><a id="eq:fubini_mct_pre"></a><!--
                                                    i=1          i=1
-->
<p>


\begin{equation}
\label {eq:fubini_mct_pre} \E \l [\sum _{i=1}^n X_i\r ]=\sum _{i=1}^n \E [X_i]
\end{equation}


</p>
<p>
follows by iterating the linearity property of \(\E \) \(n\) times (or by a trivial induction). However, in \(\E \big [\sum _{i=1}^\infty X_i\big ]=\sum _{i=1}^\infty \E [X_i]\) both sides are defined by
infinite sums, which are limits, and the linearity property does not tell us anything about limits.
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> We’ll take a limit as \(n\to \infty \) on each side of <span class="textup">(<a href="notes_1.html#??">??</a>)</span>.
We’ll have to justify ourselves in each case.
</p>
<p>
First, consider the right hand side. Since we have \(X_n\geq 0\) almost surely, \(\E [X_n]\geq 0\) by monotonicity of \(\E \). Therefore, \(\sum _{i=1}^n \E [X_i]\) is an increasing sequence of real numbers.
Hence it is convergent (either to a real number or to \(+\infty \)) and, by definition of an infinite series, the limit is written \(\sum _{i=1}^\infty \E [X_i]\).
</p>
<p>
Now, the left hand side, and here we’ll need the monotone convergence theorem. Define \(Y_n=\sum _{i=1}^n X_i\). Then \(Y_{n+1}-Y_n=X_{n+1}\geq 0\) almost surely, so \(Y_{n+1}\geq Y_n\). Since
\(X_i\geq 0\) for all \(i\), also \(Y_n\geq 0\), almost surely. Hence the monotone convergence theorem applies and \(\E [Y_n]\to \E [Y_\infty ]\) where \(Y_\infty =\sum _{i=1}^\infty X_i\).
</p>
<p>
Putting both sides together (and using the uniqueness of limits to tell us that the limits on both sides must be equal as \(n\to \infty \)) we obtain that \(\E \big [\sum _{i=1}^\infty X_i\big ]=\sum
_{i=1}^\infty \E [X_i]\).
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(c)</span></span> Since \(\P [X_i=1]=\P [X_i=-1]\), we have \(\E [X_i]=0\) and hence also \(\sum _{i=1}^\infty \E [X_i]=0\). However, the
limit \(\sum _{i=1}^\infty X_i\) is not well defined, because \(Y_n=\sum _{i=1}^n X_i\) oscillates (it jumps up/down by \(\pm 1\) on each step) and does not converge. Hence, in this case \(\E \big [\sum
_{i=1}^\infty X_i\big ]\) is not well defined.
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We have both \(\P [X_n\leq x]\to \P [X\leq x]\) and \(\P [X_n\leq x]\to \P [Y\leq x]\), so by uniqueness of limits for
real sequences, we have \(\P [X\leq x]=\P [Y\leq x]\) for all \(x\in \R \). Hence, \(X\) and \(Y\) have the same distribution (i.e.&nbsp;they have the same distribution functions \(F_X(x)=F_Y(x)\)).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> By definition of convergence in probability, for any \(a&gt;0\), for any \(\epsilon &gt;0\) there exists \(N\in \N \) such that, for
all \(n\geq N\),
</p>
<p>
\[\P [|X_n-X|&gt;a]&lt;\epsilon \hspace {1pc}\text { and }\hspace {1pc}\P [|X_n-Y|&gt;a]&lt;\epsilon .\]
</p>
<p>
By the triangle inequality we have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{5}\)</span>
<!--


               P[|X − Y | > 2a] = P[|X − Xn + Xn − Y | > 2a] ≤ P[|X − Xn | + |Xn − Y | > 2a].                             (A.6)                                       --><a id="eq:ps_uniq_limit"></a><!--

-->
<p>


\begin{equation}
\label {eq:ps_uniq_limit} \P [|X-Y|&gt;2a]=\P [|X-X_n+X_n-Y|&gt;2a]\leq \P [|X-X_n|+|X_n-Y|&gt;2a].
\end{equation}


</p>
<p>
If \(|X-X_n|+|X_n-Y|&gt;2a\) then \(|X-X_n|&gt;a\) or \(|X_n-Y|&gt;a\) (or possibly both). Hence, continuing <span class="textup">(<a href="notes_1.html#??">??</a>)</span>,
</p>
<p>
\[\P [|X-Y|&gt;2a]\leq \P [|X_n-X|&gt;a]+\P [|X_n-Y|&gt;a]\leq 2\epsilon .\]
</p>
<p>
Since this is true for any \(\epsilon &gt;0\) and any \(a&gt;0\), we have \(\P [X=Y]=1\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Suppose (aiming for a contradiction) that there exists and random variable \(X\) such that \(X_n\to X\) in probability. By the triangle
inequality we have
</p>
<p>
\[|X_n-X_{n+1}|\leq |X_n-X|+|X-X_{n+1}|\]
</p>
<p>
Hence, if \(|X_n-X_{n+1}|&gt;1\) then \(|X_n-X|&gt;\frac 12\) or \(|X_{n+1}-X|&gt;\frac 12\) (or both). Therefore,
</p>
<p>
\[\P \l [|X_n-X_{n+1}|&gt;1\r ]\leq \P \l [|X_n-X|&gt;\tfrac 12\r ]+\P \l [|X_{n+1}-X|&gt;\tfrac 12\r ].\]
</p>
<p>
Since \(X_n\to X\) in probability, the right hand side of the above tends to zero as \(n\to \infty \). This implies that
</p>
<p>
\[\P \l [|X_n-X_{n+1}|&gt;1\r ]\to 0\]
</p>
<p>
as \(n\to \infty \). But, \(X_n\) and \(X_{n+1}\) are independent and \(\P [X_n=1,X_{n+1}=0]=\frac 14\) so \(\P [|X_n-X_{n+1}|&gt;1]\geq \frac 14\) for all \(n\). Therefore we have a contradiction,
so there is no \(X\) such that \(X_n\to X\) in probability.
</p>
<p>
Hence, by Lemma <a href="notes_1.html#??">??</a>, we can’t have any \(X\) such that \(X_n\to X\) almost surely or in \(L^1\) (since it would imply convergence in probability).
</p>
<p>


</p>
</li>
</ul>
<!--
......   subsection Chapter <a href=notes_1.html#??>??</a> ......
-->
<h5 id="autosec-311">Chapter <a href="notes_1.html#??">??</a></h5>
<a id="notes_1-autopage-311"></a>



<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We have \((C\circ M)_n=\sum _{i=1}^n 0(S_i-S_{i-1})=0.\)
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> We have \((C\circ M)_n=\sum _{i=1}^n 1(S_i-S_{i-1})=S_n-S_0=S_n.\)
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(c)</span></span> We have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{6}\)</span>


<!--

                                                                                                       n
                                                                                                       X
                                                                                       (C ◦ M )n =           Si−1 (Si − Si−1 )
                                                                                                       i=1
                                                                                                       Xn
                                                                                                   =         (X1 + . . . + Xi−1 )Xi
                                                                                                       i=1
                                                                                                        n
                                                                                                                    !2        n
                                                                                                     1 X                   1X
                                                                                                   =       Xi            −      X2
                                                                                                     2 i=1                 2 i=1 i
                                                                                                       Sn2   n
                                                                                                   =       −
                                                                                                        2    2


-->


<p>


\begin{align*}
(C\circ M)_n &amp;=\sum _{i=1}^n S_{i-1}(S_i-S_{i-1})\\ &amp;=\sum _{i=1}^n (X_1+\ldots +X_{i-1})X_i\\ &amp;=\frac 12\l (\sum _{i=1}^n X_i\r )^2-\frac 12\sum _{i=1}^n X_i^2\\
&amp;=\frac {S_n^2}{2}-\frac {n}{2}
\end{align*}
In the last line we use that \(X_i^2=1\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{6}\)</span>


<!--

                                                                                       n
                                                                                       X
                                                                         (X ◦ M )n =     (αCn−1 + βDn−1 )(Mn − Mn−1 )
                                                                                       i=1
                                                                                         Xn                               n
                                                                                                                          X
                                                                                   =α         Cn−1 (Mn − Mn−1 ) + β             Dn−1 (Mn − Mn−1 )
                                                                                        i=1                               i=1

                                                                                   = α(C ◦ M )n + β(C ◦ M )n



-->


<p>


\begin{align*}
(X\circ M)_n &amp;=\sum \limits _{i=1}^n (\alpha C_{n-1}+\beta D_{n-1})(M_n-M_{n-1})\\ &amp;=\alpha \sum \limits _{i=1}^n C_{n-1}(M_n-M_{n-1}) + \beta \sum \limits _{i=1}^n
D_{n-1}(M_n-M_{n-1})\\ &amp;=\alpha (C\circ M)_n + \beta (C\circ M)_n
\end{align*}
as required.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We note that
</p>
<p>
\[|S_n|\leq \sum \limits _{i=1}^n|X_i|\leq \sum \limits _{i=1}^n 1=n&lt;\infty .\]
</p>
<p>
Hence \(S_n\in L^1\). Since \(X_i\in \mc {F}_n\) for all \(i\leq n\), we have also that \(S_n\in m\mc {F}_n\). Lastly,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{6}\)</span>


<!--



                                                                                         E[Sn+1 | Fn ] = E[Sn + Xn+1 | Fn ]

                                                                                                        = Sn + E[Xn+1 ]

                                                                                                        = Sn .



-->


<p>


\begin{align*}
\E [S_{n+1}\|\mc {F}_n] &amp;=\E [S_n+X_{n+1}\|\mc {F}_n]\\ &amp;=S_n+\E [X_{n+1}]\\ &amp;=S_n.
\end{align*}
Therefore, \(S_n\) is a martingale. In the above, in the second line we use that \(S_n\in m\mc {F}_n\) and that \(X_{n+1}\) is independent of \(\mc {F}_n\). To deduce the last line we use that \(\E
[X_i]=\frac {1}{2n^2}(1)=\frac {1}{2n^2}(-1)+(1-\frac {1}{n^2})(0)=0\).
</p>
<p>
We next check that \(S_n\) is uniformly bounded in \(L^1\). We have \(\E [|X_i|]=\frac {1}{2i^2}|1|+\frac {1}{2i^2}|-1|+(1-\frac {1}{i^2})|0|=\frac {1}{i^2}\). Using that \(|S_n|\leq
|X_1|+\ldots +|X_n|\), monotonicity of expectations gives that
</p>
<p>
\[\E [|S_n|]\leq \sum _{i=1}^n \E [|X_i|]\leq \sum _{i=1}^n \frac {1}{i^2}\leq \sum _{i=1}^\infty \frac {1}{i^2}&lt;\infty .\]
</p>
<p>
Hence \(\sup _n\E [|S_n|]&lt;\infty \), so \((S_n)\) is uniformly bounded in \(L^1\).
</p>
<p>
Therefore, the martingale convergence theorem applies, which gives that there exists some real-valued random variable \(S_\infty \) such that \(S_n\stackrel {a.s.}{\to }S_\infty \).
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Note that \((S_n)\) takes values in \(\Z \) and that, from part (b), almost surely we have that \(S_n\) converges to a real value. It
follows immediately by Lemma <a href="notes_1.html#??">??</a> that (almost surely) there exists \(N\in \N \) such that \(S_n=S_\infty \) for all \(n\geq N\).
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Here’s some <kbd>R</kbd> code to plot \(1000\) time-steps of the symmetric random walk. Changing the value given to
<kbd>set.seed</kbd> will generate a new sample.
</p>
<pre class="verbatim">
> T=1000
> set.seed(1)
> x=c(0,2*rbinom(T-1,1,0.5)-1)
> y=cumsum(x)
> par(mar=rep(2,4))
> plot(y,type="l")


</pre>
<p>
Adapting it to plot the asymmetric case and the random walk from exercise <a href="notes_1.html#??">??</a> is left for you.
</p>
<p>
In case you prefer Python, here’s some Python code that does the same job. (I prefer Python.)
</p>
<pre class="verbatim">
import numpy as np
import matplotlib.pyplot as plt

p = 0.5
rr = np.random.random(1000)
step = 2*(rr < p) - 1

start = 0
positions = [start]
for x in step:
    positions.append(positions[-1] + x)

plt.plot(positions)
plt.show()


</pre>
<p>
Extending this code as suggested is left for you. For the last part, the difference in behaviour that you should notice from the graph is that the walk from Exercise <a href="notes_1.html#??">??</a> eventually
stops moving (at some random time), whereas the random walk from Q2 of Assignment 3 never keeps moving but makes smaller and smaller steps.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We will argue by contradiction. Suppose that \((M_n)\) was uniformly bounded in \(L^1\). Then the martingale convergence theorem
would give that \(M_n\stackrel {a.s.}{\to }M_\infty \) for some real valued random variable \(M_\infty \). However, \(M_n\) takes values in \(\Z \) and we have \(M_{n+1}\neq M_n\) for all \(n\), so
Lemma <a href="notes_1.html#??">??</a> gives that \((M_n)\) cannot converge almost surely. This is a contradiction.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> Take the offspring distribution to be \(\P [G=0]=1\). Then \(Z_1=0\), and hence \(Z_n=0\) for all \(n\geq 1\), so \(Z_n\) dies out.
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Take the offspring distribution to be \(\P [G=2]=1\). Then \(Z_{n+1}=2Z_n\), so \(Z_n=2^n\), and hence \(Z_n\to \infty \).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{6}\)</span>


<!--



                                                                    E[Mn+1 | Fn ] = E Mn+1 1{nth draw is red} Fn + E Mn+1 1{nth draw is black} Fn
                                                                                      h                                                i          h                                     i


                                                                                       Bn                                  B +1
                                                                                                                                                                                         
                                                                                 =E         1{nth draw is red} Fn + E n           1{nth draw is black} Fn
                                                                                      n+3                                   n+3
                                                                                    Bn                             i B +1 h
                                                                                         E 1{nth draw is red} Fn +             E 1{nth draw is black} Fn
                                                                                          h                                                              i
                                                                                                                         n
                                                                                 =
                                                                                   n+3                                  n+3
                                                                                    Bn Bn         Bn + 1         Bn
                                                                                                                    
                                                                                 =             +           1−
                                                                                   n+3n+2         n+3           n+2
                                                                                          2
                                                                                         Bn          Bn (n + 2) + (n + 2) − Bn2 − Bn
                                                                                 =                +
                                                                                   (n + 2)(n + 3)             (n + 2)(n + 3)
                                                                                   (n + 1)Bn + (n + 2)
                                                                                 =
                                                                                      (n + 2)(n + 3)


-->


<p>


\begin{align*}
\E [M_{n+1}\|\mc {F}_n] &amp;=\E \l [M_{n+1}\1\{n^{th}\text { draw is red}\}\,\Big |\,\mc {F}_n\r ] +\E \l [M_{n+1}\1\{n^{th}\text { draw is black}\}\,\Big |\,\mc {F}_n\r ]\\
&amp;=\E \l [\frac {B_n}{n+3}\,\1\{n^{th}\text { draw is red}\}\,\Big |\,\mc {F}_n\r ] +\E \l [\frac {B_n+1}{n+3}\,\1\{n^{th}\text { draw is black}\}\,\Big |\,\mc {F}_n\r ]\\
&amp;=\frac {B_n}{n+3}\E \l [\1\{n^{th}\text { draw is red}\}\,\Big |\,\mc {F}_n\r ] +\frac {B_n+1}{n+3}\E \l [\,\1\{n^{th}\text { draw is black}\}\,\Big |\,\mc {F}_n\r ]\\
&amp;=\frac {B_n}{n+3}\frac {B_n}{n+2}+\frac {B_n+1}{n+3}\l (1-\frac {B_n}{n+2}\r )\\ &amp;=\frac {B_n^2}{(n+2)(n+3)}+\frac {B_n(n+2)+(n+2)-B_n^2-B_n}{(n+2)(n+3)}\\ &amp;=\frac
{(n+1)B_n+(n+2)}{(n+2)(n+3)}
\end{align*}
This is clearly not equal to \(M_n=\frac {B_n}{n+2}\), so \(M_n\) is not a martingale.
</p>
<p>
<i>This result might be rather surprising, but we should note that the urn process here is not ‘fair’, when considered in terms of the proportion of (say) red balls. If we started with more red balls than black balls,
then over time we would expect to see the proportion of red balls reduce towards \(\frac 12\), as we see in part (b):</i>
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Your conjecture should be that, regardless of the initial state of the urn, \(\P [M_n\to \frac 12\text { as }n\to \infty ]=1\).
(It is true.)
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> Let \(B_n\) be the number of red balls in the urn at time \(n\), and then \(M_n=\frac {B_n}{K}\). Let us abuse notation slightly
and write \(B=r\) for the event that the ball \(X\) is red, and \(X=b\) for the event that the ball \(X\) is black. Let \(X_1\) denote the first ball drawn on the \((n+1)^{th}\) draw, and \(X_2\) denote the second
ball drawn.
</p>
<p>
Let \(\mc {F}_n=\sigma (B_1,\ldots ,B_n)\). It is clear that \(M_n\in [0,1]\), so \(M_n\in L^1\), and that \(M_n\) is measurable with respect to \(\mc {F}_n\). We have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{6}\)</span>


<!--



                                                                    E[Mn+1 | Fn ] = E Mn+1 1{X1 =r,X2 =r} | Fn + E Mn+1 1{X1 =r,X2 =b} | Fn
                                                                                      h                                    i         h                                        i


                                                                                      + E Mn+1 1{X1 =b,X2 =r} | Fn + E Mn+1 1{X1 =b,X2 =b} | Fn
                                                                                               h                                   i          h                                     i

                                                                                      Bn                             B +1
                                                                                                                                                                           
                                                                                 =E      1{X1 =r,X2 =r} | Fn + E n         1{X1 =r,X2 =b} | Fn
                                                                                       K                               K
                                                                                           Bn − 1                            Bn
                                                                                                                                                 
                                                                                      +E           1{X1 =b,X2 =r} | Fn + E      1{X1 =b,X2 =b} | Fn
                                                                                             K                               K
                                                                                   Bn h                     i B +1 h
                                                                                      E 1{X1 =r,X2 =r} | Fn +           E 1{X1 =r,X2 =b} | Fn
                                                                                                                                              i
                                                                                                                   n
                                                                                 =
                                                                                   K                                K
                                                                                        Bn − 1 h                      i B
                                                                                               E 1{X1 =b,X2 =r} | Fn +       E 1{X1 =b,X2 =b} | Fn
                                                                                                                              h                    i
                                                                                                                           n
                                                                                      +
                                                                                          K                               K
                                                                                   Bn Bn Bn Bn + 1 Bn K − Bn
                                                                                 =           +
                                                                                   K K K           K K          K
                                                                                        Bn − 1 K − Bn Bn Bn K − Bn K − Bn
                                                                                      +                      +
                                                                                          K        K       K     K      K      K
                                                                                    1                                                                
                                                                                         3       2
                                                                                 = 3 Bn + 2Bn (K − Bn ) + Bn (K − B)n − Bn (K − Bn ) + Bn (K − Bn )2
                                                                                   K
                                                                                    1        
                                                                                 = 3 K 2 Bn
                                                                                   K
                                                                                 = Mn



-->


<p>


\begin{align*}
\E [M_{n+1}\|\mc {F}_{n}] &amp;=\E \l [M_{n+1}\1_{\{X_1=r, X_2=r\}}\|\mc {F}_n\r ]+\E \l [M_{n+1}\1_{\{X_1=r, X_2=b\}}\|\mc {F}_n\r ]\\ &amp;\hspace {2pc}+\E \l
[M_{n+1}\1_{\{X_1=b, X_2=r\}}\|\mc {F}_n\r ]+\E \l [M_{n+1}\1_{\{X_1=b, X_2=b\}}\|\mc {F}_n\r ]\\ &amp;=\E \l [\frac {B_n}{K}\1_{\{X_1=r, X_2=r\}}\|\mc {F}_n\r ]+\E \l [\frac
{B_n+1}{K}\1_{\{X_1=r, X_2=b\}}\|\mc {F}_n\r ]\\ &amp;\hspace {2pc}+\E \l [\frac {B_n-1}{K}\1_{\{X_1=b, X_2=r\}}\|\mc {F}_n\r ]+\E \l [\frac {B_n}{K}\1_{\{X_1=b, X_2=b\}}\|\mc
{F}_n\r ]\\ &amp;=\frac {B_n}{K}\E \l [\1_{\{X_1=r, X_2=r\}}\|\mc {F}_n\r ]+\frac {B_n+1}{K}\E \l [\1_{\{X_1=r, X_2=b\}}\|\mc {F}_n\r ]\\ &amp;\hspace {2pc}+\frac {B_n-1}{K}\E \l
[\1_{\{X_1=b, X_2=r\}}\|\mc {F}_n\r ]+\frac {B_n}{K}\E \l [\1_{\{X_1=b, X_2=b\}}\|\mc {F}_n\r ]\\ &amp;=\frac {B_n}{K}\frac {B_n}{K}\frac {B_n}{K}+\frac {B_n+1}{K}\frac
{B_n}{K}\frac {K-B_n}{K}\\ &amp;\hspace {2pc}+\frac {B_n-1}{K}\frac {K-B_n}{K}\frac {B_n}{K}+\frac {B_n}{K}\frac {K-B_n}{K}\frac {K-B_n}{K}\\ &amp;=\frac {1}{K^3}\l
(B_n^3+2B_n^2(K-B_n)+B_n(K-B)n-B_n(K-B_n)+B_n(K-B_n)^2\r )\\ &amp;=\frac {1}{K^3}\l (K^2B_n\r )\\ &amp;=M_n
\end{align*}


</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Since \(M_n\in [0,1]\) we have \(\E [|M_n|]\leq 1\), hence \(M_n\) is uniformly bounded in \(L^1\). Hence, by the martingale
convergence theorem there exists a random variable \(M_\infty \) such that \(M_n\stackrel {a.s.}{\to }M_\infty \).
</p>
<p>
Since \(M_n=\frac {B_n}{K}\), this means that also \(B_n\stackrel {a.s.}{\to }KM_\infty \). However, \(B_n\) is integer valued so, by Lemma <a href="notes_1.html#??">??</a>, if \(B_n\) is to
converge then it must eventually become constant. This can only occur if, eventually, the urn contains either (i) only red balls or (ii) only black balls.
</p>
<p>
In case (i) we have \(M_n=0\) eventually, which means that \(M_\infty =0\). In case (ii) we have \(M_n=1\) eventually, which means that \(M_\infty =1\). Hence, \(M_\infty \in \{0,1\}\) almost surely.
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> Consider the <i>usual</i> Po&#x0301;lya urn, from Section <a href="notes_1.html#??">??</a>, started with just one red ball
and one black ball. After the first draw is complete we have two possibilities:
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker">1.</span> The urn contains one red ball and two black balls.
</p>


</li>
<li>


<p>
<span class="listmarker">2.</span> The urn contains two red balls and one black ball.
</p>
</li>
</ul>
<p>
In the first case we reach precisely the initial state of the urn described in <a href="notes_1.html#??">??</a>. In the second case we also reach the initial state of the urn described in <a
href="notes_1.html#??">??</a>, but with the colours red and black swapped.
</p>
<p>
If \(M_n\stackrel {a.s.}{\to } 0\), then in the first case the fraction of red balls would tend to zero, and (by symmetry of colours) in the second case the limiting fraction of black balls would tend to zero; thus
the limiting fraction of black balls would always be either \(1\) (the first case) or \(0\) (the second case). However, we know from Proposition <a href="notes_1.html#??">??</a> that the limiting fraction of
black balls is actually uniform on \((0,1)\). Hence, \(\P [M_n\to 0]=0\).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> You should discover that having a higher proportion of initial red balls makes it more likely for \(M_\infty \) (the limiting proportion
of red balls) to be closer to \(1\). However, since \(M_\infty \) is a random variable in this case, you may need to take several samples of the process (for each initial condition that you try) to see the effect.
</p>
<p>
<b>Follow-up challenge question:</b> In fact, the limiting value \(M_\infty \) has a Beta distribution, \(Be(\alpha ,\beta )\). You may need to look up what this means, if you haven’t seen the Beta
distribution before. Consider starting the urn with \(n_1\) red and \(n_2\) black balls, and see if you can use your simulations to guess a formula for \(\alpha \) and \(\beta \) in terms of \(n_1\) and \(n_2\)
(the answer is a simple formula!).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We have that \((M_n)\) is a non-negative martingale, hence \(\sup _n\E [|M_n|]=\sup _n\E [M_n]=\E [M_0]=1\). Thus
\((M_n)\) is uniformly bounded in \(L^1\) and the (first) martingale convergence theorem applies, giving that \(M_n\stackrel {a.s.}{\to }M_\infty \) for a real valued \(M_\infty \). Since \(M_n\geq 0\)
almost surely we must have \(M_\infty \geq 0\) almost surely.
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> The possible values of \(M_n\) are \(W=\{(q/p)^{z}\-z\in \Z \}\). Note that \(M_{n+1}\neq M_n\) for all \(n\in \N \).
</p>
<p>
On the event that \(M_\infty &gt;0\) there exists \(\epsilon &gt;0\) such that \(M_n\geq \epsilon \) for all sufficiently large \(n\). Choose \(m\in \N \) such that \((q/p)^m\leq \epsilon \) and then for
all sufficiently large \(n\) we have \(M_n\in W_m=\{(q/p)^z\-z\geq m, z\in \Z \}\). For such \(n\), since \(M_n\neq M_{n+1}\) we have \(|M_n-M_{n+1}|\geq |(q/p)^m-(q/p)^{m-1}|&gt;0\), which implies
that \((M_n)\) could not converge. Hence, the only possible almost sure limit is \(M_\infty =0\).
</p>
<p>
If \((M_n)\) was uniformly bounded in \(L^2\) then by the (second) martingale convergence theorem we would have \(\E [M_n]\to \E [M_\infty ]\). However \(\E [M_n]=\E [M_0]=1\) and \(\E [M_\infty
]=0\), so this is a contradiction. Hence \((M_n)\) is not uniformly bounded in \(L^2\).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(c)</span></span> We have \((q/p)^S_n\stackrel {a.s.}{\to }0\). Noting that \(q/p&gt;1\) and hence \(\log (q/p)&gt;0\), taking logs gives
that \(S_n\log (q/p)\stackrel {a.s.}{\to }-\infty \), which gives \(S_n\stackrel {a.s.}{\to }-\infty \).
</p>
<p>
For the final part, if \((S_n)\) is a simple asymmetric random walk with upwards drift, then \((-S_n)\) is a simple asymmetric random walk with downwards drift. Applying what we’ve just discovered thus gives
\(-S_n\stackrel {a.s.}{\to }-\infty \), and multiplying by \(-1\) gives the required result.
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> This is a trivial induction: if \(S_n\) is even then \(S_{n+1}\) is odd, and vice versa.
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> By part (a), \(p_{2n+1}=0\) since \(0\) is an even number and \(S_{2n+1}\) is an odd number.
</p>
<p>
If \(S_n\) is to start at zero and reach zero at time \(2n\), then during time \(1,2,\ldots ,2n\) it must have made precisely \(n\) steps upwards and precisely \(n\) steps downwards. The number of different
ways in which the walk can do this is \(\binom {2n}{n}\) (we are choosing exactly \(n\) steps on which to go upwards, out of \(2n\) total steps). Each one of these ways has probability \((\frac 12)^{2n}\) of
occurring, so we obtain \(p_{2n}=\binom {2n}{n}2^{-2n}\).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(c)</span></span> From (b) we note that
</p>
<p>
\[p_{2(n+1)}=\frac {(2n+2)(2n+1)}{(n+1)(n+1)}2^{-2}p_{2n}=2\l (\frac {2n+1}{n+1}\r )2^{-2}p_n=\l (\frac {2n+1}{2n+2}\r )p_{2n}=\l (1-\frac {1}{2(n+1)}\r )\frac 12 p_{2n}.\]
</p>
<p>
Hence, using the hint, we have
</p>
<p>
\[p_{2(n+1)}\leq \exp \l (-\frac {1}{2(n+1)}\r )p_{2n}.\]
</p>
<p>
Iterating this formula obtains that
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{6}\)</span>


<!--


                                                                                                  1          1                  1
                                                                                                                                                                     
                                                                                     p2n ≤ exp −    exp −          . . . exp −      p0
                                                                                                 2n       2(n − 1)             2(1)
                                                                                                     n
                                                                                                                   !
                                                                                                  1X    1
                                                                                          = exp −         .                                        (A.7)                                           --><a id="eq:ssrw_p2n"></a><!--
                                                                                                  2 i=1 i


-->


<p>


\begin{align}
p_{2n} &amp;\leq \exp \l (-\frac {1}{2n}\r )\exp \l (-\frac {1}{2(n-1)}\r )\ldots \exp \l (-\frac {1}{2(1)}\r )p_0\notag \\ &amp;=\exp \l (-\frac {1}{2}\sum \limits _{i=1}^n\frac
{1}{i}\r ).\label {eq:ssrw_p2n}
\end{align}
Recall that \(\sum _{i=1}^n\frac {1}{i}\to \infty \) as \(n\to \infty \), and that \(e^{-x}\to 0\) as \(x\to \infty \). Hence, the right hand side of the expression above tends to zero as \(n\to
\infty \). Hence, since \(p_{n}\geq 0\), by the sandwich we have \(p_{2n}\to 0\) as \(n\to \infty \).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We note that
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{7}\)</span>


<!--


                                                                                                 Sn+1            Sn        1
                                                                                                              
                                                                                      E                 Fn =           +         E[Xn+1 | Fn ]
                                                                                              f (n + 1)      f (n + 1) f (n + 1)
                                                                                                                 Sn        1
                                                                                                           =           +         E[Xn+1 ]
                                                                                                             f (n + 1) f (n + 1)
                                                                                                                 Sn
                                                                                                           =
                                                                                                             f (n + 1)


-->


<p>


\begin{align*}
\E \l [\frac {S_{n+1}}{f(n+1)}\,\Big |\,\mc {F}_n\r ]&amp;=\frac {S_n}{f(n+1)}+\frac {1}{f(n+1)}\E [X_{n+1}\|\mc {F}_n]\notag \\ &amp;=\frac {S_n}{f(n+1)}+\frac {1}{f(n+1)}\E
[X_{n+1}]\notag \\ &amp;=\frac {S_n}{f(n+1)}
\end{align*}
Here we use that \(S_n\in m\mc {F}_n\) and that \(X_{n+1}\) is independent of \(\mc {F}_n\).
</p>
<p>
Since \(\frac {S_n}{f(n)}\) is a martingale we must have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{7}\)</span>
<!--

                                                            Sn       Sn
                                                                  =       .                                                                                  (A.8)                                    --><a id="eq:ESnfn"></a><!--
                                                        f (n + 1)   f (n)
-->
<p>


\begin{equation}
\label {eq:ESnfn} \frac {S_n}{f(n+1)}=\frac {S_n}{f(n)}.
\end{equation}


</p>
<p>
Since \(\P [S_n\neq 0]&gt;0\) this implies that \(\frac {1}{f(n+1)}=\frac {1}{f(n)}\), which implies that \(f(n+1)=f(n)\). Since this holds for any \(n\), we have \(f(1)=f(n)\) for all \(n\); so \(f\) is
constant.
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> If \(\frac {S_n}{f(n)}\) is to be a supermartingale then, in place of <span class="textup">(<a
href="notes_1.html#??">??</a>)</span>, we would need
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{8}\)</span>
<!--

                                                         Sn       Sn
                                                               ≤       .                                                                      (A.9)                                             --><a id="eq:ssrw_fnmart"></a><!--
                                                     f (n + 1)   f (n)
-->
<p>


\begin{equation}
\label {eq:ssrw_fnmart} \frac {S_n}{f(n+1)}\leq \frac {S_n}{f(n)}.
\end{equation}


</p>
<p>
Note that we would need this equation to hold true with probability one. To handle the \(\geq \) we now need to care about the sign of \(S_n\). The key idea is that \(S_n\) can be both positive or negative, so
the only way that <span class="textup">(<a href="notes_1.html#??">??</a>)</span> can hold is if \(\frac {1}{f(n+1)}=\frac {1}{f(n)}\).
</p>
<p>
We will use some of the results from the solution of exercise <a href="notes_1.html#??">??</a>. In particular, from for odd \(n\in \N \) from <a href="notes_1.html#??">??</a>(b) we have
\(P[S_n=0]=0\), and for even \(n\in \N \) from <span class="textup">(<a href="notes_1.html#??">??</a>)</span> we have \(\P [S_n=0]&lt;1\). In both cases we have \(\P [S_n\neq 0]&gt;0\). Since
\(S_n\) and \(-S_n\) have the same distribution we have \(\P [S_n&lt;0]=\P [-S_n&lt;0]=\P [S_n&gt;0]\), and hence
</p>
<p>
\[\P [S_n\neq 0]=\P [S_n&lt;0]+\P [S_n&gt;0]=2\P [S_n&lt;0]=2\P [S_n&gt;0].\]
</p>
<p>
Hence, for all \(n\in \N \) we have \(\P [S_n&gt;0]&gt;0\) and \(\P [S_n&lt;0]&gt;0\).
</p>
<p>
Now, consider \(n\geq 1\). We have that here is positive probability that \(S_n&gt;0\). Hence, by <span class="textup">(<a href="notes_1.html#??">??</a>)</span>, we must have \(\frac
{1}{f(n+1)}\leq \frac {1}{f(n)}\), which means that \(f(n+1)\geq f(n)\). But, there is also positive probability that \(S_n&lt;0\), hence by <span class="textup">(<a
href="notes_1.html#??">??</a>)</span> we must have \(\frac {1}{f(n+1)}\geq \frac {1}{f(n)}\), which means that \(f(n+1)\leq f(n)\). Hence \(f(n+1)=f(n)\), for all \(n\), and by a trivial
induction we have \(f(1)=f(n)\) for all \(n\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> Since \(M_n=\frac {Z_n}{\mu ^n}\) we note that
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{9}\)</span>


<!--


                                                                                                     1
                                                                                                             (Zn+1 − µZn )2 = (Mn+1 − Mn )2 .                             (A.10)                   --><a id="eq:gw_sqr_1"></a><!--
                                                                                                   µ2(n+1)


-->


<p>


\begin{align}
\label {eq:gw_sqr_1} \frac {1}{\mu ^{2(n+1)}}(Z_{n+1}-\mu Z_n)^2=(M_{n+1}-M_n)^2.
\end{align}
Further,
</p>
<p>
\[Z_{n+1}-\mu Z_n=\sum \limits _{i=1}^{Z_n}(X^{n+1}_i-\mu )\]
</p>
<p>
so it makes sense to define \(Y_i=X^{n+1}_i-\mu \). Then \(\E [Y_i]=0\),
</p>
<p>
\[\E [Y_i^2]=\E [(X^{n+1}_i-\mu )^2]=\var (X^{n+1}_i)=\sigma ^2,\]
</p>
<p>
and \(Y_1,Y_2,\ldots ,Y_n\) are independent. Moreover, the \(Y_i\) are identically distributed and independent of \(\mc {F}_n\). Hence, by taking out what is known (\(Z_n\in m\mc {F}_n\)) we have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{10}\)</span>


<!--

                                                                                                                               
                                                                                                                                     Zn
                                                                                                                                                  !2         
                                                                                      h                            i                 X
                                                                                    E (Zn+1 − µZn )2 | Fn = E                               Yi           Fn 
                                                                                                                                       i=1
                                                                                                                           Zn
                                                                                                                           X                               Zn
                                                                                                                                                           X
                                                                                                                       =           E[Yi2 | Fn ] +                  E[Yi Yj | Fn ]
                                                                                                                           i=1                             i,j=1
                                                                                                                                                           i̸=j
                                                                                                                           Zn
                                                                                                                           X                       Zn
                                                                                                                                                   X
                                                                                                                       =           E[Yi2 ] +               E[Yi ]E[Yj ]
                                                                                                                           i=1                     i,j=1
                                                                                                                                                   i̸=j

                                                                                                                       = Zn E[Y1 ]2 + 0

                                                                                                                       = Zn σ 2 .



-->


<p>


\begin{align*}
\E \l [(Z_{n+1}-\mu Z_n)^2\|\mc {F}_n\r ] &amp;=\E \l [\l (\sum \limits _{i=1}^{Z_n}Y_{i}\r )^2\,\bigg |\,\mc {F}_n\r ]\\ &amp;=\sum \limits _{i=1}^{Z_n}\E [Y_{i}^2\|\mc
{F}_n]+\sum \limits _{\stackrel {i,j=1}{i\neq j}}^{Z_n}\E [Y_iY_j\|\mc {F}_n]\\ &amp;=\sum \limits _{i=1}^{Z_n}\E [Y_{i}^2]+\sum \limits _{\stackrel {i,j=1}{i\neq j}}^{Z_n}\E
[Y_i]\E [Y_j]\\ &amp;=Z_n\E [Y_1]^2+0\\ &amp;=Z_n\sigma ^2.
\end{align*}
So, from <span class="textup">(<a href="notes_1.html#??">??</a>)</span> we obtain
</p>
<p>
\[\E [(M_{n+1}-M_n)^2\|\mc {F}_n]=\frac {Z_n\sigma ^2}{\mu ^{2(n+1)}}.\]
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> By part (a) and <span class="textup">(<a href="notes_1.html#??">??</a>)</span>, we have
</p>
<p>
\[\E [M_{n+1}^2\|\mc {F}_n]=M_n^2+\frac {Z_n\sigma ^2}{\mu ^{2(n+1)}}.\]
</p>
<p>
Taking expectations, and using that \(\E [Z_n]=\mu ^n\), we obtain
</p>
<p>
\[\E [M_{n+1}^2]=\E [M_n^2]+\frac {\sigma ^2}{\mu ^{n+2}}.\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> Note that \(X_{n+1}\geq X_n\geq 0\) for all \(n\), because taking an \(\inf \) over \(k&gt;n+1\) is an infimum over a smaller set
than over \(k&gt;n\). Hence, \((X_n)\) is monotone increasing and hence the limit \(X_\infty (\omega )=\lim _{n\to \infty }X_n(\omega )\) exists (almost surely).
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Since \(|M_n|\leq \inf _{k&gt;n}|M_k|\), we have \(X_n\leq |M_n|\). By definition of \(\inf \), for each \(\epsilon &gt;0\)
and \(n\in \N \) there exists some \(n&apos;\geq n\) such that \(X_n\geq |X_{n&apos;}|-\epsilon \). Combining these two properties, we obtain
</p>
<p>
\[|M_{n&apos;}|-\epsilon \leq X_n\leq |M_n|.\]
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(c)</span></span> Letting \(n\to \infty \), in the above equation, which means that also \(n&apos;\to \infty \) because \(n&apos;\geq n\), we
take an almost sure limit to obtain \(|M_\infty |-\epsilon \leq X_\infty \leq |M_\infty |\). Since we have this inequality for any \(\epsilon &gt;0\), in fact \(X_\infty =|M_\infty |\) almost surely.
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(d)</span></span> We have shown that \((X_n)\) is an increasing sequence and \(X_n\stackrel {a.s.}{\to } X_\infty \), and since each
\(|M_n|\geq 0\), we have also \(X_n\geq 0\). Hence, the monotone convergence theorem applies to \(X_n\) and \(X_\infty \), and therefore \(\E [X_n]\to \E [X_\infty ]\).
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(e)</span></span> From the monotone convergence theorem, writing in terms of \(M_n\) rather than \(X_n\), we obtain
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{10}\)</span>
<!--

                                                            
                                              E inf |Mn | → E[|M∞ |].                                                                             (A.11)                                             --><a id="eq:Mnconv"></a><!--
                                                 k≥n

-->
<p>


\begin{equation}
\label {eq:Mnconv} \E \l [\inf _{k\geq n}|M_n|\r ]\to \E [|M_\infty |].
\end{equation}


</p>
<p>
However, since \(\inf _{k\geq n}|M_n|\leq |M_n|\leq C=\sup _{j\in \N } \E [|M_j|]\), the monotonicity property of expectation gives us that, for all \(n\),
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{11}\)</span>
<!--

                                                         
                                               E inf |Mn | ≤ C.                                                     (A.12)                                                  --><a id="eq:Mn_bound"></a><!--
                                                  k≥n

-->
<p>


\begin{equation}
\label {eq:Mn_bound} \E \l [\inf _{k\geq n}|M_n|\r ]\leq C.
\end{equation}


</p>
<p>
Combining <span class="textup">(<a href="notes_1.html#??">??</a>)</span> and <span class="textup">(<a href="notes_1.html#??">??</a>)</span>, with the fact that limits preserve weak
inequalities, we obtain that \(\E [|M_\infty |]\leq C\), as required.
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> All the \(\wt {X}^{n+1}_i\) have the same distribution, and are independent by independence of the \(X^{n+1}_i\) and
\(C^{n+1}_i\). Thus, \(\wt {Z}_n\) is a Galton-Watson process with off-spring distribution \(\wt {G}\) given by the common distribution of the \(X^{n+1}_i\).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> By definition of \(\wt {X}^{n+1}_i\) we have \(X^{n+1}_i\leq \wt {X}^{n+1}_i\). Hence, from <span class="textup">(<a
href="notes_1.html#??">??</a>)</span>, if \(Z_{n}\leq \wt {Z}_n\) then also \(Z_{n+1}\leq \wt {Z}_{n+1}\). Since \(Z_0=\wt {Z}_0=1\), an easy induction shows that \(Z_n\leq \wt {Z}_n\) for all
\(n\in \N \).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(c)</span></span> We have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--


                                                                                               h        i
                                                                                                e n+1
                                                                                      f (α) = E X i
                                                                                                                     ∞
                                                                                                                     X
                                                                                           = P[G = 0]P[C = 1] +             iP[G = i]
                                                                                                                      i=1
                                                                                                            ∞
                                                                                                            X
                                                                                           = αP[G = 0] +          iP[G = i].
                                                                                                            i=1



-->


<p>


\begin{align*}
f(\alpha )&amp;=\E \l [\wt {X}^{n+1}_i\r ]\\ &amp;=\P [G=0]\P [C=1]+\sum \limits _{i=1}^\infty i\P [G=i]\\ &amp;=\alpha \P [G=0]+\sum \limits _{i=1}^\infty i\P [G=i].
\end{align*}
Recall that each \(X^{n+1}_i\) has the same distribution as \(G\). Hence,
</p>
<p>
\[\mu =\E [X^{n+1}_i]=\E [G]=\sum _{i=1}^\infty i\P [G=i]=f(0).\]
</p>
<p>
Since \(\mu &lt;1\) we therefore have \(f(0)&lt;1\). Moreover,
</p>
<p>
\[f(1)=\P [G=0]+\sum _{i=1}^\infty i\P [G=i]\geq \sum _{i=0}^\infty \P [G=i]=1.\]
</p>
<p>
It is clear that \(f\) is continuous (in fact, \(f\) is linear). Hence, by the intermediate value theorem, there is some \(\alpha \in [0,1]\) such that \(f(\alpha )=1\).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(d)</span></span> By (c), we can choose \(\alpha \) such that \(f(\alpha )=1\). Hence, \(\E [\wt {X}^{n+1}_i]=1\), so by (a) \(\wt {Z}_n\) is a
Galton-Watson process with an offspring distribution that has mean \(\hat {\mu }=1\). By Lemma <a href="notes_1.html#??">??</a>, \(\wt {Z}\) dies out, almost surely. Since, from (b), \(0\leq Z_n\leq
\wt {Z}_n\), this means \(Z_n\) must also die out, almost surely.
</p>
</li>
</ul>
</li>
</ul>
<!--
......   subsection Chapter <a href=notes_1.html#??>??</a> ......
-->
<h5 id="autosec-315">Chapter <a href="notes_1.html#??">??</a></h5>
<a id="notes_1-autopage-315"></a>



<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We have \(U^{-1}&lt;1\) so, for all \(\omega \in \Omega \), \(X_n(\omega )=U^{-n}(\omega )\to 0\) as \(n\to \infty \).
Hence \(X_n(\omega )\to 0\) almost surely as \(n\to \infty \). Further, \(|X_n|\leq 1\) for all \(n\in \N \) and \(\E [1]=1&lt;\infty \), so the dominated convergence theorem applies and shows that
\(\E [X_n]\to \E [0]=0\) as \(n\to \infty \).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We check the two conditions of the dominated convergence theorem. To check the first condition, note that if \(n\geq |X|\) then
\(X\1_{\{|X|\leq n\}}=X\). Hence, since \(|X|&lt;\infty \), as \(n\to \infty \) we have \(X_n=X\1_{\{|X|\leq n\}}\to X\) almost surely.
</p>
<p>
To check the second condition, set \(Y=|X|\) and then \(\E [|Y|]=\E [|X|]&lt;\infty \) so \(Y\in L^1\). Also, \(|X_n|=|X\1_{\{|X|\leq n\}}|\leq |X|=Y\), so \(Y\) is a dominated random variable for
\((X_n)\). Hence, the dominated convergence theorem applies and \(\E [X_n]\to \E [X]\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We look to use the dominated convergence theorem. For any \(\omega \in \Omega \) we have \(Z(\omega )&lt;\infty \), hence
for all \(n\in \N \) such that \(n&gt;Z(\omega )\) we have \(X_n(\omega )=0\). Therefore, as \(n\to \infty \), \(X_n(\omega )\to 0\), which means that \(X_n\to 0\) almost surely.
</p>
<p>
We have \(|X_n|\leq Z\) and \(Z\in L^1\), so we can use \(Z\) are the dominating random variable. Hence, by the dominated convergence theorem, \(\E [X_n]\to \E [0]=0\).
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> We have
</p>
<p>
\[\E [Z]=\int _1^\infty x f(x)\,dx=\int _1^\infty x^{-1}\,dx=\l [\log x\r ]_1^\infty =\infty \]
</p>
<p>
which means \(Z\notin L^1\) and also that
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--

                                                                                                            Z ∞
                                                                          E[Xn ] = E[1{Z∈[n,n+1)} Z] =            1{x∈[n,n+1)} xf (x) dx
                                                                                                            1
                                                                                     Z n+1
                                                                                                                                                  n+1
                                                                                                                                                      
                                                                                 =           x−1 dx = [log x]n+1
                                                                                                             n   = log(n + 1) − log n = log           .
                                                                                      n                                                            n


-->


<p>


\begin{align*}
\E [X_n]&amp;=\E [\1_{\{Z\in [n,n+1)\}}Z]=\int _1^\infty \1_{\{x\in [n,n+1)\}}x f(x)\,dx\\ &amp;=\int _{n}^{n+1} x^{-1}\,dx=\l [\log x\r ]_{n}^{n+1}=\log (n+1)-\log n=\log \l
(\frac {n+1}{n}\r ).
\end{align*}
As \(n\to \infty \), we have \(\frac {n+1}{n}=1+\frac {1}{n}\to 1\), hence (using that \(\log \) is a continuous function) we have \(\log (\frac {n+1}{n})\to \log 1=0\). Hence, \(\E [X_n]\to 0\).
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(c)</span></span> Suppose that we wanted to use the DCT in (b). We still have \(X_n\to 0\) almost surely, but any dominating random variable \(Y\)
would have to satisfy \(Y\geq |X_n|\) for all \(n\), meaning that also \(Y\geq Z\), which means that \(\E [Y]\geq \E [Z]=\infty \); thus there is no dominating random variable \(Y\in L^1\). Therefore, we
can’t use the DCT here, but we have shown in (b) that the conclusion of the DCT does hold: we have that \(\E [X_n]\) does tend to zero.
</p>
<p>
We obtain that the conditions of the DCT are <i>sufficient</i> but not <i>necessary</i> for its conclusion to hold.
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Note that \(\{T=n\}=\{T\leq n\}\sc \{T\leq n-1\}.\) If \(T\) is a stopping time then both parts of the right hand side are
elements of \(\mc {F}_n\), hence so is \(\{T=n\}\).
</p>
<p>
Conversely, if \(\{T=n\}\in \mc {F}_n\) for all \(n\), then for \(i\leq n\) we have \(\{T=i\}\in \mc {F}_i\sw \mc {F}_n\). Hence the identity \(\{T\leq n\}=\cup _{i=1}^n\{T=i\}\) shows that
\(\{T\leq n\}\in \mc {F}_n\), and therefore \(T\) is a stopping time.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We can write \(\{R=n\}=\{S_i\neq 0\text { for }i=1,\ldots ,n-1\}\cap \{S_n=0\} =\{S_n=0\}\cap \l (\bigcap
_{i=1}^n\Omega \sc \{S_i=0\}\r )\). By exercise <a href="notes_1.html#??">??</a> we have \(\{S_i=0\}\in \mc {F}_n\) for all \(i\leq n\), hence \(\{R=n\}\in \mc {F}_n\) for all \(n\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We can write
</p>
<p>
\[\{S+T\leq n\}=\bigcup _{k=0}^n\bigcup _{j=0}^{n-k}\{S\leq j\}\cap \{T\leq k\}\]
</p>
<p>
Since \(S\) and \(T\) are both stopping times we have \(\{T\leq n\},\{S\leq n\}\in \mc {F}_n\) for all \(n\in \N \), which means that \(\{T\leq j\},\{S\leq k\}\in \mc {F}_n\) for all \(j,k\leq n\).
Hence \(\{S+T\leq n\}\in \mc {F}_n\) and therefore \(S+T\) is also a stopping time.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> If \(A\in \mc {F}_S\) then \(A\cap \{S\leq n\}\in \mc {F}_n\). Since \(T\) is a stopping time we have \(\{T\leq n\}\in \mc
{F}_n\), hence
</p>
<p>
\[A\cap \{S\leq n\}\cap \{T\leq n\}\in \mc {F}_n.\]
</p>
<p>
As \(S\leq T\) we have \(\{T\leq n\}\sw \{S\leq n\}\), hence \(A\cap \{T\leq n\}\in \mc {F}_n\).
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We have
</p>
<p>
\[T=\inf \{n&gt;0\-B_n=2\}.\]
</p>
<p>
Note that \(T\geq n\) if and only if \(B_i=1\) for all \(i=1,2,\ldots ,n-1\). That is, if and only if we pick a red ball out of the urn at times \(i-1,2,\ldots ,n-1\). Hence,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--


                                                                                                                   12     n−1
                                                                                                     P[T ≥ n] =       ...
                                                                                                                   23      n
                                                                                                                   1
                                                                                                                  = .
                                                                                                                   n


-->


<p>


\begin{align*}
\P [T\geq n] &amp;=\frac {1}{2}\frac {2}{3}\ldots \frac {n-1}{n}\\ &amp;=\frac {1}{n}.
\end{align*}
Therefore, since \(\P [T=\infty ]\leq \P [T\geq n]\) for all \(n\), we have \(\P [T=\infty ]=0\) and \(\P [T&lt;\infty ]=1\).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> We note that \(T\) is a stopping time, with respect to \(\mc {F}_t=\sigma (B_i\-i\leq n)\), since
</p>
<p>
\[\{T\leq n\}=\{\text {a red ball was drawn at some time }i\leq n\}=\bigcup \limits _{i=1}^n\{B_i=2\}.\]
</p>
<p>
We showed in Section <a href="notes_1.html#??">??</a> that \(M_n\) was a martingale. Since \(M_n\in [0,1]\) the process \((M_n)\) is bounded. By part (a) for any \(n\in \N \) we have \(\P [T=\infty
]\leq \P [T\geq n]=\frac {1}{n}\), hence \(\P [T=\infty ]=0\) and \(\P [T&lt;\infty ]=1\). Hence, we have condition (b) of the optional stopping theorem, so
</p>
<p>
\[\E [M_T]=\E [M_0]=\frac 12.\]
</p>
<p>
By definition of \(T\) we have \(B_T=2\). Hence \(M_T=\frac {2}{T+2}\), so we obtain \(\E [\frac {1}{T+2}]=\frac 14\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We showed in exercise <a href="notes_1.html#??">??</a> that, almost surely, there was a point in time at which the urn
contained either all red balls or all black balls. Hence, \(\P [T&lt;\infty ]=1\).
</p>
<p>
Moreover, almost surely, since the urn eventually contains either all red balls or all black balls, we have either \(M_T=1\) (all red balls) or \(M_T=0\) (all black balls).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> We note that \(T\) is a stopping time, since
</p>
<p>
\[\{T\leq n\}=\big \{\text {for some }i\leq n\text { we have }M_i\in \{0,1\}\big \}=\bigcup _{i=0}^n M_i^{-1}(\{0,1\}).\]
</p>
<p>
In exercise <a href="notes_1.html#??">??</a> we showed that \(M_n\) was a martingale. Since \(M_n\in [0,1]\) in fact \(M_n\) is a bounded martingale, and we have \(\P [T&lt;\infty ]\) from above, so
conditions (b) of the optional stopping theorem apply. Hence,
</p>
<p>
\[\E [M_T]=\E [M_0]=\frac {r}{r+b}.\]
</p>
<p>
Lastly, since \(M_T\) is either \(0\) or \(1\) (almost surely) we note that
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--



                                                                                             E[MT ] = E[MT 1{MT =1} + MT 1{MT ]=0} ]

                                                                                                     = E[1{MT =1} ] + 0

                                                                                                     = P[MT = 1].



-->


<p>


\begin{align*}
\E [M_T] &amp;=\E [M_T\1_{\{M_T=1\}}+M_T\1_{\{M_T]=0\}}]\\ &amp;=\E [\1_{\{M_T=1\}}]+0\\ &amp;=\P [M_T=1].
\end{align*}
Hence, \(\P [M_T=1]=\frac {r}{r+b}\).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We use the filtration \(\mc {F}_n=\sigma (N_i\-i\leq n)\). We have \(P_n\in m\mc {F}_n\) and since \(0\leq P_n\leq 1\) we
have also that \(P_n\in L^1\). Also,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--



                                                        E[Pn+1 | Fn ] = E[Pn+1 1{the nth ball was red} | Fn ] + E[Pn+1 1{the nth ball was blue} | Fn ]
                                                                            Nn − 1                                          Nn
                                                                                                                                                                  
                                                                     =E              1{the nth ball was red} Fn + E                 1{the nth ball was blue} Fn
                                                                         2m − n − 1                                     2m − n − 1
                                                                         Nn − 1                                         Nn
                                                                     =            E[1{the nth ball was red} | Fn ] +            E[1{the nth ball was blue} | Fn ]
                                                                       2m − n − 1                                    2m − n − 1
                                                                         Nn − 1     Nn          Nn       2m − n − Nn
                                                                     =                    +
                                                                       2m − n − 1 2m − n 2m − n − 1 2m − n
                                                                         Nn
                                                                     =
                                                                       2m − n
                                                                     = Pn .



-->


<p>


\begin{align*}
\E [P_{n+1}\|\mc {F}_n] &amp;=\E [P_{n+1}\1\{\text {the }n^{th}\text { ball was red}\}\|\mc {F}_n]+\E [P_{n+1}\1\{\text {the }n^{th}\text { ball was blue}\}\|\mc {F}_n]\\
&amp;=\E \l [\frac {N_n-1}{2m-n-1}\1\{\text {the }n^{th}\text { ball was red}\}\,\Big {|}\,\mc {F}_n\r ] +\E \l [\frac {N_n}{2m-n-1}\1\{\text {the }n^{th}\text { ball was
blue}\}\,\Big {|}\,\mc {F}_n\r ]\\ &amp;=\frac {N_n-1}{2m-n-1}\E [\1\{\text {the }n^{th}\text { ball was red}\}\|\mc {F}_n]+\frac {N_n}{2m-n-1}\E [\1\{\text {the }n^{th}\text {
ball was blue}\}\|\mc {F}_n]\\ &amp;=\frac {N_n-1}{2m-n-1}\frac {N_n}{2m-n}+\frac {N_n}{2m-n-1}\frac {2m-n-N_n}{2m-n}\\ &amp;=\frac {N_n}{2m-n}\\ &amp;=P_n.
\end{align*}
Here we use taking out what is known (since \(N_n\in m\mc {F}_n\)), along with the definition of our urn process to calculate e.g.&nbsp;\(\E [\1\{\text {the }n^{th}\text { ball was red}\}\|\mc
{F}_n]\) as a function of \(N_n\). Hence \((P_n)\) is a martingale.
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Since \(0\leq P_n\leq 1\), the process \((P_n)\) is a bounded martingale. The time \(T\) is bounded above by \(2m\), hence
condition (b) for the optional stopping theorem holds and \(\E [P_T]=\E [P_0]\). Since \(P_0=\frac 12\) this gives us \(\E [P_T]=\frac 12\). Hence,
</p>
</li>
</ul>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--



                                                                              P[(T + 1)st ball is red] = P[NT +1 = NT + 1]
                                                                                                            2m−1
                                                                                                             X
                                                                                                        =          P[NT +1 = NT + 1 | T = i]P[T = i]
                                                                                                             i=1
                                                                                                            2m−1
                                                                                                             X   m−1
                                                                                                        =               P[T = i]
                                                                                                             i=1
                                                                                                                 2m − i

                                                                                                        = E[PT ]
                                                                                                         1
                                                                                                        = .
                                                                                                         2


-->


<p>


\begin{align*}
\P [(T+1)^{st}\text { ball is red}] &amp;=\P [N_{T+1}=N_T+1]\\ &amp;=\sum \limits _{i=1}^{2m-1}\P [N_{T+1}=N_T+1\|T=i]\P [T=i]\\ &amp;=\sum \limits _{i=1}^{2m-1}\frac
{m-1}{2m-i}\P [T=i]\\ &amp;=\E [P_T]\\ &amp;=\frac 12.
\end{align*}
as required.
</p>
</li>
</ul>
<!--
......     subsection Chapter <a href=notes_1.html#??>??</a> ......
-->
<h5 id="autosec-316">Chapter <a href="notes_1.html#??">??</a></h5>
<a id="notes_1-autopage-316"></a>



<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We have that \(T\) is a stopping time and (from Section <a href="notes_1.html#??">??</a>) \(M_n=S_n-(p-q)n\) is a
martingale. We want to apply the optional stopping theorem to \((M_n)\).
</p>
<p>
We have \(\E [T]&lt;\infty \) from Lemma <a href="notes_1.html#??">??</a> and for all \(n\) we have
</p>
<p>
\[|M_{n+1}-M_n|\leq |S_{n+1}-S_n|+|p-q|= 1+|p-q|.\]
</p>
<p>
Thus, condition (c) for the optional stopping theorem applies and \(\E [M_T]=\E [M_0]\). That is,
</p>
<p>
\[\E [S_{T}-(T)(p-q)]=\E [S_{0}-(0)(p-q)]=\E [S_0-0(p-q)]=0.\]
</p>
<p>
We thus have \(\E [S_T]=(p-q)\E [T]\) as required.
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> We have
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--



                                                                                              E[ST ] = E[ST 1{T = Ta } + ST 1{T = Tb }]

                                                                                                     = E[a1{T = Ta } + b1{T = Tb }]

                                                                                                     = aP[T = Ta ] + bP[T = Tb ]



-->


<p>


\begin{align*}
\E [S_T] &amp;=\E [S_T\1\{T=T_a\}+S_T\1\{T=T_b\}]\\ &amp;=\E [a\1\{T=T_a\}+b\1\{T=T_b\}]\\ &amp;=a\P [T=T_a]+b\P [T=T_b]
\end{align*}
Putting equations <span class="textup">(<a href="notes_1.html#??">??</a>)</span> and <span class="textup">(<a href="notes_1.html#??">??</a>)</span> into the above, and then using part (a)
gives us
</p>
<p>
\[\E [T]=\frac {1}{p-q}\l (a\frac {(q/p)^b-1}{(q/p)^b-(q/p)^a}+b\frac {1-(q/p)^a}{(q/p)^b-(q/p)^a}\r ).\]
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We have \(\P [T_a&lt;\infty ]=\P [T_b&gt;\infty ]=1\) from Lemma <a href="notes_1.html#??">??</a>. Hence also \(\P
[T=T_a&lt;\infty \text { or }T=T_b&lt;\infty ]=1\). Since these two possibilities are disjoint, in fact \(\P [T=T_a]+\P [T=T_b]=1\).
</p>
<p>
For the second equation, we will use that \((S_n)\) is a martingale. We want to apply the optional stopping theorem, but none of the conditions apply directly to \((S_n)\). Instead, set \(M_n=S_{n\wedge T}\)
and note that Lemma <a href="notes_1.html#??">??</a> implies that \((M_n)\) is a martingale. By definition of \(T\) we have \(|M_n|\leq \max \{|a|,|b|\}\), hence \((M_n)\) is bounded. We have \(\P
[T&lt;\infty ]\) from above, hence conditions (b) of the optional stopping theorem apply to \((M_n)\). We thus deduce that
</p>
<p>
\[\E [M_T]=\E [M_0]=\E [S_0]=0.\]
</p>
<p>
Hence
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--



                                                                                              E[MT ] = E[MT 1{T =Ta } + MT 1{T =Tb } ]

                                                                                                      = E[MTa 1{T =Ta } ] + E[MTb 1{T =Tb } ]

                                                                                                      = E[STa 1{T =Ta } ] + E[STa 1{T =Ta } ]

                                                                                                      = E[a1{T =Ta } ] + E[b1{T =Ta } ]

                                                                                                      = aP[T = Ta ] + bP[T = Ta ]



-->


<p>


\begin{align*}
\E [M_T] &amp;= \E [M_T\1_{\{T=T_a\}} + M_T\1_{\{T=T_b\}}] \\ &amp;= \E [M_{T_a}\1_{\{T=T_a\}}] + \E [M_{T_b}\1_{\{T=T_b\}}] \\ &amp;= \E [S_{T_a}\1_{\{T=T_a\}}] + \E
[S_{T_a}\1_{\{T=T_a\}}] \\ &amp;= \E [a\1_{\{T=T_a\}}] + \E [b\1_{\{T=T_a\}}] \\ &amp;= a\P [T=T_a] + b\P [T=T_a]
\end{align*}
as required. In the above, the first line partitions on the value of \(T\) (using what we already deduced). The third line follows from the second because \(Ta\leq T\), hence \(M_{T_a}=S_{T_a\wedge
T}=S_{T_a}\), and similarly for \(T_b\).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Solving the two equations (this is left for you) gives that \(\P [T=T_a]=\frac {b}{b-a}\) and \(\P [T=T_b]=\frac {-a}{b-a}\).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(c)</span></span> From Exercise <a href="notes_1.html#??">??</a> we have that \(S_n^2-n\) is a martingale. Again, none of the conditions of
optional stopping directly apply, but \(X_n=S_{n\wedge T}^2-(n\wedge T)\) is a martingale by Theorem <a href="notes_1.html#??">??</a>. Similarly to above, we have \(|M_n|\leq \max
\{a^2+|a|,b^2+|b|\}\), so \((X_n)\) is bounded. Hence conditions (b) of the optional stopping theorem apply and
</p>
<p>
\[\E [X_T]=\E [X_0]=\E [S_0^2-0=]=0.\]
</p>
<p>
Thus \(\E [S_T^2]=\E [T]\), which gives
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--



                                                                                                  E[T ] = E[ST2 1{T =Ta } + ST2 1{T =Ta } ]

                                                                                                        = E[a2 1{T =Ta } + b2 1{T =Ta } ]

                                                                                                        = a2 P[T = Ta ] + b2 P[T = Tb ]
                                                                                                                 a2 b   b2 (−a)
                                                                                                        =             +
                                                                                                                b−1      b−a
                                                                                                        = −ab.



-->


<p>


\begin{align*}
\E [T] &amp;= \E [S_T^2\1_{\{T=T_a\}} + S_T^2\1_{\{T=T_a\}}] \\ &amp;= \E [a^2\1_{\{T=T_a\}} + b^2\1_{\{T=T_a\}}] \\ &amp;= a^2\P [T=T_a]+b^2\P [T=T_b] \\ &amp;= \frac
{a^2b}{b-1}+\frac {b^2(-a)}{b-a} \\ &amp;= -ab.
\end{align*}
Here, the second line follows because when \(T=T_a\) we have \(S_T=S_{T_a}=a\), hence also \(S_{T_a}^2=a^2\), and similarly for \(S_{T_b}^2\). The fourth line uses part (b).
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> Note that movements up at by \(+2\), whilst movements down are by \(-1\). In order for \(S_n\) to return to zero at time \(n\), it
must have made twice as many movements down as up. In particular \(n\) must therefore be a multiple of \(3\), so \(\P [S_{3n+1}=0]=\P [S_{3n+2}=0]=0\).
</p>
<p>
For \(S_{3n}\), we can count the possible ways to have \(n\) movements up and \(2n\) movements down by choosing \(n\) upwards movements out of \(3n\) movements, that is \(\binom {3n}{n}\). Taking the
probabilities into account and then using <span class="textup">(<a href="notes_1.html#??">??</a>)</span> we obtain
</p>
<p>
\[\P [S_{3n}=0] =\binom {3n}{n}\l (\frac 13\r )^{n}\l (\frac 23\r )^{2n} =\frac {(3n)!}{n!(2n)!}\frac {2^{2n}}{3^{3n}} \sim \frac {\sqrt {6\pi n}(3n/e)^{3n}}{\sqrt {8}\pi
n(n/e)^n(2n/e)^{2n}}\frac {2^{2n}}{3^{3n}} =\frac {\sqrt {3}}{2\sqrt {\pi n}}.                             \]
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> The argument that \(\E [G]&lt;\infty \), under the assumption \(\P [R&lt;\infty ]=1\), is exactly the same as before.
</p>
<p>
We need to modify the next part, beginning with <span class="textup">(<a href="notes_1.html#??">??</a>)</span> which is replaced by the result in part (a). Note that that \(S_{3n}\) now plays the role
of what was previously \(S_{2n}\), being the possible returns to zero. Having (a) introduces an extra factor of \(\frac {\sqrt {3}}{2}\), but using the same method as before we can still deduce that \(\E
[G]=\infty \). We thus arrive at a contradiction in the same way.
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> We have \(\E [X_i]=(p)(1)+(1-\frac {p}{2})(-1-2)=\frac {5p}{2}-3\). The condition \(p\in (\frac 56,1]\) implies that
\(\E [X_i]&gt;0\). Writing \(\mu =\E [X_i]\), it follows from the strong law of large numbers that (almost surely) there exists \(N\in \N \) such that for all \(n\geq N\) we have \(S_n\geq n\frac {\mu
}{2}\). Hence \(S_n\stackrel {a.s.}{\to }\infty \).
</p>


</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> We calculate
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--



                                                              E[T1 ] = E[T1 1{S1 =1} ] + E[T1 1{S1 =−1} ]                                + E[T1 1{S1 =−2} ]

                                                                    = E[11{S1 =1} ] + E[E[T1 1{S1 =−1} | F1 ]]                           + E[E[T1 1{S1 =−2} | F1 ]]

                                                                    =p                + E[1{S1 =−1} E[T1 | F1 ]]                         + E[1{S1 =−2} E[T1 | F1 ]]

                                                                    =p                + E[1{S1 =−1} E[1 + T1′ + T1′′ | F1 ]] + E[1{S1 =−2} E[1 + T1′ + T1′′ + T1′′′ | F1 ]]

                                                                    =p                + E[1{S1 =−1} E[1 + T1′ + T1′′ ]]                  + E[1{S1 =−1} E[1 + T1′ + T1′′ + T1′′′ ]]

                                                                    =p                + E[1{S1 =−1} (1 + 2E[T1 ])]                       + E[1{S1 =−1} (1 + 3E[T1 ])]
                                                                                          1−p                                                    1−p
                                                                    =p                +       (1 + 2E[T1 ])                              +           (1 + 3E[T1 ]).
                                                                                           2                                                      2


-->


<p>


\begin{alignat*} {3}
\E [T_1] &amp;= \E [T_1\1_{\{S_1=1\}}] &amp;&amp; +\E [T_1\1_{\{S_1=-1\}}]&amp;&amp; +\E [T_1\1_{\{S_1=-2\}}] \\ &amp;= \E [1\1_{\{S_1=1\}}] &amp;&amp; + \E [\E
[T_1\1_{\{S_1=-1\}}\|\mc {F}_{1}]] &amp;&amp; + \E [\E [T_1\1_{\{S_1=-2\}}\|\mc {F}_{1}]] \\ &amp;= p &amp;&amp; + \E [\1_{\{S_1=-1\}}\E [T_1\|\mc {F}_{1}]] &amp;&amp; + \E
[\1_{\{S_1=-2\}}\E [T_1\|\mc {F}_{1}]] \\ &amp;= p &amp;&amp; + \E [\1_{\{S_1=-1\}}\E [1+T&apos;_1+T&apos;&apos;_1\|\mc {F}_{1}]] &amp;&amp; + \E [\1_{\{S_1=-2\}}\E
[1+T&apos;_1+T&apos;&apos;_1+T&apos;&apos;&apos;_1\|\mc {F}_{1}]]\\ &amp;= p &amp;&amp; + \E [\1_{\{S_1=-1\}}\E [1+T&apos;_1+T&apos;&apos;_1]] &amp;&amp; + \E [\1_{\{S_1=-1\}}\E
[1+T&apos;_1+T&apos;&apos;_1+T&apos;&apos;&apos;_1]] \\ &amp;= p &amp;&amp; + \E [\1_{\{S_1=-1\}}(1+2\E [T_1])] &amp;&amp; + \E [\1_{\{S_1=-1\}}(1+3\E [T_1])] \\ &amp;= p
&amp;&amp; + \frac {1-p}{2}(1+2\E [T_1]) &amp;&amp; + \frac {1-p}{2}(1+3\E [T_1]).
\end{alignat*}
In the first line of the above we partition on the value of \(S_1\). The first term of the second line follows because if \(S_1=1\) then \(T_1=1\), and the other terms use the relationship between expectation and
conditional expectation. The third line follows because \(S_1\in \mc {F}_1\).
</p>
<p>
The fourth line uses that, on the event \(S_1=-1\), \(T_1\) is equal to \(1\) (accounting for the first move \(0\mapsto -1\)) plus two independent copies (\(T&apos;_1\) and \(T&apos;&apos;_1\)) of \(T_1\)
(accounting for the time to move from \(-1\mapsto 0\) and then \(0\mapsto 1\)). Similarly, on the event \(S_1=-1\), \(T_1\) is equal to \(1\) plus three independent copies of \(T_1\). Note that it is crucial
here that \((S_n)\) can only jump upwards by \(1\) in each step of time, meaning that it cannot go above \(1\) without first being equal to \(1\).
</p>
<p>
The strong Markov property gives that \(T&apos;_1,T&apos;&apos;_1\) and \(T&apos;&apos;&apos;_1\) are independent of \(\mc {F}_1\), leading to the fifth line. The remainder of the calculation is
straightforward. We thus obtain
</p>
<p>
\[\E [T_1]=p+\frac {1-p}{2}(2+5\E [T_t]).\]
</p>
<p>
Solving this equation gives \(\E [T_1]=\frac {2}{5p-3}\).
</p>
<p>


</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Recall that \(S_n\) even when \(n\) even, and odd when \(n\) is odd. Hence \(T_1\) is odd. From the fact given in the question, there
exists \(N\in \N \) such that for all \(n\geq N\) we have \(\P [T_1=2n-1]\geq \frac {1}{4\sqrt {\pi }n^{3/2}}\). Hence
</p>
<p>
\[\E [T_1]\geq \sum _{n=N}^\infty (2n-1)\P [T_1=2n-1] \geq \frac {1}{4\sqrt {\pi }}\sum _{n=N}^\infty \frac {2n-1}{n^{3/2}} \geq \frac {1}{2\sqrt {\pi }}\sum _{n=N}^\infty \frac
{1}{n^{1/2}} =\infty \]
</p>
<p>
as required.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> Note that \(M^{(\theta )}_n = \prod _{i=1}^n (e^{\theta X_i}/\cosh \theta )\). Since \(X_i\in m\mc {F}_n\) for all
\(i\leq n\), \(S_n\in m\mc {F}_n\) for all \(n\) by Proposition <a href="notes_1.html#??">??</a>. Since \(|S_n|\leq n\) we have \(|M_n|\leq \frac {e^{\theta n}}{(\cosh \theta )^n}&lt;\infty
\), hence \(M_n\in L^1\). We have also that
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--

                                                                                                                      n
                                                                                                                                     !       "             #
                                                                                                                      Y eθXi               eθXn+1
                                                                                              E[Mn+1 |Fn ] =                             E        Fn
                                                                                                                      i=1
                                                                                                                            cosh θ         cosh θ
                                                                                                                            "            #
                                                                                                                       eθXn+1
                                                                                                                = Mn E
                                                                                                                       cosh θ

                                                                                                                = Mn .



-->


<p>


\begin{align*}
\E [M_{n+1}|\mc {F}_n]&amp;=\l (\prod _{i=1}^n \frac {e^{\theta X_i}}{\cosh \theta }\r )\E \l [\frac {e^{\theta X_{n+1}}}{\cosh \theta }\Big {|}\mc {F}_n\r ]\\ &amp;=M_n\E \l
[\frac {e^{\theta X_{n+1}}}{\cosh \theta }\r ]\\ &amp;=M_n.
\end{align*}
Here we use the taking out what is known rule, the fact that \(X_{n+1}\) is independent of \(\mc {F}_n\) and the relationship between conditional expectation and independence. To deduce the final line we note
that \(\E [e^{\theta X_i}/\cosh \theta ] = \frac 1 2 (e^{\theta }+e^{-\theta })/\cosh \theta = 1\).
</p>
<p>
<i>Note: This is a more general case of the martingale from Exercise <a href="notes_1.html#??">??</a>.</i>
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> Condition (1) fails because \(T_m\) is not bounded – to see this, note that for any \(N\in \N \) there is a positive probability that
\((S_n)_{n=0}^N\) remains below zero, in which case \(T_m\geq N\). Condition (b) fails because \((M_n)\) is unbounded. Condition (c) fails because we have \(\E [T_1]=\infty \) by Lemma <a
href="notes_1.html#??">??</a>, and \(T_m\geq T_1\) so also \(\E [T_m]=\infty \).
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(c)</span></span> By Lemma <a href="notes_1.html#??">??</a> the process \(L^{(\theta )}_n=M_{n\wedge T}^{(\theta )}\) is a martingale,
and by definition of \(T\) we have that \(\frac {e^{-m\theta }}{(\cosh \theta )^n}\leq L^{(\theta )}_n\leq \frac {e^{m\theta }}{(\cosh \theta )^n}\). Since \(\cosh \theta \geq 1\), we thus
have \(|L^{(\theta )}_n|\leq e^{m\theta }+e^{-m\theta }\), so the martingale \((L^{\theta }_n)\) is bounded. By Lemma <a href="notes_1.html#??">??</a> \(\P [T&lt;\infty ]=1\), so conditions
(b) of the optional stopping theorem apply to \((L^{(\theta )}_n)\) and \(T\), hence
</p>
<p>
\[\E \l [\frac {e^{\theta S_{T}}}{(\cosh \theta )^{T}}\r ]=\E \l [\frac {e^{\theta S_0}}{(\cosh \theta )^0}\r ]=1.\]
</p>
<p>
By Lemma <a href="notes_1.html#??">??</a> we have \(\P [T&lt;\infty ]=1\), hence almost surely either \(T=T_m\) or \(T=T_{-m}\). Hence,
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--

                                                                                     "                                                                #
                                                                                     eθSτ                   eθSτ
                                                                              1=E           1{T =T   } +           1{T =T−m }
                                                                                  (cosh θ)T        m
                                                                                                         (cosh θ)T
                                                                                     "                              #           "                              #
                                                                                          emθ                           e−mθ
                                                                                =E            T
                                                                                                  1{T =T  }   +  E                1{T =T−m }
                                                                                     (cosh θ) m         m
                                                                                                                    (cosh θ)T−m
                                                                                              1                                     1
                                                                                                                                                    
                                                                                = emθ E               1{T =Tm }   + e−mθ
                                                                                                                         E                 1 {T =T−m }
                                                                                          (cosh θ)Tm                           (cosh θ)T−m
                                                                                                            1
                                                                                                                            
                                                                                                                   1{T =Tm }
                                                                                               
                                                                                = emθ + e−mθ E
                                                                                                      (cosh θ)Tm


-->


<p>


\begin{align*}
1 &amp;= \E \l [\frac {e^{\theta S_{\tau }}}{(\cosh \theta )^{T}}\1_{\{T=T_m\}} + \frac {e^{\theta S_{\tau }}}{(\cosh \theta )^{T}}\1_{\{T=T_{-m}\}}\r ] \\ &amp;= \E \l [\frac
{e^{m\theta }}{(\cosh \theta )^{T_m}}\1_{\{T=T_m\}}\r ] + \E \l [\frac {e^{-m\theta }}{(\cosh \theta )^{T_{-m}}}\1_{\{T=T_{-m}\}}\r ] \\ &amp;= e^{m\theta }\E \l [\frac
{1}{(\cosh \theta )^{T_m}}\1_{\{T=T_m\}}\r ] + e^{-m\theta }\E \l [\frac {1}{(\cosh \theta )^{T_{-m}}}\1_{\{T=T_{-m}\}}\r ] \\ &amp;= \l (e^{m\theta }+ e^{-m\theta }\r )\E \l
[\frac {1}{(\cosh \theta )^{T_{m}}}\1_{\{T=T_{m}\}}\r ]
\end{align*}
In the above, the final line follows by symmetry about zero, which implies that the two expectations are in fact equal. Thus \(\E \l [\frac {\1_{\{T=T_m\}}}{(\cosh \theta )^{T_m}}\r ]=\frac {1}{2\cosh
(m\theta )}\), for all \(m\in \N \). With this in hand we can calculate
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--

                                                                                          
                                                                                            1
                                                                                                                 
                                                                                                                        1{T =Tm }               
                                                                                                                                                     1{T =T−m } 
                                                                                      E                    =E              +E
                                                                                        (cosh θ)T               (cosh θ)Tm     (cosh θ)T−m
                                                                                                                  1           1
                                                                                                           =             +
                                                                                                             2 cosh(mθ) 2 cosh(−mθ)
                                                                                                                 1
                                                                                                           =           .
                                                                                                             cosh(mθ)


-->


<p>


\begin{align*}
\E \l [\frac {1}{(\cosh \theta )^{T}}\r ] &amp;= \E \l [\frac {\1_{\{T=T_m\}}}{(\cosh \theta )^{T_m}}\r ] + \E \l [\frac {\1_{\{T=T_{-m}\}}}{(\cosh \theta )^{T_{-m}}}\r ] \\
&amp;=\frac {1}{2\cosh (m\theta )}+\frac {1}{2\cosh (-m\theta )}\\ &amp;=\frac {1}{\cosh (m\theta )}.
\end{align*}
In the first line we partition on whether \(T\) is equal to \(T_m\) or \(T_{-m}\). In the second line we apply the formulae deduced above at both \(m\) and \(-m\), and to deduce the final line we recall that \(\cosh
(x)=\cosh (-x)\).
</p>
<p>
<i>Note: \(T_m\) and \(\1_{\{T=T_m\}}\) are not independent, but if they were then it would be possible to write a simpler solution. Check that you didn’t fall into this trap.</i>
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> We have show in Lemma <a href="notes_1.html#??">??</a> that \((S_n)\) will almost surely return to the origin in finite time. By
applying the strong Markov property at each return to the origin, we obtain that \((S_n)\) will return to the origin infinitely often.
</p>
<p>
Fix some \(\v {z}\in \Z ^2\). There is a path from the origin to \(\v {z}\), and with positive probability the walk will take that path, hence \(\P [S_n=\v {z}\text { for some }n\in \N ]\) is positive. By
the strong Markov property, this means the probability that \((S_n)\) visits \(\v {z}\) in between any two successive visits to the origin, is also positive. By the strong Markov property, whether this occur is
independent, for each successive pair of returns to the origin. Hence we have infinitely many trials to try and visit \(\v {z}\), each with the same positive success probability. This will result in infinitely many visits
to \(\v {z}\), almost surely.
</p>
<p>
This holds for any \(\v {z}\in \Z ^2\), which completes the argument.
</p>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span>
</p>
<ul class="enumerate" style="list-style-type:none">


<li>
<p>
<span class="listmarker"><span class="textnormal">(a)</span></span> If \(L=\infty \) then the set \(\{n\-S_n=0\}\) much be infinite, meaning that \((S_n)\) has returned infinitely many times to the
origin. However, this has probability zero; by the strong Markov property, after each visit there is a probability \(\P [R=\infty ]&gt;0\) of not returning again, independently of the past; eventually that will
happen.
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(b)</span></span> \(L\) is not a stopping time. One way to see this is to note that \(\{L=0\}=\{R=\infty \}\), which depends on the whole path
taken by the walk, so is not an element of \(\mc {F}_n\) for any \(N\in \N \).
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(c)</span></span> The event \(\{L=2n\}\) is the event that \(S_{2n}=0\), but the walk is never zero after that time. Hence
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--



                                                                                       P[L = 2n] = E[1{L=2n} ]

                                                                                                     = E[1{S2n =0} 1{Sk ̸=0 for all k>2n} ]

                                                                                                     = E[E[1{S2n =0} 1{Sk ̸=0 for all k>2n} | F2n ]]

                                                                                                     = E[1{S2n =0} E[1{Sk ̸=0 for all k>2n} | F2n ]]

                                                                                                     = E[1{S2n =0} P[R = ∞]]

                                                                                                     = P[R = ∞]P[S2n = 0].



-->


<p>


\begin{align*}
\P [L=2n] &amp;= \E [\1_{\{L=2n\}}] \\ &amp;= \E [\1_{\{S_{2n}=0\}}\1_{\{S_{k}\neq 0\text { for all }k&gt;2n\}}] \\ &amp;= \E [\E [\1_{\{S_{2n}=0\}}\1_{\{S_{k}\neq 0\text { for
all }k&gt;2n\}}\|\mc {F}_{2n}]] \\ &amp;= \E [\1_{\{S_{2n}=0\}}\E [\1_{\{S_{k}\neq 0\text { for all }k&gt;2n\}}\|\mc {F}_{2n}]] \\ &amp;= \E [\1_{\{S_{2n}=0\}}\P [R=\infty ]] \\
&amp;= \P [R=\infty ]\P [S_{2n}=0].
\end{align*}
Note that we have used the Markov property to deduce the fifth line.
</p>
<p>
Recall that \(S_n\) is even if and only if \(n\) is even. Taking the above equation and summing over \(n\), noting that \(\sum _{n=0}^\infty \P [L=2n]=1\) we obtain
</p>
<span class="hidden"> \(\seteqnumber{0}{A.}{12}\)</span>


<!--

                                                                                                                ∞
                                                                                                                X
                                                                                       1 = P[R = ∞]                   P[S2n = 0] = P[R = ∞]E[G].
                                                                                                                n=0



-->


<p>


\begin{align*}
1 \;=\; \P [R=\infty ]\sum \limits _{n=0}^\infty \P [S_{2n}=0] \;=\; \P [R=\infty ]\E [G].
\end{align*}
The required result follows by noting that \(\P [R&lt;\infty ]=1-\P [R=\infty ]\).
</p>
</li>
<li>


<p>
<span class="listmarker"><span class="textnormal">(d)</span></span> During the proof of Lemma <a href="notes_1.html#??">??</a>, whilst we were arguing by contradiction, we noted that if \(p=\P
[R&lt;\infty ]&lt;1\) then \(G\) would have a Geometric(\(p\)) distribution. In part (c) we’ve discovered the mean of this geometric distribution. We also came close, for the same reason, during the proofs of
Lemmas <a href="notes_1.html#??">??</a> and <a href="notes_1.html#??">??</a>, and in Remark <a href="notes_1.html#??">??</a>. In all of those cases, all we needed was to note that \(\E [G]\)
would be finite – we didn’t need to know its value.
</p>
</li>
</ul>
</li>
<li>


<p>
<span class="listmarker"><a href="notes_1.html#??">??</a></span> Let \(B_r=\{\v {z}\in \Z ^3\-|\v {z}|&lt;n\}\). Note that \(B_r\) is a finite set.
</p>
<p>
Consider some \(\v {z}\in \Z ^3\). If \((S_n)\) visits \(\v {z}\) then, the first time that it does, we may apply the strong Markov property. From then on the walk will behave independently of its past, again
moving one unit of space in a random direction on each step of time. Apply part (c) of Exercise <a href="notes_1.html#??">??</a>, the expected number of returns to \(\v {z}\) is finite, hence there will almost
surely be a last return time to \(\v {z}\).
</p>
<p>
We can do the above for any \(\v {z}\in B_N\). Since \(B_r\) is finite, this means that almost surely there is a last return time to the set \(B_r\). That is, there exists some random variable \(N\in \N \) such
that \(S_n\notin B_r\) for all \(n\geq N\). Hence \(|S_n|\geq r\) for all \(n\geq N\). Since this holds for any \(r\in \N \), we have \(|S_n|\stackrel {a.s.}{\to }\infty \).
</p>
<p>


</p>
</li>
</ul>

</section>

</main>

</div>

<footer>

<p>
Copyright Nic Freeman, Sheffield University, last updated April 22, 2025
</p>

</footer>



<nav class="botnavigation"><a href="notes_1.html" class="linkhome" >
Home</a></nav>

</body>
</html>
